{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df878f0e-238e-497f-bb35-33c35737d09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROMPT = \"\"\"\n",
    "The dialogue above is from **ELIZA**, an early natural language processing system\n",
    "ELIZA\n",
    "that could carry on a limited conversation with a user by imitating the responses of a Rogerian psychotherapist (Weizenbaum, 1966). ELIZA is a surprisingly simple program that uses pattern matching to recognize phrases like \"I need X\" and translate them into suitable outputs like \"What would it mean to you if you got X?\". This simple technique succeeds in this domain because ELIZA doesn't actually need to know anything to mimic a Rogerian psychotherapist. As Weizenbaum notes, this is one of the few dialogue genres where listeners can act as if they know nothing of the world. ELIZA's mimicry of human conversation was remarkably successful: many people who interacted with ELIZA came to believe that it really *understood* them and their problems, many continued to believe in ELIZA's abilities even after the program's operation was explained to them (Weizenbaum, 1976), and even today\n",
    "such **chatbots** are a fun diversion.\n",
    "chatbots\n",
    "Of course modern conversational agents are much more than a diversion; they can answer questions, book flights, or find restaurants, functions for which they rely on a much more sophisticated understanding of the user's intent, as we will see in Chapter 15. Nonetheless, the simple pattern-based methods that powered ELIZA and other chatbots play a crucial role in natural language processing.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3b2577e-5d53-45ea-b52c-368648578d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from markdown import markdown\n",
    "\n",
    "def markdown_to_text(markdown_string):\n",
    "    \"\"\" Converts a markdown string to plaintext \"\"\"\n",
    "\n",
    "    # md -> html -> text since BeautifulSoup can extract text cleanly\n",
    "    html = markdown(markdown_string)\n",
    "\n",
    "    # remove code snippets\n",
    "    html = re.sub(r'<pre>(.*?)</pre>', ' ', html)\n",
    "    html = re.sub(r'<code>(.*?)</code >', ' ', html)\n",
    "\n",
    "    # extract text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = ''.join(soup.findAll(text=True))\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6778dc0-13f1-4820-a2d2-29ebd345fe91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1600831/4139579501.py:17: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  text = ''.join(soup.findAll(text=True))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The dialogue above is from ELIZA, an early natural language processing system ELIZA that could carry on a limited \n",
       "conversation with a user by imitating the responses of a Rogerian psychotherapist <span style=\"font-weight: bold\">(</span>Weizenbaum, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1966</span><span style=\"font-weight: bold\">)</span>. ELIZA is a \n",
       "surprisingly simple program that uses pattern matching to recognize phrases like <span style=\"color: #008000; text-decoration-color: #008000\">\"I need X\"</span> and translate them into\n",
       "suitable outputs like <span style=\"color: #008000; text-decoration-color: #008000\">\"What would it mean to you if you got X?\"</span>. This simple technique succeeds in this domain \n",
       "because ELIZA doesn't actually need to know anything to mimic a Rogerian psychotherapist. As Weizenbaum notes, this\n",
       "is one of the few dialogue genres where listeners can act as if they know nothing of the world. ELIZA's mimicry of \n",
       "human conversation was remarkably successful: many people who interacted with ELIZA came to believe that it really \n",
       "understood them and their problems, many continued to believe in ELIZA's abilities even after the program's \n",
       "operation was explained to them <span style=\"font-weight: bold\">(</span>Weizenbaum, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1976</span><span style=\"font-weight: bold\">)</span>, and even today such chatbots are a fun diversion. chatbots Of \n",
       "course modern conversational agents are much more than a diversion; they can answer questions, book flights, or \n",
       "find restaurants, functions for which they rely on a much more sophisticated understanding of the user's intent, as\n",
       "we will see in Chapter <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>. Nonetheless, the simple pattern-based methods that powered ELIZA and other chatbots play\n",
       "a crucial role in natural language processing.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The dialogue above is from ELIZA, an early natural language processing system ELIZA that could carry on a limited \n",
       "conversation with a user by imitating the responses of a Rogerian psychotherapist \u001b[1m(\u001b[0mWeizenbaum, \u001b[1;36m1966\u001b[0m\u001b[1m)\u001b[0m. ELIZA is a \n",
       "surprisingly simple program that uses pattern matching to recognize phrases like \u001b[32m\"I need X\"\u001b[0m and translate them into\n",
       "suitable outputs like \u001b[32m\"What would it mean to you if you got X?\"\u001b[0m. This simple technique succeeds in this domain \n",
       "because ELIZA doesn't actually need to know anything to mimic a Rogerian psychotherapist. As Weizenbaum notes, this\n",
       "is one of the few dialogue genres where listeners can act as if they know nothing of the world. ELIZA's mimicry of \n",
       "human conversation was remarkably successful: many people who interacted with ELIZA came to believe that it really \n",
       "understood them and their problems, many continued to believe in ELIZA's abilities even after the program's \n",
       "operation was explained to them \u001b[1m(\u001b[0mWeizenbaum, \u001b[1;36m1976\u001b[0m\u001b[1m)\u001b[0m, and even today such chatbots are a fun diversion. chatbots Of \n",
       "course modern conversational agents are much more than a diversion; they can answer questions, book flights, or \n",
       "find restaurants, functions for which they rely on a much more sophisticated understanding of the user's intent, as\n",
       "we will see in Chapter \u001b[1;36m15\u001b[0m. Nonetheless, the simple pattern-based methods that powered ELIZA and other chatbots play\n",
       "a crucial role in natural language processing.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "print(markdown_to_text(USER_PROMPT).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00d3c592-2999-4df1-9104-0cd18cbae225",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gector'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GECToR, predict, load_verb_dict\n\u001b[1;32m      4\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgotutiyan/gector-roberta-base-5k\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m GECToR\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gector'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from gector import GECToR, predict, load_verb_dict\n",
    "\n",
    "model_id = 'gotutiyan/gector-roberta-base-5k'\n",
    "model = GECToR.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "encode, decode = load_verb_dict('data/verb-form-vocab.txt')\n",
    "srcs = [\n",
    "    USER_PROMPT\n",
    "]d\n",
    "corrected = predict(\n",
    "    model, tokenizer, srcs,\n",
    "    encode, decode,\n",
    "    keep_confidence=0.0,\n",
    "    min_error_prob=0.0,\n",
    "    n_iteration=5,\n",
    "    batch_size=2,\n",
    ")\n",
    "print(corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "942a07f1-9b35-49dd-90f0-0a806bed1875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'User:\\nI am unhappy. ELIZA: DO YOU THINK COMING HERE WILL HELP YOU NOT TO BE </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">UNHAPPY\\nUser:\\nI need some help, that much seems certain. ELIZA: WHAT WOULD IT MEAN TO YOU IF YOU GOT SOME </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">HELP\\nUser:\\nPerhaps I could learn to get along with my mother. ELIZA: TELL ME MORE ABOUT YOUR FAMILY\\nUser:\\nMy </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mother takes care of me. ELIZA: WHO ELSE IN YOU FAMILY TAKES CARE OF YOU\\nUser:\\nMy father. ELIZA: YOUR </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">FATHER\\nUser:\\nYou are like my father in some ways. Weizenbaum (1966)\\nThe dialogue above is from ELIZA, an early </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">natural language processing system\\nELIZA\\nthat could carry on a limited conversation with a user by imitating the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">responses of a Rogerian psychotherapist (Weizenbaum, 1966). ELIZA is a surprisingly simple program that uses </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pattern matching to recognize phrases like \"I need X\" and translate them into suitable outputs like \"What would it </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mean to you if you got X?\". This simple technique succeeds in this domain because ELIZA doesn\\'t actually need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">know anything to mimic a Rogerian psychotherapist. As Weizenbaum notes, this is one of the few dialogue genres </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">where listeners can act as if they know nothing of the world. ELIZA\\'s mimicry of human conversation was remarkably</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">successful: many people who interacted with ELIZA came to believe that it really understood them and their </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">problems, many continued to believe in ELIZA\\'s abilities even after the program\\'s operation was explained to them</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(Weizenbaum, 1976), and even today\\nsuch chatbots are a fun diversion. chatbots\\nOf course modern conversational </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">agents are much more than a diversion; they can answer questions, book flights, or find restaurants, functions for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">which they rely on a much more sophisticated understanding of the user\\'s intent, as we will see in Chapter 15. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Nonetheless, the simple pattern-based methods that powered ELIZA and other chatbots play a crucial role in natural </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language processing. We\\'ll begin with the most important tool for describing text patterns: the regular </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">expression. Regular expressions can be used to specify strings we might want to extract from a document, from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">transforming \"I need X\" in ELIZA above, to defining strings like $199 or $24.99 for extracting tables of prices </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">from a document. We\\'ll then turn to a set of tasks collectively called text normalization, in which text </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">normalization regular expressions play an important part.'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Normalizing text means converting it to a more convenient, standard form. For example, most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of what we are going to do with language relies on first separating out or tokenizing words from running text, the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">task of tokenization. English words are often separated from each other tokenization by whitespace, but whitespace </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is not always sufficient. New York and rock 'n' roll are sometimes treated as large words despite the fact that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">they contain spaces, while sometimes we'll need to separate I'm into the two words I and am. For processing tweets </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">or texts we'll need to tokenize emoticons like :) or hashtags like #nlproc.\"</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Some languages, like Japanese, don't have spaces between words, so word tokenization becomes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">more difficult. Another part of text normalization is lemmatization, the task of determining lemmatization that two</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">words have the same root, despite their surface differences. For example, the words sang, sung, and sings are forms</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of the verb sing. The word sing is the common lemma of these words, and a lemmatizer maps from all of these to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sing. Lemmatization is essential for processing morphologically complex languages like Arabic. Stemming refers to a</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">simpler version of lemmatization in which we mainly stemming just strip suffixes from the end of the word. Text </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">normalization also includes sentence segmentation: breaking up a text into individual sentences, using cues like </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sentence segmentation periods or exclamation points.\"</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Finally, we'll need to compare words and other strings. We'll introduce a metric called edit </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">distance that measures how similar two strings are based on the number of edits (insertions, deletions, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">substitutions) it takes to change one string into the other. Edit distance is an algorithm with applications </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">throughout language processing, from spelling correction to speech recognition to coreference resolution.\"</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"One of the unsung successes in standardization in computer science has been the regular </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">expression (often shortened to regex), a language for specifying text search regular expression strings. This </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">practical language is used in every computer language, word processor, and text processing tools like the Unix </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tools grep or Emacs. Formally, a regular expression is an algebraic notation for characterizing a set of strings. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Regular expressions are particularly useful for searching in texts, when we have a pattern to search for and a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">corpus of texts to search through. A regular expression search function corpus will search through the corpus, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">returning all texts that match the pattern. The corpus can be a single document or a collection. For example, the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Unix command-line tool grep takes a regular expression and returns every line of the input document that matches </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the expression. A search can be designed to return every match on a line, if there are more than one, or just the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">first match. In the following examples we generally underline the exact part of the pattern that matches the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">regular expression and show only the first match. We'll show regular expressions delimited by slashes but note that</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">slashes are not part of the regular expressions. Regular expressions come in many variants.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.1 Regular Expressions'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"We'll be describing extended regular expressions; different regular expression parsers may </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">only recognize subsets of these, or treat some expressions slightly differently. Using an online regular expression</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tester is a handy way to test out your expressions and explore these variations.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.1 Regular Expressions'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The simplest kind of regular expression is a sequence of simple characters; putting </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">characters in sequence is called concatenation. To search for woodchuck, we type concatenation\\n/woodchuck/. The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">expression /Buttercup/ matches any string containing the substring Buttercup; grep with that expression would </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">return the line I\\'m called little Buttercup. The search string can consist of a single character (like /!/) or a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sequence of characters (like /urgl/) (see Fig. 2.1). Regular expressions are case sensitive; lower case /s/ is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">distinct from upper\\n| Regex                                             </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n|---------------------------------------------------|\\n| /woodchucks/                                      |\\n| </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"interesting links to woodchucks and lemurs\"      |\\n| /a/                                               |\\n| \"Mary</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Ann stopped by Mona\\'s\"                      |\\n| /!/                                               |\\n| \"You\\'ve </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">left the burglar behind again!\" said Nori |\\ncase /S/ (/s/ matches a lower case s but not an upper case S). This </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">means that the pattern /woodchucks/ will not match the string Woodchucks. We can solve this problem with the use of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the square braces [ and ]. The string of characters inside the braces specifies a disjunction of characters to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">match. For example, Fig. 2.2 shows that the pattern /[wW]/ matches patterns containing either w or W. | Regex      </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">| Match              | Example Patterns   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n|------------------------|--------------------|--------------------|\\n| /[wW]oodchuck/         |                </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|                    |\\n| Woodchuck or woodchuck | \"Woodchuck\"        |                    |\\n| /[abc]/            </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|                    |                    |\\n| \\'a\\', \\'b\\',              | or                 | \\'c\\'             </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| /[1234567890]/         |                    |                    |\\n| any digit              | \"plenty of 7 to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">5\" |                    |\\nThe regular expression /[1234567890]/ specifies any single digit. While such classes of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">characters as digits or letters are important building blocks in expressions, they can get awkward (e.g., it\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inconvenient to specify\\n/[ABCDEFGHIJKLMNOPQRSTUVWXYZ]/\\nto mean \"any capital letter\"). In cases where there is a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">well-defined sequence associated with a set of characters, the brackets can be used with the dash (-) to specify </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">any one character in a range. The pattern /[2-5]/ specifies any one of the characrange ters 2, 3, 4, or 5. The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pattern /[b-g]/ specifies one of the characters b, c, d, e, f, or g. Some other examples are shown in Fig. 2.3. | </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Regex                | Match                                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n|----------------------|------------------------------------------|\\n| /[A-Z]/              |                   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| an upper case letter | \"we should call it \\'Drenched Blossoms\\' \" |\\n| /[a-z]/              |                 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| a lower case letter  | \"my beans were impatient to be hoed!\"    |\\n| /[0-9]/              |                   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| a single digit       | \"Chapter 1: Down the Rabbit Hole\"        |\\nThe square braces can also be used to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">specify what a single character cannot be, by use of the caret ˆ. If the caret ˆ is the first symbol after the open</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">square brace [, the resulting pattern is negated. For example, the pattern /[ˆa]/ matches any single character </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(including special characters) except a. This is only true when the caret is the first symbol after the open square</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">brace. If it occurs anywhere else, it usually stands for a caret; Fig.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.1.1 Basic Regular Expression Patterns'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2.4 shows some examples. | Regex                    | Match (single characters)          </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n|--------------------------|------------------------------------|\\n| /[ˆA-Z]/                 |                 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| not an upper case letter | \"Oyfn pripetchik\"                  |\\n| /[ˆSs]/                  |                 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| neither \\'S\\' nor \\'s\\'      | \"I have no exquisite reason for\\'t\" |\\n| /[ˆ.]/                   |            </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| not a period             | \"our resident Djinn\"               |\\n| /[eˆ]/                   |                 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| either \\'e\\' or \\'          |                                    |\\n| ˆ                        |              </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| \\'                        | \"look up ˆ now\"                    |\\n| /aˆb/                    |                </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| the pattern \\'            |                                    |\\n| aˆb                      |                </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| \\'                        | \"look up aˆ b now\"                 |\\nHow can we talk about optional elements, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">like an optional s in woodchuck and woodchucks? We can\\'t use the square brackets, because while they allow us to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">say\\n\"s or S\", they don\\'t allow us to say \"s or nothing\". For this we use the question mark\\n/?/, which means \"the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preceding character or nothing\", as shown in Fig. 2.5. | Regex                   | Match       </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n|-------------------------|-------------|\\n| /woodchucks?/           |             |\\n| woodchuck or woodchucks </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">| \"woodchuck\" |\\n| /colou?r/               |             |\\n| color or colour         | \"color\"     |\\nWe can think</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of the question mark as meaning \"zero or one instances of the previous character\". That is, it\\'s a way of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">specifying how many of something that we want, something that is very important in regular expressions. For </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">example, consider the language of certain sheep, which consists of strings that look like the following:\\nbaa! </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">baaa!'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.1.1 Basic Regular Expression Patterns'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'baaaa! baaaaa! ... This language consists of strings with a b, followed by at least two a\\'s,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">followed by an exclamation point. The set of operators that allows us to say things like \"some number of as\" are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">based on the asterisk or , commonly called the Kleene *** (gen-\\nKleene *\\nerally pronounced \"cleany star\"). The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Kleene star means \"zero or more occurrences of the immediately previous character or regular expression\". So /a/ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">means \"any string of zero or more as\". This will match a or aaaaaa, but it will also match the empty string at the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">start of Off Minor since the string Off Minor starts with zero a\\'s. So the regular expression for matching one or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">more a is /aa/, meaning one a followed by zero or more as. More complex patterns can also be repeated. So </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">/[ab]/\\nmeans \"zero or more a\\'s or b\\'s\" (not \"zero or more right square braces\"). This will match strings like </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">aaaa or ababab or bbbb. For specifying multiple digits (useful for finding prices) we can extend /[0-9]/, the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">regular expression for a single digit. An integer (a string of digits) is thus\\n/[0-9][0-9]/. (Why isn\\'t it just </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">/[0-9]/?)\\nSometimes it\\'s annoying to have to write the regular expression for digits twice, so there is a shorter</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">way to specify \"at least one\" of some character. This is the Kleene +, which means \"one or more occurrences of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">immediately preceding Kleene +\\ncharacter or regular expression\". Thus, the expression /[0-9]+/ is the normal way </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to specify \"a sequence of digits\". There are thus two ways to specify the sheep language: /baaa!/ or /baa+!/. One </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">very important special character is the period (/./), a wildcard expression that matches any single character </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(except a carriage return), as shown in Fig. 2.6. | Regex                 | Match   | Example Matches   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n|-----------------------|---------|-------------------|\\n| /beg.n/               |         |                   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| any character between | beg     | and               |\\nThe wildcard is often used together with the Kleene </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">star to mean \"any string of characters\". For example, suppose we want to find any line in which a particular word, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for example, aardvark, appears twice. We can specify this with the regular expression /aardvark.aardvark/. Anchors </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are special characters that anchor regular expressions to particular places anchors in a string. The most common </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">anchors are the caret ˆ and the dollar sign $. The caret\\nˆ matches the start of a line. The pattern /ˆThe/ matches</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the word The only at the start of a line. Thus, the caret ˆ has three uses: to match the start of a line, to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">indicate a negation inside of square brackets, and just to mean a caret. (What are the contexts that allow grep or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Python to know which function a given caret is supposed to have?) The dollar sign $ matches the end of a line. So </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the pattern ␣$ is a useful pattern for matching a space at the end of a line, and /ˆThe dog.$/ matches a line that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">contains only the phrase The dog. (We have to use the backslash here since we want the . to mean \"period\" and not </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the wildcard.)\\nMatch\\nˆ\\nstart of line\\n$\\nend of line\\n\\\\b\\nword boundary\\n\\\\B\\nnon-word boundary\\nThere are also</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">two other anchors: \\\\b matches a word boundary, and \\\\B matches a non-boundary. Thus, /\\\\bthe\\\\b/ matches the word </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the but not the word other. More technically, a \"word\" for the purposes of a regular expression is defined as any </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sequence of digits, underscores, or letters; this is based on the definition of \"words\" in programming languages. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">For example, /\\\\b99\\\\b/ will match the string 99 in There are 99 bottles of beer on the wall (because 99 follows a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">space) but not 99 in There are 299 bottles of beer on the wall (since 99 follows a number). But it will match 99 in</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">$99 (since 99 follows a dollar sign ($), which is not a digit, underscore, or letter).'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.1.1 Basic Regular Expression Patterns'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Suppose we need to search for texts about pets; perhaps we are particularly interested in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cats and dogs. In such a case, we might want to search for either the string cat or the string dog. Since we can\\'t</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">use the square brackets to search for \"cat or dog\" (why can\\'t we say /[catdog]/?), we need a new operator, the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">disjunction operator, also disjunction called the pipe symbol |. The pattern /cat|dog/ matches either the string </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cat or the string dog. Sometimes we need to use this disjunction operator in the midst of a larger sequence. For </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">example, suppose I want to search for information about pet fish for my cousin David. How can I specify both guppy </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and guppies? We cannot simply say /guppy|ies/, because that would match only the strings guppy and ies. This is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">because sequences like guppy take precedence over the disjunction operator |. precedence To make the disjunction </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">operator apply only to a specific pattern, we need to use the parenthesis operators ( and ). Enclosing a pattern in</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">parentheses makes it act like a single character for the purposes of neighboring operators like the pipe | and the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Kleene. So the pattern /gupp(y|ies)/ would specify that we meant the disjunction only to apply to the suffixes y </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and ies. The parenthesis operator ( is also useful when we are using counters like the Kleene. Unlike the | </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">operator, the Kleene operator applies by default only to a single character, not to a whole sequence. Suppose we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">want to match repeated instances of a string. Perhaps we have a line that has column labels of the form Column 1 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Column 2 Column 3. The expression /Column␣[0-9]+␣/ will not match any number of columns; instead, it will match a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">single column followed by\\nany number of spaces! The star here applies only to the space ␣ that precedes it, not to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the whole sequence. With the parentheses, we could write the expression /(Column␣[0-9]+␣)/ to match the word </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Column, followed by a number and optional spaces, the whole pattern repeated zero or more times. This idea that one</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">operator may take precedence over another, requiring us to sometimes use parentheses to specify what we mean, is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">formalized by the operator precedence hierarchy for regular expressions. The following table gives the order </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">operator precedence of RE operator precedence, from highest precedence to lowest precedence. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Parenthesis\\n()\\nCounters\\n* + ? {}\\nSequences and anchors\\nthe ˆmy end$\\nDisjunction\\n|\\nThus, because counters </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">have a higher precedence than sequences,\\n/the/ matches theeeee but not thethe. Because sequences have a higher </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">precedence than disjunction, /the|any/ matches the or any but not thany or theny. Patterns can be ambiguous in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">another way. Consider the expression /[a-z]/\\nwhen matching against the text once upon a time. Since /[a-z]/ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">matches zero or more letters, this expression could match nothing, or just the first letter o, on, onc, or once. In</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">these cases regular expressions always match the largest string they can;\\nwe say that patterns are greedy, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">expanding to cover as much of a string as they can. greedy There are, however, ways to enforce non-greedy matching,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">using another meannon-greedy ing of the ? qualifier.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.1.2 Disjunction, Grouping, And Precedence'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The operator ? is a Kleene star that matches as little text as\\n*? possible. The operator +? </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is a Kleene plus that matches as little text as possible. +?'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.1.2 Disjunction, Grouping, And Precedence'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Suppose we wanted to write a RE to find cases of the English article the. A simple\\n(but </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">incorrect) pattern might be:\\n/the/\\nOne problem is that this pattern will miss the word when it begins a sentence </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and hence is capitalized (i.e., The).'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.1.3 A Simple Example'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"This might lead us to the following pattern:\\n/[tT]he/\\nBut we will still incorrectly return </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">texts with the embedded in other words (e.g., other or theology). So we need to specify that we want instances with</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a word boundary on both sides:\\n/\\\\b[tT]he\\\\b/\\nSuppose we wanted to do this without the use of /\\\\b/. We might </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">want this since\\n/\\\\b/ won't treat underscores and numbers as word boundaries; but we might want to find the in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">some context where it might also have underlines or numbers nearby\\n(the or the25). We need to specify that we want</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">instances in which there are no alphabetic letters on either side of the the:\\n/[ˆa-zA-Z][tT]he[ˆa-zA-Z]/\\nBut </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there is still one more problem with this pattern: it won't find the word the when it begins a line. This is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">because the regular expression [ˆa-zA-Z], which we used to avoid embedded instances of the, implies that there must</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">be some single\\n(although non-alphabetic) character before the the. We can avoid this by specifying that before the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the we require either the beginning-of-line or a non-alphabetic character, and the same at the end of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">line:\\n/(ˆ|[ˆa-zA-Z])[tT]he([ˆa-zA-Z]|$)/\\nThe process we just went through was based on fixing two kinds of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">errors: false positives, strings that we incorrectly matched like other or there, and false negafalse positives </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tives, strings that we incorrectly missed, like The. Addressing these two kinds of false negatives errors comes up </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">again and again in implementing speech and language processing systems.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.1.3 A Simple Example'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Reducing the overall error rate for an application thus involves two antagonistic efforts:\\n-</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Increasing precision (minimizing false positives)\\n- Increasing recall (minimizing false negatives)\\nWe'll come </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">back to precision and recall with more precise definitions in Chapter 4.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.1.3 A Simple Example'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Figure 2.8 shows some aliases for common ranges, which can be used mainly to save typing. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Besides the Kleene * and Kleene + we can also use explicit numbers as counters, by enclosing them in curly </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">brackets. The regular expression /{3}/ means \"exactly 3 occurrences of the previous character or expression\". So </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">/a.{24}z/ will match a followed by 24 dots followed by z (but not a followed by 23 or 25 dots followed by a z). | </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Regex                       | Expansion    |\\n|-----------------------------|--------------|\\n| \\\\d                </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">| [0-9]        |\\n| any digit                   | Party        |\\n| ␣                           |              |\\n|</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of                          |              |\\n| ␣                           |              |\\n| 5                  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|              |\\n| \\\\D                          | [ˆ0-9]       |\\n| any non-digit               | Blue         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ␣                           |              |\\n| moon                        |              |\\n| \\\\w           </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">| [a-zA-Z0-9_] |\\n| any alphanumeric/underscore | Daiyu        |\\n| \\\\W                          | [ˆ\\\\w]        </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| a non-alphanumeric          | !!!! |\\n| \\\\s                          | [␣\\\\r\\\\t\\\\n\\\\f]  |\\n| whitespace </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(space, tab)     | in Concord   |\\n| \\\\S                          | [ˆ\\\\s]        |\\n| Non-whitespace              </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">| in           |\\n| ␣                           |              |\\n| Concord                     |              |\\nA</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">range of numbers can also be specified. So /{n,m}/ specifies from n to m occurrences of the previous char or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">expression, and /{n,}/ means at least n occurrences of the previous expression. REs for counting are summarized in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Fig. 2.9. | Regex                                                       | Match   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n|-------------------------------------------------------------|---------|\\n| *                                  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|         |\\n| zero or more occurrences of the previous char or expression |         |\\n| +                        </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|         |\\n| one or more occurrences of the previous char or expression  |         |\\n| ? |         |\\n| zero or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">one occurrence of the previous char or expression   |         |\\n| {n}                                             </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|         |\\n| exactly                                                     | n       |\\n| {n,m}                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|         |\\n| from                                                        | n       |\\n| {n,}                     </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|         |\\n| at least                                                    | n       |\\n| {,m}                     </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|         |\\n| up to                                                       | m       |\\nFinally, certain special </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">characters are referred to by special notation based on the backslash () (see Fig. 2.10). The most common of these </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are the newline character newline\\n\\\\n and the tab character \\\\t. To refer to characters that are special </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">themselves (like\\n., , [, and ), precede them with a backslash, (i.e., /./, /*/, /[/, and /\\\\/). | Regex           </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">| Match                                  |\\n|-----------------|----------------------------------------|\\n| *      </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|                                        |\\n| an asterisk \"\" | \"KAPLA*N\"                          |\\n| . |         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| a period \".\"    | \"Dr.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.1.4 More Operators'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Livingston, I presume\"            |\\n| \\\\? |                                        |\\n| a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">question mark | \"Why don\\'t they come and lend a hand?\" |\\n| \\\\n              |                                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| a newline       |                                        |\\n| \\\\t              |                              </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| a tab           |                                        |'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.1.4 More Operators'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Let\\'s try out a more significant example of the power of REs. Suppose we want to build an </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">application to help a user buy a computer on the Web. The user might want \"any machine with at least 6 GHz and 500 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">GB of disk space for less than $1000\". To do this kind of retrieval, we first need to be able to look for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">expressions like 6\\nGHz or 500 GB or Mac or $999.99. In the rest of this section we\\'ll work out some simple </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">regular expressions for this task.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.1.5 A More Complex Example'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"First, let's complete our regular expression for prices. Here's a regular expression for a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dollar sign followed by a string of digits:\\n/$[0-9]+/\\nNote that the $ character has a different function here </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">than the end-of-line function we discussed earlier. Most regular expression parsers are smart enough to realize </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that $ here doesn't mean end-of-line. (As a thought experiment, think about how regex parsers might figure out the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">function of $ from the context.)\\nNow we just need to deal with fractions of dollars. We'll add a decimal point and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">two digits afterwards:\\n/$[0-9]+.[0-9][0-9]/\\nThis pattern only allows $199.99 but not $199. We need to make the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cents optional and to make sure we're at a word boundary:\\n/(ˆ|\\\\W)$[0-9]+(.[0-9][0-9])?\\\\b/\\nOne last catch! This </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pattern allows prices like $199999.99 which would be far too expensive! We need to limit the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dollars:\\n/(ˆ|\\\\W)$[0-9]{0,3}(.[0-9][0-9])?\\\\b/\\nFurther fixes (like avoiding matching a dollar sign with no price </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">after it) are left as an exercise for the reader. How about disk space?\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.1.5 A More Complex Example'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'We\\'ll need to allow for optional fractions again (5.5 GB);\\nnote the use of ? for making the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">final s optional, and the use of /␣/ to mean \"zero or more spaces\" since there might always be extra spaces lying </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">around:\\n/\\\\b[0-9]+(.[0-9]+)?␣(GB|[Gg]igabytes?)\\\\b/\\nModifying this regular expression so that it only matches </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">more than 500 GB is left as an exercise for the reader.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.1.5 A More Complex Example'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'An important use of regular expressions is in substitutions. For example, the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">substisubstitution\\ntution operator s/regexp1/pattern/ used in Python and in Unix commands like vim or sed allows a</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">string characterized by a regular expression to be replaced by another string:\\ns/colour/color/\\nIt is often useful</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to be able to refer to a particular subpart of the string matching the first pattern. For example, suppose we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">wanted to put angle brackets around all integers in a text, for example, changing the 35 boxes to the &lt;35&gt; boxes. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">We\\'d like a way to refer to the integer we\\'ve found so that we can easily add the brackets. To do this, we put </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">parentheses ( and ) around the first pattern and use the number operator \\\\1 in the second pattern to refer back. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Here\\'s how it looks:\\ns/([0-9]+)/&lt;\\\\1&gt;/\\nThe parenthesis and number operators can also specify that a certain </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">string or expression must occur twice in the text. For example, suppose we are looking for the pattern \"the Xer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">they were, the Xer they will be\", where we want to constrain the two X\\'s to be the same string. We do this by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">surrounding the first X with the parenthesis operator, and replacing the second X with the number operator \\\\1, as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">follows:\\n/the (.)er they were, the \\\\1er they will be/\\nHere the \\\\1 will be replaced by whatever string matched </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first item in parentheses. So this will match the bigger they were, the bigger they will be but not the bigger </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">they were, the faster they will be. This use of parentheses to store a pattern in memory is called a capture group.</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capture group\\nEvery time a capture group is used (i.e., parentheses surround a pattern), the resulting match is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">stored in a numbered register. If you match two different sets of\\nregister\\nparentheses, \\\\2 means whatever </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">matched the second capture group. Thus\\n/the (.)er they (.), the \\\\1er we \\\\2/\\nwill match the faster they ran, the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">faster we ran but not the faster they ran, the faster we ate. Similarly, the third capture group is stored in \\\\3, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the fourth is \\\\4, and so on. Parentheses thus have a double function in regular expressions; they are used to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">group terms for specifying the order in which operators should apply, and they are used to capture something in a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">register. Occasionally we might want to use parentheses for grouping, but don\\'t want to capture the resulting </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pattern in a register. In that case we use a non-capturing group, which is specified by putting the special </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">non-capturing group commands ?: after the open parenthesis, in the form (?: pattern ). /(?:some|a few) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(people|cats) like some \\\\1/\\nwill match some cats like some cats but not some cats like some some. Substitutions </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and capture groups are very useful in implementing simple chatbots like ELIZA (Weizenbaum, 1966). Recall that ELIZA</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">simulates a Rogerian psychologist by carrying on conversations like the following:\\nUser1:\\nMen are all alike. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ELIZA1: IN WHAT WAY\\nUser2:\\nThey\\'re always bugging us about something or other. ELIZA2: CAN YOU THINK OF A </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">SPECIFIC EXAMPLE User3:\\nWell, my boyfriend made me come here. ELIZA3: YOUR BOYFRIEND MADE YOU COME HERE User4:\\nHe</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">says I\\'m depressed much of the time. ELIZA4: I AM SORRY TO HEAR YOU ARE DEPRESSED\\nELIZA works by having a series </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">or cascade of regular expression substitutions each of which matches and changes some part of the input lines. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">After the input is uppercased, substitutions change all instances of MY to YOUR, and I\\'M to YOU\\nARE, and so on. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">That way when ELIZA repeats back part of the user utterance, it will seem to be referring correctly to the user. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The next set of substitutions matches and replaces other patterns in the input. Here are some examples:\\ns/. YOU </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ARE (depressed|sad) ./I AM SORRY TO HEAR YOU ARE \\\\1/\\ns/. YOU ARE (depressed|sad) ./WHY DO YOU THINK YOU ARE </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\1/\\ns/. all ./IN WHAT WAY/\\ns/. always .*/CAN YOU THINK OF A SPECIFIC EXAMPLE/\\nSince multiple substitutions can </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">apply to a given input, substitutions are assigned\\na rank and applied in order. Creating patterns is the topic of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Exercise 2.3, and we\\nreturn to the details of the ELIZA architecture in Chapter 15.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.1.6 Substitution, Capture Groups, And Eliza'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Finally, there will be times when we need to predict the future: look ahead in the text to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">see if some pattern matches, but not yet advance the pointer we always keep to where we are in the text, so that we</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">can then deal with the pattern if it occurs, but if it doesn\\'t we can check for something else instead. These </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lookahead assertions make use of the (? syntax that we saw in the previlookahead ous section for non-capture </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">groups. The operator (?= pattern) is true if pattern occurs, but is zero-width, i.e. the match pointer doesn\\'t </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">advance. The operator zero-width\\n(?! pattern) only returns true if a pattern does not match, but again is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">zero-width and doesn\\'t advance the pointer. Negative lookahead is commonly used when we are parsing some complex </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pattern but want to rule out a special case. For example suppose we want to match, at the beginning of a line, any </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">single word that doesn\\'t start with \"Volcano\". We can use negative lookahead to do </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this:\\n/ˆ(?!Volcano)[A-Za-z]+/'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.1.7 Lookahead Assertions'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Before we talk about processing words, we need to decide what counts as a word. Let's start </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">by looking at one particular corpus (plural corpora), a computer-readable corpus corpora collection of text or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">speech. For example the Brown corpus is a million-word collection of samples from 500 written English texts from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">different genres (newspaper, fiction, non-fiction, academic, etc.), assembled at Brown University in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1963–64\\n(Kuˇcera and Francis, 1967). How many words are in the following Brown sentence?\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.2 Words'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"He stepped out into the hall, was delighted to encounter a water brother. This sentence has </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">13 words if we don't count punctuation marks as words, 15\\nif we count punctuation.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.2 Words'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Whether we treat period (\".\"), comma (\",\"), and so on as words depends on the task. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Punctuation is critical for finding boundaries of things (commas, periods, colons) and for identifying some aspects</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of meaning (question marks, exclamation marks, quotation marks). For some tasks, like part-of-speech tagging or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">parsing or speech synthesis, we sometimes treat punctuation marks as if they were separate words. The Switchboard </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">corpus of American English telephone conversations between strangers was collected in the early 1990s; it contains </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2430 conversations averaging 6 minutes each, totaling 240 hours of speech and about 3 million words (Godfrey et </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">al., 1992). Such corpora of spoken language introduce other complications with regard to defining words. Let\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">look at one utterance from Switchboard; an utterance is the spoken correlate of a sentence:\\nutterance I do uh </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">main- mainly business data processing This utterance has two kinds of disfluencies. The broken-off word main- is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">disfluency called a fragment. Words like uh and um are called fillers or filled pauses. Should fragment filled </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pause we consider these to be words? Again, it depends on the application. If we are building a speech </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">transcription system, we might want to eventually strip out the disfluencies. But we also sometimes keep </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">disfluencies around. Disfluencies like uh or um are actually helpful in speech recognition in predicting the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">upcoming word, because they may signal that the speaker is restarting the clause or idea, and so for speech </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">recognition they are treated as regular words. Because people use different disfluencies they can also be a cue to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">speaker identification. In fact Clark and Fox Tree (2002) showed that uh and um have different meanings. What do </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you think they are?'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.2 Words'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Perhaps most important, in thinking about what is a word, we need to distinguish two ways of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">talking about words that will be useful throughout the book. Word types word type\\nare the number of distinct words</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in a corpus; if the set of words in the vocabulary\\nis V, the number of types is the vocabulary size |V|. Word </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">instances are the total\\nword instance\\nnumber N of running words.1\\nIf we ignore punctuation, the following Brown </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sentence has 16 instances and 14\\ntypes:\\nThey picnicked by the pool, then lay back on the grass and looked at the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">stars. We still have decisions to make!'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.2 Words'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"For example, should we consider a capitalized string (like They) and one that is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">uncapitalized (like they) to be the same word type? The answer is that it depends on the task! They and they might </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">be lumped together as the same type in some tasks, like speech recognition, where we might just care about getting </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the words in order and don't care about the formatting, while for other tasks, such as deciding whether a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">particular word is a noun or verb (part-of-speech tagging) or whether a word is a name of a person or location </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(named-entity tagging), capitalization is a useful feature and is retained. Sometimes we keep around two versions </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of a particular NLP model, one with capitalization and one without capitalization. How many words are there in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">English? When we speak about the number of words in the language, we are generally referring to word types. Fig. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2.11 shows the rough numbers of types and instances computed from some English corpora. | Corpus                   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">| Instances =              |\\n|-------------------------------------|--------------------------|\\n| N              </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|                          |\\n| Types =                             |                          |\\n| |              </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|                          |\\n| V                                   |                          |\\n| |              </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|                          |\\n| Shakespeare                         | 884 thousand 31 thousand |\\n| Brown corpus   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">| 1 million 38 thousand    |\\n| Switchboard telephone conversations | 2.4 million 20 thousand  |\\n| COCA           </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">| 440 million              |\\n| Google n-grams                      | 1 trillion               |\\nThe larger the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">corpora we look at, the more word types we find, and in fact this relationship between the number of types |V| and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">number of instances N is called Herdan's Law (Herdan, 1960) or Heaps' Law (Heaps, 1978) after its discoverers </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Herdan's Law Heaps' Law\\n(in linguistics and information retrieval respectively). It is shown in Eq.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.2 Words'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"2.1, where k and β are positive constants, and 0 &lt; β &lt; 1. $|V|=kN^{\\\\beta}$ (2.1)\\nThe value </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of β depends on the corpus size and the genre, but at least for the large corpora in Fig. 2.11, β ranges from .67 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to .75. Roughly then we can say that the vocabulary size for a text goes up significantly faster than the square </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">root of its length in words. It's sometimes useful to make a further distinction.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.2 Words'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Consider inflected forms like cats versus cat. We say these two words are different wordforms</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">but have the same lemma. A lemma is a set of lexical forms having the same stem, the same lemma major </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">part-of-speech, and the same word sense. The wordform is the full inflected wordform or derived form of the word. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The two wordforms cat and cats thus have the same lemma, which we can represent as cat. For morphologically complex</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">languages like Arabic, we often need to deal with lemmatization. For most tasks in English, however, wordforms are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sufficient, and when we talk about words in this book we almost always mean wordsforms (although we will discuss </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">basic algorithms for lemmatization and the related task of stemming below in Section 2.6). One of the situations </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">even in English where we talk about lemmas is when we measure the number of words in a dictionary. Dictionary </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entries or boldface forms are a very rough approximation to (an upper bound on) the number of lemmas (since some </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lemmas have multiple boldface forms). The 1989 edition of the Oxford English Dictionary had 615,000 entries. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Finally, we should note that in practice, for many NLP applications (for example for neural language modeling) we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">don't actually use words as our internal unit of representation at all! We instead tokenize the input strings into </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tokens, which can be words but can also be only parts of words. We'll return to this tokenization question when we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">introduce the BPE algorithm in Section 2.5.2.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.2 Words'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Words don't appear out of nowhere. Any particular piece of text that we study is produced by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">one or more specific speakers or writers, in a specific dialect of a specific language, at a specific time, in a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">specific place, for a specific function. Perhaps the most important dimension of variation is the language. NLP </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">algorithms are most useful when they apply across many languages. The world has 7097\\nlanguages at the time of this</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">writing, according to the online Ethnologue catalog\\n(Simons and Fennig, 2018). It is important to test algorithms </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on more than one language, and particularly on languages with different properties; by contrast there is an </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">unfortunate current tendency for NLP algorithms to be developed or tested just on English (Bender, 2019). Even when</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">algorithms are developed beyond English, they tend to be developed for the official languages of large </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">industrialized nations (Chinese, Spanish, Japanese, German etc.), but we don't want to limit tools to just these </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">few languages. Furthermore, most languages also have multiple varieties, often spoken in different regions or by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">different social groups. Thus, for example, if we're processing text that uses features of African American English</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(AAE) or AAE\\nAfrican American Vernacular English (AAVE)—the variations of English used by millions of people in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">African American communities (King 2020)—we must use\\nNLP tools that function with features of those varieties. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Twitter posts might use features often used by speakers of African American English, such as constructions </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">like\\niont (I don't in Mainstream American English (MAE)), or talmbout corresponding\\nMAE\\nto MAE talking about, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">both examples that influence word segmentation (Blodgett et al. 2016, Jones 2015).\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.3 Corpora'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"It's also quite common for speakers or writers to use multiple languages in a\\nsingle </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">communicative act, a phenomenon called code switching. Code switching\\ncode switching\\nis enormously common across </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the world; here are examples showing Spanish and (transliterated) Hindi code switching with English (Solorio et al.</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2014, Jurgens et al.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.3 Corpora'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2017): (2.2)\\nPor primera vez veo a @username actually being hateful! it was </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">beautiful:)\\n[For the first time I get to see @username actually being hateful! it was beautiful:) ]\\n(2.3)\\ndost </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tha or ra- hega ... dont wory ... but dherya rakhe\\n[\"he was and will remain a friend ... don\\'t worry ...'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.3 Corpora'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'but have faith\"]\\nAnother dimension of variation is the genre. The text that our algorithms </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">must process might come from newswire, fiction or non-fiction books, scientific articles, Wikipedia, or religious </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">texts. It might come from spoken genres like telephone conversations, business meetings, police body-worn cameras, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">medical interviews, or transcripts of television shows or movies. It might come from work situations like doctors\\'</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">notes, legal text, or parliamentary or congressional proceedings. Text also reflects the demographic </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">characteristics of the writer (or speaker): their age, gender, race, socioeconomic class can all influence the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">linguistic properties of the text we are processing. And finally, time matters too. Language changes over time, and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for some languages we have good corpora of texts from different historical periods. Because language is so </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">situated, when developing computational models for language processing from a corpus, it\\'s important to consider </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">who produced the language, in what context, for what purpose. How can a user of a dataset know all these details? </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The best way is for the corpus creator to build a datasheet (Gebru et al., datasheet\\n2020) or data statement </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(Bender et al., 2021) for each corpus. A datasheet specifies properties of a dataset like:\\nMotivation: Why was the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">corpus collected, by whom, and who funded it? Situation: When and in what situation was the text written/spoken?'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.3 Corpora'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"For example,\\nwas there a task? Was the language originally spoken conversation, edited text,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">social media communication, monologue vs. dialogue? Language variety: What language (including dialect/region) was </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the corpus in? Speaker demographics: What was, e.g., the age or gender of the text's authors? Collection process: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">How big is the data? If it is a subsample how was it sampled? Was the data collected with consent? How was the data</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pre-processed, and what metadata is available? Annotation process: What are the annotations, what are the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">demographics of the\\nannotators, how were they trained, how was the data annotated? Distribution: Are there </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">copyright or other intellectual property restrictions?\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.3 Corpora'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Before almost any natural language processing of a text, the text has to be normalized, a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">task called text normalization. At least three tasks are commonly applied as text normalization part of any </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">normalization process:\\n1. Tokenizing (segmenting) words 2. Normalizing word formats 3. Segmenting sentences\\nIn </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the next sections we walk through each of these tasks, but we'll first start with an easy, if somewhat naive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">version of word tokenization and normalization (and frequency computation) that can be accomplished for English </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">solely in a single UNIX command-line, inspired by Church (1994). We'll make use of some Unix commands: tr, used to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">systematically change particular characters in the input; sort, which sorts input lines in alphabetical order; and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">uniq, which collapses and counts adjacent identical lines. For example let's begin with the 'complete words' of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Shakespeare in one file, sh.txt. We can use tr to tokenize the words by changing every sequence of nonalphabetic </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">characters to a newline ('A-Za-z' means alphabetic and the -c option complements to non-alphabet, so together they </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mean to change every non-alphabetic character into a newline. The -s ('squeeze') option is used to replace the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">result of multiple consecutive changes into a single output, so a series of non-alphabetic characters in a row </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">would all be 'squeezed' into a single newline):\\ntr -sc 'A-Za-z' '\\\\n' &lt; sh.txt\\nThe output of this command will </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">be:\\nTHE\\nSONNETS\\nby\\nWilliam\\nShakespeare\\nFrom\\nfairest\\ncreatures\\nWe\\n... Now that there is one word per line,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">we can sort the lines, and pass them to uniq\\n-c which will collapse and count them:\\ntr -sc 'A-Za-z' '\\\\n' &lt; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sh.txt | sort | uniq -c\\nwith the following output:\\n1945 A\\n72 AARON\\n19 ABBESS\\n25 Aaron\\n6 Abate\\n1 Abates\\n5 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Abbess\\n6 Abbey\\n3 Abbot\\n... Alternatively, we can collapse all the upper case to lower case:\\ntr -sc 'A-Za-z' </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'\\\\n' &lt; sh.txt | tr A-Z a-z | sort | uniq -c\\nwhose output is\\n14725 a\\n97 aaron\\n1 abaissiez\\n10 abandon\\n2 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">abandoned\\n2 abase\\n1 abash\\n14 abate\\n3 abated\\n3 abatement\\n... Now we can sort again to find the frequent words.</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The -n option to sort means\\nto sort numerically rather than alphabetically, and the -r option means to sort </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in\\nreverse order (highest-to-lowest):\\ntr -sc 'A-Za-z' '\\\\n' &lt; sh.txt | tr A-Z a-z | sort | uniq -c | sort -n </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">-r\\nThe results show that the most frequent words in Shakespeare, as in any other\\ncorpus, are the short function </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">words like articles, pronouns, prepositions:\\n27378 the 26084 and 22538 i 19771 to 17481 of 14725 a 13826 you 12489</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">my 11318 that 11112 in\\n... Unix tools of this sort can be very handy in building quick word count statistics for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">any corpus in English.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.4 Simple Unix Tools For Word Tokenization'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'While in some versions of Unix these command-line tools also correctly handle Unicode </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">characters and so can be used for many languages, in general for handling most languages outside English we use </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">more sophisticated tokenization algorithms.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.4 Simple Unix Tools For Word Tokenization'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The simple UNIX tools above were fine for getting rough word statistics but more </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sophisticated algorithms are generally necessary for tokenization, the task of segtokenization menting running text</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">into words. There are roughly two classes of tokenization algorithms. In top-down tokenization, we define a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">standard and implement rules to implement that kind of tokenization. In bottom-up tokenization, we use simple </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">statistics of letter sequences to break up words into subword tokens.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.5 Word Tokenization'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'While the Unix command sequence just removed all the numbers and punctuation, for most NLP </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">applications we\\'ll need to keep these in our tokenization. We often want to break off punctuation as a separate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">token; commas are a useful piece of information for parsers, periods help indicate sentence boundaries. But we\\'ll </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">often want to keep the punctuation that occurs word internally, in examples like m.p.h., Ph.D., AT&amp;T, and cap\\'n. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Special characters and numbers will need to be kept in prices ($45.55) and dates (01/02/06); we don\\'t want to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">segment that price into separate tokens of \"45\" and \"55\". And there are URLs (https://www.stanford.edu), Twitter </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hashtags (#nlproc), or email addresses (someone@cs.colorado.edu). Number expressions introduce other complications </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">as well; while commas normally appear at word boundaries, commas are used inside numbers in English, every three </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">digits: 555,500.50. Languages, and hence tokenization requirements, differ on this; many continental European </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">languages like Spanish, French, and German, by contrast, use a comma to mark the decimal point, and spaces (or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sometimes periods) where English puts commas, for example, 555 500,50. A tokenizer can also be used to expand </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">clitic contractions that are marked by clitic apostrophes, for example, converting what\\'re to the two tokens what </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are, and we\\'re to we are. A clitic is a part of a word that can\\'t stand on its own, and can only occur when it is</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attached to another word. Some such contractions occur in other alphabetic languages, including articles and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pronouns in French (j\\'ai, l\\'homme). Depending on the application, tokenization algorithms may also tokenize </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">multiword expressions like New York or rock \\'n\\' roll as a single token, which requires a multiword expression </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dictionary of some sort. Tokenization is thus intimately tied up with named entity recognition, the task of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">detecting names, dates, and organizations (Chapter 8). One commonly used tokenization standard is known as the Penn</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Treebank tokenization standard, used for the parsed corpora (treebanks) released by the Lin-\\nPenn Treebank guistic</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Data Consortium (LDC), the source of many useful datasets. This standard separates out clitics (doesn\\'t becomes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">does plus n\\'t), keeps hyphenated words together, and separates out all punctuation (to save space we\\'re showing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">visible spaces\\n\\' \\' between tokens, although newlines is a more common output):\\nInput:\\n\"The San Francisco-based</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">restaurant,\" they said,\\n\"doesn\\'t charge $10\". Output: \" The San Francisco-based restaurant , \" they said ,\\n\" </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">does n\\'t charge $ 10 \" .'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.5.1 Top-Down (Rule-Based) Tokenization'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'In practice, since tokenization needs to be run before any other language processing, it </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">needs to be very fast.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.5.1 Top-Down (Rule-Based) Tokenization'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The standard method for tokenization is therefore to use deterministic algorithms based on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">regular expressions compiled into very efficient finite state automata. For example, Fig. 2.12 shows an example of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a basic regular expression that can be used to tokenize English with the nltk.regexp tokenize function of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Python-based Natural Language Toolkit (NLTK) (Bird et al. 2009; https://www.nltk.org).'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.5.1 Top-Down (Rule-Based) Tokenization'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"text = 'That U.S.A. poster-print costs $12.40...'\\npattern = r'''(?x)\\n\\n\\n\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.5.1 Top-Down (Rule-Based) Tokenization'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'... (?:[A-Z].)+'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'set flag to allow verbose regexps'</span><span style=\"font-weight: bold\">})</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'... | \\\\w+(?:-\\\\w+)*'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'abbreviations, e.g. U.S.A.'</span><span style=\"font-weight: bold\">})</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'... | \\\\$?\\\\d+(?:.\\\\d+)?%?'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'words with optional internal hyphens'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'... | ...'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'currency, percentages, e.g. $12.40, 82%'</span><span style=\"font-weight: bold\">})</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'... | [][.,;\"\\'?():_`-]'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'ellipsis'</span><span style=\"font-weight: bold\">})</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"... '''\\n\\n\\n\\nnltk.regexp_tokenize(text, pattern)\\n['That', 'U.S.A.', 'poster-print', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'costs', '$12.40', '...']\\nCarefully designed deterministic algorithms can deal with the ambiguities that arise, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">such as the fact that the apostrophe needs to be tokenized differently when used as a genitive marker (as in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">book's cover), a quotative as in 'The other class', she said, or in clitics like they're. Word tokenization is more</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">complex in languages like written Chinese, Japanese, and Thai, which do not use spaces to mark potential </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">word-boundaries.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'these are separate tokens; includes ], ['</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'In Chinese, for example, words are composed of characters (called hanzi in Chinese). Each </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hanzi\\ncharacter generally represents a single unit of meaning (called a morpheme) and is\\npronounceable as a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">single syllable. Words are about 2.4 characters long on average. But deciding what counts as a word in Chinese is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">complex. For example, consider the following sentence: (2.4)\\n姚明进入总决赛\\ny´ao m´ıng jın ru zˇong ju´e </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">s`ai\\n\"Yao Ming reaches the finals\"\\nAs Chen et al. (2017) point out, this could be treated as 3 words (\\'Chinese </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Treebank\\' segmentation):\\n(2.5)\\n姚明\\nYaoMing\\n进入\\nreaches\\n总决赛\\nfinals\\nor as 5 words (\\'Peking </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">University\\' segmentation):\\n(2.6)\\n姚\\nYao\\n明\\nMing\\n进入\\nreaches\\n总\\noverall\\n决赛\\nfinals\\nFinally, it is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">possible in Chinese simply to ignore words altogether and use characters as the basic elements, treating the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sentence as a series of 7 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">characters:\\n(2.7)\\n姚\\nYao\\n明\\nMing\\n进\\nenter\\n入\\nenter\\n总\\noverall\\n决\\ndecision\\n赛\\ngame\\nIn fact, for most</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Chinese NLP tasks it turns out to work better to take characters rather than words as input, since characters are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at a reasonable semantic level for most applications, and since most word standards, by contrast, result in a huge </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">vocabulary with large numbers of very rare words (Li et al., 2019). However, for Japanese and Thai the character is</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">too small a unit, and so algorithms for word segmentation are required. These can also be useful for Chinese word </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">segmentation in the rare situations where word rather than character boundaries are required. The standard </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">segmentation algorithms for these languages use neural sequence models trained via supervised machine learning on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hand-segmented training sets; we\\'ll introduce sequence models in Chapter 8 and Chapter 9. '</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'these are separate tokens; includes ], ['</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"There is a third option to tokenizing text, one that is most commonly used by large language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models. Instead of defining tokens as words (whether delimited by spaces or more complex algorithms), or as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">characters (as in Chinese), we can use our data to automatically tell us what the tokens should be. This is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">especially useful in dealing with unknown words, an important problem in language processing. As we will see in the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">next chapter, NLP algorithms often learn some facts about language from one corpus (a training corpus) and then use</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">these facts to make decisions about a separate test corpus and its language. Thus if our training corpus contains, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">say the words low, new, newer, but not lower, then if the word lower appears in our test corpus, our system will </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">not know what to do with it. To deal with this unknown word problem, modern tokenizers automatically induce sets of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tokens that include tokens smaller than words, called subwords. Subsubwords words can be arbitrary substrings, or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">they can be meaning-bearing units like the morphemes -est or -er. (A morpheme is the smallest meaning-bearing unit </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of a language; for example the word unlikeliest has the morphemes un-, likely, and -est.)\\nIn modern tokenization </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">schemes, most tokens are words, but some tokens are frequently occurring morphemes or other subwords like -er. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Every unseen word like lower can thus be represented by some sequence of known subword units, such as low and er, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">or even as a sequence of individual letters if necessary. Most tokenization schemes have two parts: a token </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">learner, and a token segmenter. The token learner takes a raw training corpus (sometimes roughly preseparated into </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">words, for example by whitespace) and induces a vocabulary, a set of tokens. The token segmenter takes a raw test </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sentence and segments it into the tokens in the vocabulary. Two algorithms are widely used: byte-pair </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">encoding\\n(Sennrich et al., 2016), and unigram language modeling (Kudo, 2018), There is also a SentencePiece </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">library that includes implementations of both of these (Kudo and Richardson, 2018), and people often use the name </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">SentencePiece to simply mean unigram language modeling tokenization. In this section we introduce the simplest of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the three, the byte-pair encoding or BPE algorithm (Sennrich et al., 2016); see Fig. 2.13. The BPE token learner </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">begins BPE\\nwith a vocabulary that is just the set of all individual characters. It then examines the training </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">corpus, chooses the two symbols that are most frequently adjacent (say 'A', 'B'), adds a new merged symbol 'AB' to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the vocabulary, and replaces every adjacent 'A' 'B' in the corpus with the new 'AB'. It continues to count and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">merge, creating new longer and longer character strings, until k merges have been done creating\\nk novel tokens; k </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is thus a parameter of the algorithm. The resulting vocabulary\\nconsists of the original set of characters plus k </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">new symbols. The algorithm is usually run inside words (not merging across word boundaries),\\nso the input corpus </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is first white-space-separated to give a set of strings, each corresponding to the characters of a word, plus a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">special end-of-word symbol\\n, and its\\ncounts. Let's see its operation on the following tiny input corpus of 18 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">word tokens with counts for each word (the word low appears 5 times, the word newer 6 times,\\nand so on), which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">would have a starting vocabulary of 11 letters:\\n| corpus      | vocabulary                     </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n|-------------|--------------------------------|\\n| 5           |                                |\\n| l o w     </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">| , d, e, i, l, n, o, r, s, t, w |\\n| 2           |                                |\\n| l o w e s t |              </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| 6           |                                |\\n| n e w e r   |                                |\\n| 3         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|                                |\\n| w i d e r   |                                |\\n| 2           |              </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| n e w       |                                |\\nThe BPE algorithm first counts all pairs of adjacent symbols: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the most frequent is the pair e r because it occurs in newer (frequency of 6) and wider (frequency of\\n3) for a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">total of 9 occurrences.2 We then merge these symbols, treating er as one symbol, and count again:\\n| corpus      | </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">vocabulary                         |\\n|-------------|------------------------------------|\\n| 5           |        </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| l o w       | , d, e, i, l, n, o, r, s, t, w, er |\\n| 2           |                                    |\\n| l </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">o w e s t |                                    |\\n| 6           |                                    |\\n| n e w er </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|                                    |\\n| 3           |                                    |\\n| w i d er    |      </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| 2           |                                    |\\n| n e w       |                                    |\\nNow </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the most frequent pair is er\\n, which we merge; our system has learned that there should be a token for word-final </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">er, represented as er :\\n| corpus      |\\n|-------------|\\n| 5           |\\n| l o w       |\\n| ,           |\\n| d  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,           |\\n| e           |\\n| ,           |\\n| i           |\\n| ,           |\\n| l           |\\n| ,       </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| n           |\\n| ,           |\\n| o           |\\n| ,           |\\n| r           |\\n| ,           |\\n| s       </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,           |\\n| t           |\\n| ,           |\\n| w           |\\n| ,           |\\n| er          |\\n| ,       </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| er          |\\n| 2           |\\n| l o w e s t |\\n| 6           |\\n| n e w er    |\\n| 3           |\\n| w i d er</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| 2           |\\n| n e w       |\\nNext n e (total count of 8) get merged to ne:\\n| corpus      </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n|-------------|\\n| 5           |\\n| l o w       |\\n| ,           |\\n| d           |\\n| ,           |\\n| e       </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,           |\\n| i           |\\n| ,           |\\n| l           |\\n| ,           |\\n| n           |\\n| ,       </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| o           |\\n| ,           |\\n| r           |\\n| ,           |\\n| s           |\\n| ,           |\\n| t       </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,           |\\n| w           |\\n| ,           |\\n| er          |\\n| ,           |\\n| er          |\\n| ,       </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ne          |\\n| 2           |\\n| l o w e s t |\\n| 6           |\\n| ne w er     |\\n| 3           |\\n| w i d er</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| 2           |\\n| ne w        |\\nIf we continue, the next merges are:\\n| merge      | current vocabulary   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n|------------|----------------------|\\n| (ne, w)    |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| d          |                      |\\n| ,          |                      |\\n| e          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| i          |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| l          |                      |\\n| ,          |                      |\\n| n          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| o          |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| r          |                      |\\n| ,          |                      |\\n| s          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| t          |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| w          |                      |\\n| ,          |                      |\\n| er         |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| er         |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ne         |                      |\\n| ,          |                      |\\n| new        |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| (l, o)     |                      |\\n| ,          |                      |\\n| d          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| e          |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| i          |                      |\\n| ,          |                      |\\n| l          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| n          |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| o          |                      |\\n| ,          |                      |\\n| r          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| s          |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| t          |                      |\\n| ,          |                      |\\n| w          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| er         |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| er         |                      |\\n| ,          |                      |\\n| ne         |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| new        |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| lo         |                      |\\n| (lo, w)    |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| d          |                      |\\n| ,          |                      |\\n| e          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| i          |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| l          |                      |\\n| ,          |                      |\\n| n          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| o          |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| r          |                      |\\n| ,          |                      |\\n| s          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| t          |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| w          |                      |\\n| ,          |                      |\\n| er         |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| er         |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ne         |                      |\\n| ,          |                      |\\n| new        |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| lo         |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| low        |                      |\\n| (new, er ) |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| d          |                      |\\n| ,          |                      |\\n| e          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| i          |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| l          |                      |\\n| ,          |                      |\\n| n          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| o          |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| r          |                      |\\n| ,          |                      |\\n| s          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| t          |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| w          |                      |\\n| ,          |                      |\\n| er         |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| er         |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ne         |                      |\\n| ,          |                      |\\n| new        |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| lo         |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| low        |                      |\\n| ,          |                      |\\n| newer      |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| (low,      | )                    |\\n| ,          |                      |\\n| d          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| e          |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| i          |                      |\\n| ,          |                      |\\n| l          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| n          |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| o          |                      |\\n| ,          |                      |\\n| r          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| s          |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| t          |                      |\\n| ,          |                      |\\n| w          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| er         |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| er         |                      |\\n| ,          |                      |\\n| ne         |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| new        |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| lo         |                      |\\n| ,          |                      |\\n| low        |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| ,          |                      |\\n| newer      |                      |\\n| ,          |                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| low        |                      |\\nOnce we've learned our vocabulary, the token segmenter is used to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tokenize a test sentence. The token segmenter just runs on the test data the merges we have learned from the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">training data, greedily, in the order we learned them.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'these are separate tokens; includes ], ['</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.5.2 Byte-Pair Encoding: A Bottom-Up Tokenization Algorithm'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"(Thus the frequencies in the test data don't play a role, just the frequencies in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">training data). So first we segment each test sentence word into characters.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'these are separate tokens; includes ], ['</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.5.2 Byte-Pair Encoding: A Bottom-Up Tokenization Algorithm'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Then we apply the first rule: replace every instance of e r in the test corpus with er, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">then the second rule: replace every instance of er in the test corpus with er , and so on. function BYTE-PAIR </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ENCODING(strings C, number of merges k) returns vocab V\\nV←all unique characters in C'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'these are separate tokens; includes ], ['</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.5.2 Byte-Pair Encoding: A Bottom-Up Tokenization Algorithm'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'tNEW ←tL + tR'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'merge tokens k times tL, tR ←Most frequent pair of adjacent tokens in C'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'By the end, if the test corpus contained the character sequence n e w e r\\n, it would be </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tokenized as a full word. But the characters of a new (unknown) word like l o w e r would be merged into the two </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tokens low er . Of course in real settings BPE is run with many thousands of merges on a very large input corpus. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The result is that most words will be represented as full symbols, and only the very rare words (and unknown words)</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">will have to be represented by their parts.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'and update the corpus return V'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Word normalization is the task of putting words/tokens in a standard format. The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">normalization simplest case of word normalization is case folding. Mapping everything to lower case folding case </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">means that Woodchuck and woodchuck are represented identically, which is very helpful for generalization in many </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tasks, such as information retrieval or speech recognition. For sentiment analysis and other text classification </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tasks, information extraction, and machine translation, by contrast, case can be quite helpful and case folding is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generally not done. This is because maintaining the difference between, for example, US the country and us the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pronoun can outweigh the advantage in generalization that case folding would have provided for other words. Systems</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that use BPE or other kinds of bottom-up tokenization may do no further word normalization. In other NLP systems, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">we may want to do further normalizations, like choosing a single normal form for words with multiple forms like USA</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and US or uh-huh and uhhuh. This standardization may be valuable, despite the spelling information that is lost in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the normalization process. For information retrieval or information extraction about the US, we might want to see </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information from documents whether they mention the US or the USA.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'and update the corpus return V'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.6 Word Normalization, Lemmatization And Stemming'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'For other natural language processing situations we also want two morphologically different </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">forms of a word to behave similarly. For example in web search, someone may type the string woodchucks but a useful</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">system might want to also return pages that mention woodchuck with no s. This is especially common in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">morphologically complex languages like Polish, where for example the word Warsaw has different endings when it is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the subject (Warszawa), or after a preposition like \"in Warsaw\" (w Warszawie), or \"to Warsaw\" (do Warszawy), and so</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on. Lemmatization is the task lemmatization of determining that two words have the same root, despite their surface</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">differences. The words am, are, and is have the shared lemma be; the words dinner and dinners both have the lemma </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dinner. Lemmatizing each of these forms to the same lemma will let us find all mentions of words in Polish like </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Warsaw. The lemmatized form of a sentence like He is reading detective stories would thus be He be read detective </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">story. How is lemmatization done?'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'and update the corpus return V'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.6.1 Lemmatization'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The most sophisticated methods for lemmatization involve complete morphological parsing of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the word. Morphology is the study of the way words are built up from smaller meaning-bearing units called </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">morphemes. morpheme Two broad classes of morphemes can be distinguished: stems—the central morstem pheme of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">word, supplying the main meaning—and affixes—adding \"additional\"\\naffix meanings of various kinds. So, for example,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the word fox consists of one morpheme\\n(the morpheme fox) and the word cats consists of two: the morpheme cat and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the morpheme -s. A morphological parser takes a word like cats and parses it into the two morphemes cat and s, or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">parses a Spanish word like amaren (\\'if in the future they would love\\') into the morpheme amar \\'to love\\', and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the morphological features\\n3PL and future subjunctive.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'and update the corpus return V'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.6.1 Lemmatization'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Lemmatization algorithms can be complex. For this reason we sometimes make use of a simpler </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">but cruder method, which mainly consists of chopping off wordfinal affixes. This naive version of morphological </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">analysis is called stemming. For stemming example, the Porter stemmer, a widely used stemming algorithm (Porter, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1980), Porter stemmer when applied to the following paragraph:\\nThis was not the map we found in Billy Bones's </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">chest, but an accurate copy, complete in all things-names and heights and soundings-with the single exception of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the red crosses and the written notes. produces the following stemmed output:\\nThi wa not the map we found in Billi</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Bone s chest but an accur copi complet in all thing name and height and sound with the singl except of the red </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cross and the written note The algorithm is based on series of rewrite rules run in series: the output of each pass</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is fed as input to the next pass. Here are some sample rules (more details can be found at </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://tartarus.org/martin/PorterStemmer/):\\nATIONAL → ATE (e.g., relational → relate)\\nING → ϵ\\nif the stem </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">contains a vowel (e.g., motoring → motor)\\nSSES → SS\\n(e.g., grasses → grass)\\nSimple stemmers can be useful in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cases where we need to collapse across different variants of the same lemma. Nonetheless, they do tend to commit </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">errors of both over- and under-generalizing, as shown in the table below (Krovetz, 1993):\\n| Errors of Commission  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">| Errors of Omission   |\\n|------------------------|----------------------|\\n| organization organ     | European </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Europe      |\\n| doing                  | doe                  |\\n| numerical              | numerous             </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| policy                 | police               |\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'and update the corpus return V'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Stemming: The Porter Stemmer'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Sentence segmentation is another important step in text processing. The most usesentence </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">segmentation ful cues for segmenting a text into sentences are punctuation, like periods, question marks, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">exclamation points. Question marks and exclamation points are relatively unambiguous markers of sentence </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">boundaries.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'and update the corpus return V'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.7 Sentence Segmentation'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Periods, on the other hand, are more ambiguous. The period character \".\" is ambiguous between</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a sentence boundary marker and a marker of abbreviations like Mr. or Inc. The previous sentence that you just read </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">showed an even more complex case of this ambiguity, in which the final period of Inc. marked both an abbreviation </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and the sentence boundary marker. For this reason, sentence tokenization and word tokenization may be addressed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">jointly. In general, sentence tokenization methods work by first deciding (based on rules or machine learning) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">whether a period is part of the word or is a sentence-boundary marker. An abbreviation dictionary can help </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">determine whether the period is part of a commonly used abbreviation; the dictionaries can be hand-built or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">machinelearned (Kiss and Strunk, 2006), as can the final sentence splitter. In the Stanford CoreNLP toolkit </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(Manning et al., 2014), for example sentence splitting is rule-based, a deterministic consequence of tokenization; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a sentence ends when a sentence-ending punctuation (., !, or ?) is not already grouped with other characters into a</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">token (such as for an abbreviation or number), optionally followed by additional final quotes or brackets.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'and update the corpus return V'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.7 Sentence Segmentation'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Much of natural language processing is concerned with measuring how similar two strings are. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">For example in spelling correction, the user typed some erroneous string—let's say graffe–and we want to know what </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the user meant. The user probably intended a word that is similar to graffe. Among candidate similar words, the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">word giraffe, which differs by only one letter from graffe, seems intuitively to be more similar than, say grail or</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">graf, which differ in more letters. Another example comes from coreference, the task of deciding whether two </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">strings such as the following refer to the same entity:\\nStanford President Marc Tessier-Lavigne Stanford </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">University President Marc Tessier-Lavigne Again, the fact that these two strings are very similar (differing by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">only one word) seems like useful evidence for deciding that they might be coreferent. Edit distance gives us a way </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to quantify both of these intuitions about string similarity. More formally, the minimum edit distance between two </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">strings is defined minimum edit distance as the minimum number of editing operations (operations like insertion, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">deletion, substitution) needed to transform one string into another. The gap between intention and execution, for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">example, is 5 (delete an i, substitute e for n, substitute x for t, insert c, substitute u for n). It's much easier</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to see this by looking at the most important visualization for string distances, an alignment alignment between the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">two strings, shown in Fig. 2.14. Given two sequences, an alignment is a correspondence between substrings of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">two sequences. Thus, we say I aligns with the empty string, N with E, and so on. Beneath the aligned strings is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">another representation; a series of symbols expressing an operation list for converting the top string into the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">bottom string: d for deletion, s for substitution, i for insertion. We can also assign a particular cost or weight </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to each of these operations. The Levenshtein distance between two sequences is the simplest weighting factor in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">which each of the three operations has a cost of 1 (Levenshtein, 1966)—we assume that the substitution of a letter </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for itself, for example, t for t, has zero cost. The Levenshtein distance between intention and execution is 5. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Levenshtein also proposed an alternative version of his metric in which each insertion or deletion has a cost of 1 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and substitutions are not allowed. (This is equivalent to allowing substitution, but giving each substitution a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cost of 2 since any substitution can be represented by one insertion and one deletion). Using this version, the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Levenshtein distance between intention and execution is 8.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'and update the corpus return V'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.8 Minimum Edit Distance'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"How do we find the minimum edit distance? We can think of this as a search task, in which we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are searching for the shortest path—a sequence of edits—from one string to another. i n t e n t i o </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">n\\ndel\\nins\\nsubst\\nn t e n t i o n\\ni n t e c n t i o n\\ni n x e n t i o n\\nThe space of all possible edits is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">enormous, so we can't search naively. However, lots of distinct edit paths will end up in the same state (string), </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">so rather than recomputing all those paths, we could just remember the shortest path to a state each time we saw </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">it. We can do this by using dynamic programming. Dynamic programming is the name for a class of algorithms, first </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">introduced by Bellman (1957), that apply a table-driven method to solve problems by combining solutions to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">subproblems. Some of the most commonly used algorithms in natural language processing make use of dynamic </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">programming, such as the Viterbi algorithm (Chapter 8) and the CKY algorithm for parsing (Chapter 17). The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">intuition of a dynamic programming problem is that a large problem can be solved by properly combining the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">solutions to various subproblems. Consider the shortest path of transformed words that represents the minimum edit </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">distance between the strings intention and execution shown in Fig. 2.16. Imagine some string (perhaps it is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">exention) that is in this optimal path (whatever it is). The intuition of dynamic programming is that if exention </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is in the optimal\\ni n t e n t i o n\\ndelete i\\nn t e n t i o n\\nsubstitute n by e\\ne t e n t i o n\\nsubstitute t </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">by x\\ne x e n t i o n\\ninsert u\\ne x e n u t i o n\\nsubstitute n by c\\ne x e c u t i o n\\noperation list, then the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">optimal sequence must also include the optimal path from intention to exention. Why?\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'and update the corpus return V'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.8.1 The Minimum Edit Distance Algorithm'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"If there were a shorter path from intention to exention, then we could use it instead, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">resulting in a shorter overall path, and the optimal sequence wouldn't be optimal, thus leading to a contradiction.</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The minimum edit distance algorithm was named by Wagner and Fischer\\n(1974) but independently discovered by many </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">people (see the Historical Notes section of Chapter 8). Let's first define the minimum edit distance between two </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">strings. Given two strings, the source string X of length n, and target string Y of length m, we'll define D[i, j] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">as the edit distance between X[1..i] and Y[1.. j], i.e., the first i characters of X and the first j characters of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Y. The edit distance between X and Y is thus D[n,m]. We'll use dynamic programming to compute D[n,m] bottom up, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">combining solutions to subproblems. In the base case, with a source substring of length i but an empty target </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">string, going from i characters to 0 requires i deletes. With a target substring of length j but an empty source </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">going from 0 characters to j characters requires j inserts. Having computed D[i, j] for small i, j we then compute </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">larger D[i, j] based on previously computed smaller values. The value of D[i, j] is computed by taking the minimum </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of the three possible paths through the matrix which arrive there:\\n(2.8) D[i, j] = min \\uf8f1 \\uf8f2 D[i−1, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">j]+del-cost(source[i]) D[i, j −1]+ins-cost(target[j]) D[i−1, j −1]+sub-cost(source[i],target[j]) \\uf8f3 If we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">assume the version of Levenshtein distance in which the insertions and deletions each have a cost of $1$ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(in-cost($\\\\cdot$) = del-cost($\\\\cdot$) = $1$), and substitutions have a cost of $2$ (except substitution of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">identical letters have zero cost), the computation for $D[i,j]$ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">becomes:\\n$$D[i,j]=\\\\min\\\\left{\\\\begin{array}{ll}D[i-1,j]+1\\\\ D[i,j-1]+1\\\\ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">D[i-1,j-1]+\\\\left{\\\\begin{array}{ll}2;&amp;\\\\mbox{if}\\\\;\\\\;source[i]\\\\neq target[j]\\\\ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">0;&amp;\\\\mbox{if}\\\\;\\\\;source[i]=target[j]\\\\end{array}\\\\right.\\\\end{array}\\\\right.\\\\tag{2.9}$$\\nThe algorithm is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">summarized in Fig. 2.17; Fig.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'and update the corpus return V'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.8.1 The Minimum Edit Distance Algorithm'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2.18 shows the results of applying the algorithm to the distance between intention and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">execution with the version of Levenshtein in Eq. 2.9. Alignment Knowing the minimum edit distance is useful for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">algorithms like finding potential spelling error corrections. But the edit distance algorithm is important in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">strings is useful throughout speech and'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'and update the corpus return V'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.8.1 The Minimum Edit Distance Algorithm'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'n←LENGTH(source)\\nm←LENGTH(target)\\nCreate a distance matrix D[n+1,m+1] # Initialization: the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">zeroth row and column is the distance from the empty string D[0,0] = 0\\nfor each row i from 1 to n do </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">D[i,0]←D[i-1,0] + del-cost(source[i])\\nfor each column j from 1 to m do D[0,j]←D[0,j-1] + ins-cost(target[j])'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'and update the corpus return V'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Function Min-Edit-Distance(Source, *Target*) **Returns** Min-Distance'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'for each row i from 1 to n do for each column j from 1 to m do D[i,j]←MIN( D[i−1, j] + </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">del-cost(source[i]), D[i−1, j−1] + sub-cost(source[i],target[j]), D[i, j−1] + ins-cost(target[j]))'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Recurrence relation:'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Src\\\\Tar'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Termination return D[n,m]'</span><span style=\"font-weight: bold\">})</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"e\\nx\\ne\\nc\\nu\\nt\\ni\\no\\nn\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\ni\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n6\\n7\\n8\\nn\\n</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2\\n3\\n4\\n5\\n6\\n7\\n8\\n7\\n8\\n7\\nt\\n3\\n4\\n5\\n6\\n7\\n8\\n7\\n8\\n9\\n8\\ne\\n4\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n9\\nn\\n5\\n4\\n5\\n6\\n7\\n</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">8\\n9\\n10\\n11\\n10\\nt\\n6\\n5\\n6\\n7\\n8\\n9\\n8\\n9\\n10\\n11\\ni\\n7\\n6\\n7\\n8\\n9\\n10\\n9\\n8\\n9\\n10\\no\\n8\\n7\\n8\\n9\\n10\\n11\\n10\\n</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">9\\n8\\n9\\nn\\n9\\n8\\n9\\n10\\n11\\n12\\n11\\n10\\n9\\n8\\nlanguage processing. In speech recognition, minimum edit distance </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alignment is used to compute the word error rate (Chapter 16). Alignment plays a role in machine translation, in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. To </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extend the edit distance algorithm to produce an alignment, we can start by visualizing an alignment as a path </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">through the edit distance matrix. Figure 2.19 shows this path with boldfaced cells. Each boldfaced cell represents </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an alignment of a pair of letters in the two strings. If two boldfaced cells occur in the same row, there will be </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an insertion in going from the source to the target; two boldfaced cells in the same column indicate a deletion. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Figure 2.19 also shows the intuition of how to compute this alignment path. The computation proceeds in two steps. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">In the first step, we augment the minimum edit distance algorithm to store backpointers in each cell. The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">backpointer from a cell points to the previous cell (or cells) that we came from in entering the current cell. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">We've shown a schematic of these backpointers in Fig. 2.19. Some cells have multiple backpointers because the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">minimum extension could have come from multiple previous cells. In the second step, we perform a backtrace. In a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">backtrace, we start backtrace from the last cell (at the final row and column), and follow the pointers back </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">through the dynamic programming matrix. Each complete path between the final cell and the initial cell is a minimum</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">distance alignment. Exercise 2.7 asks you to modify the minimum edit distance algorithm to store the pointers and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">compute the backtrace to output an alignment. e\\nx\\ne\\nc\\nu\\nt\\ni\\no\\nn\\n0\\n← 1\\n← 2\\n← 3\\n← 4\\n← 5\\n← 6\\n← 7\\n← </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">8\\n← 9\\ni\\n↑ 1\\n↖←↑ 2\\n↖←↑ 3\\n↖←↑ 4\\n↖←↑ 5\\n↖←↑ 6\\n↖←↑ 7\\n↖ 6\\n← 7\\n← 8\\nn\\n↑ 2\\n↖←↑ 3\\n↖←↑ 4\\n↖←↑ 5\\n↖←↑ 6\\n↖←↑ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">7\\n↖←↑ 8\\n↑ 7\\n↖←↑ 8\\n↖ 7\\nt\\n↑ 3\\n↖←↑ 4\\n↖←↑ 5\\n↖←↑ 6\\n↖←↑ 7\\n↖←↑ 8\\n↖ 7\\n←↑ 8\\n↖←↑ 9\\n↑ 8\\ne\\n↑ 4\\n↖ 3\\n← 4\\n↖← </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">5\\n← 6\\n← 7\\n←↑ 8\\n↖←↑ 9\\n↖←↑ 10\\n↑ 9\\nn\\n↑ 5\\n↑ 4\\n↖←↑ 5\\n↖←↑ 6\\n↖←↑ 7\\n↖←↑ 8\\n↖←↑ 9\\n↖←↑ 10\\n↖←↑ 11 ↖↑ 10\\nt\\n↑ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">6\\n↑ 5\\n↖←↑ 6\\n↖←↑ 7\\n↖←↑ 8\\n↖←↑ 9\\n↖ 8\\n← 9\\n← 10 ←↑ 11\\ni\\n↑ 7\\n↑ 6\\n↖←↑ 7\\n↖←↑ 8\\n↖←↑ 9\\n↖←↑ 10\\n↑ 9\\n↖ 8\\n← </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">9\\n← 10\\no\\n↑ 8\\n↑ 7\\n↖←↑ 8\\n↖←↑ 9\\n↖←↑ 10\\n↖←↑ 11\\n↑ 10\\n↑ 9\\n↖ 8\\n← 9\\nn\\n↑ 9\\n↑ 8\\n↖←↑ 9\\n↖←↑ 10\\n↖←↑ 11\\n↖←↑ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">12\\n↑ 11\\n↑ 10\\n↑ 9\\n↖ 8\\nWhile we worked our example with simple Levenshtein distance, the algorithm in Fig.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2.17 allows arbitrary weights on the operations. For spelling correction, for example, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">substitutions are more likely to happen between letters that are next to each other on the keyboard. The Viterbi </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">algorithm is a probabilistic extension of minimum edit distance. Instead of computing the \"minimum edit distance\" </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">between two strings, Viterbi computes the \"maximum probability alignment\" of one string with another. We\\'ll </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">discuss this more in Chapter 8.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"This chapter introduced a fundamental tool in language processing, the regular expression, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings.</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Here's a summary of the main points we covered about these ideas:\\n- The regular expression language is a powerful </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tool for pattern-matching. - Basic operations in regular expressions include concatenation of symbols,\\ndisjunction</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of symbols ([], |, and .), counters (, +, and {n,m}), anchors\\n(ˆ, $) and precedence operators ((,)). - Word </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tokenization and normalization are generally done by cascades of\\nsimple regular expression substitutions or finite</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">automata. - The Porter algorithm is a simple and efficient way to do stemming, stripping\\noff affixes. It does not </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">have high accuracy but may be useful for some tasks.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.9 Summary'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'- The minimum edit distance between two strings is the minimum number of\\noperations it takes</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to edit one into the other. Minimum edit distance can be\\ncomputed by dynamic programming, which also results in an</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alignment* of\\nthe two strings.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2.9 Summary'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Kleene 1951; 1956 first defined regular expressions and the finite automaton, based on the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">McCulloch-Pitts neuron. Ken Thompson was one of the first to build regular expressions compilers into editors for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">text searching (Thompson, 1968). His editor ed included a command \"g/regular expression/p\", or Global Regular </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Expression Print, which later became the Unix grep utility. Text normalization algorithms have been applied since </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the beginning of the field. One of the earliest widely used stemmers was Lovins (1968). Stemming was also applied </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">early to the digital humanities, by Packard (1973), who built an affix-stripping morphological parser for Ancient </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Greek. Currently a wide variety of code for tokenization and normalization is available, such as the Stanford </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Tokenizer (https://nlp.stanford.edu/software/tokenizer.shtml) or specialized tokenizers for Twitter (O\\'Connor et </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">al., 2010), or for sentiment (http: //sentiment.christopherpotts.net/tokenizing.html). See Palmer (2012)\\nfor a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">survey of text preprocessing. NLTK is an essential tool that offers both useful Python libraries </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(https://www.nltk.org) and textbook descriptions (Bird et al.,\\n2009) of many algorithms including text </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">normalization and corpus interfaces. For more on Herdan\\'s law and Heaps\\' Law, see Herdan (1960, p.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Bibliographical And Historical Notes'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'28), Heaps\\n(1978), Egghe (2007) and Baayen (2001); Yasseri et al. (2012) discuss the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relationship with other measures of linguistic complexity. For more on edit distance, see the excellent Gusfield </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(1997). Our example measuring the edit distance from \\'intention\\' to \\'execution\\' was adapted from Kruskal </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(1983). There are various publicly available packages to compute edit distance, including Unix diff and the NIST </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sclite program (NIST, 2005). In his autobiography Bellman (1984) explains how he originally came up with the term </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dynamic programming:\\n\"...The 1950s were not good years for mathematical research. [the]\\nSecretary of Defense </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">...had a pathological fear and hatred of the word, research... I decided therefore to use the word, \"programming\". </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">I\\nwanted to get across the idea that this was dynamic, this was multistage... I thought, let\\'s ... take a word </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that has an absolutely precise meaning, namely dynamic... it\\'s impossible to use the word, dynamic, in a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pejorative sense. Try thinking of some combination that will possibly give it a pejorative meaning. It\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">impossible. Thus, I thought dynamic programming was a good name. It was something not even a Congressman could </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">object to.\"'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Bibliographical And Historical Notes'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2.1\\nWrite regular expressions for the following languages. 1. the set of all alphabetic </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">strings; 2. the set of all lower case alphabetic strings ending in a b;\\n3. the set of all strings from the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alphabet a,b such that each a is immediately preceded by and immediately followed by a b;\\n2.2\\nWrite regular </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">expressions for the following languages. By \"word\", we mean an alphabetic string separated from other words by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">whitespace, any relevant\\npunctuation, line breaks, and so forth. 1. the set of all strings with two consecutive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">repeated words (e.g., \"Humbert Humbert\" and \"the the\" but not \"the bug\" or \"the big bug\");\\n2. all strings that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">start at the beginning of the line with an integer and that\\nend at the end of the line with a word;\\n3. all </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">strings that have both the word grotto and the word raven in them\\n(but not, e.g., words like grottos that merely </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">contain the word grotto);\\n4. write a pattern that places the first word of an English sentence in a\\nregister. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Deal with punctuation. 2.3\\nImplement an ELIZA-like program, using substitutions such as those described on page </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">10. You might want to choose a different domain than a Rogerian psychologist, although keep in mind that you would </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">need a domain in which your program can legitimately engage in a lot of simple repetition. 2.4\\nCompute the edit </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">distance (using insertion cost 1, deletion cost 1, substitution cost 1) of \"leda\" to \"deal\". Show your work (using </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the edit distance grid). 2.5\\nFigure out whether drive is closer to brief or to divers and what the edit distance </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is to each. You may use any version of distance that you like. 2.6\\nNow implement a minimum edit distance algorithm</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and use your hand-computed results to check your code. 2.7\\nAugment the minimum edit distance algorithm to output </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an alignment; you will need to store pointers and add a stage to compute the backtrace. Baayen, R.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Exercises'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'H. 2001. Word frequency distributions. Springer. Bellman, R. 1957. Dynamic Programming. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Princeton University Press. Kleene, S. C.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Exercises'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'1956. Representation of events in nerve nets\\nand finite automata. In C. Shannon and J. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">McCarthy, editors, Automata Studies, pages 3–41. Princeton University Press. Bellman, R. 1984. Eye of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Hurricane: an autobiography. World Scientific Singapore. Krovetz, R. 1993. Viewing morphology as an inference </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">process. SIGIR-93. Bender, E. M. 2019. The #BenderRule: On naming the languages we study and why it matters. Blog </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">post. Kruskal, J. B. 1983. An overview of sequence comparison. In D. Sankoff and J.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Exercises'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'B. Kruskal, editors, Time\\nWarps, String Edits, and Macromolecules:\\nThe Theory and Practice </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of Sequence Comparison, pages 1–44. Addison-Wesley.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Exercises'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Bender, E. M., B. Friedman, and A. McMillan-Major. 2021. A guide for writing data statements </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for natural language processing. Available at http://techpolicylab.uw. edu/data-statements/. Bird, S., E. Klein, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and E. Loper. 2009. Natural Language\\nProcessing with Python. O'Reilly. Kudo, T. 2018. Subword regularization: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Improving neural\\nnetwork translation models with multiple subword candidates. ACL. Blodgett, S. L., L. Green, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">B. O'Connor. 2016. Demographic dialectal variation in social media: A case study of African-American English. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">EMNLP. Kudo, T. and J. Richardson. 2018. SentencePiece: A simple\\nand language independent subword tokenizer and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">detokenizer for neural text processing. EMNLP. Bostrom, K. and G. Durrett.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Exercises'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"2020. Byte pair encoding is\\nsuboptimal for language model pretraining. Findings of EMNLP. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Kuˇcera, H. and W. N. Francis. 1967. Computational Analysis of Present-Day American English. Brown University </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Press, Providence, RI. Chen, X., Z. Shi, X. Qiu, and X. Huang. 2017. Adversarial multi-criteria learning for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Chinese word segmentation. ACL. Levenshtein, V. I. 1966. Binary codes capable of correcting deletions, insertions, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and reversals. Cybernetics and Control Theory, 10(8):707–710. Original in Doklady\\nAkademii Nauk SSSR 163(4): </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">845–848 (1965). Church, K. W. 1994. Unix for Poets. Slides from 2nd EL-\\nSNET Summer School and unpublished paper </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ms. Li, X., Y. Meng, X. Sun, Q. Han, A. Yuan, and J. Li. 2019. Clark, H. H. and J. E. Fox Tree. 2002. Using uh and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">um in\\nspontaneous speaking. Cognition, 84:73–111. Lovins, J. B. 1968. Development of a stemming algorithm. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Mechanical Translation and Computational Linguistics,\\n11(1–2):9–13. Egghe, L. 2007. Untangling Herdan's law and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Heaps'\\nlaw: Mathematical and informetric arguments. JASIST,\\n58(5):702–709. Manning, C. D., M. Surdeanu, J. Bauer,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">J. Finkel, S. Bethard,\\nand D. McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. ACL. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Gebru, T., J. Morgenstern, B. Vecchione, J. W. Vaughan,\\nH. Wallach, H. Daum´e III, and K. Crawford. 2020. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Datasheets for datasets. ArXiv. NIST. 2005. Speech recognition scoring toolkit (sctk) version 2.1. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">http://www.nist.gov/speech/tools/. Godfrey, J., E. Holliman, and J. McDaniel. 1992. SWITCH-\\nBOARD: Telephone </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">speech corpus for research and development. ICASSP. O'Connor, B., M. Krieger, and D. Ahn. 2010. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Tweetmotif:\\nExploratory search and topic summarization for twitter. ICWSM. Gusfield, D. 1997. Algorithms on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Strings, Trees, and Sequences: Computer Science and Computational Biology. Cambridge University Press. Packard, D. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">W. 1973. Computer-assisted morphological\\nanalysis of ancient Greek. COLING. Heaps, H. S. 1978. Information </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieval. Computational and\\ntheoretical aspects. Academic Press. Herdan, G. 1960. Type-token mathematics. Mouton.</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Palmer, D. 2012. Text preprocessing. In N. Indurkhya and\\nF. J. Damerau, editors, Handbook of Natural Language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Processing, pages 9–30. CRC Press. Porter, M. F.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Exercises'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'1980. An algorithm for suffix stripping. Program, 14(3):130–137. Jones, T. 2015. Toward a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">description of African American\\nVernacular English dialect regions using \"Black Twitter\". American Speech, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">90(4):403–440. Sennrich, R., B. Haddow, and A. Birch. 2016. Neural machine translation of rare words with subword </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">units. ACL. Jurgens, D., Y. Tsvetkov, and D. Jurafsky. 2017. Incorporating dialectal variability for socially </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">equitable language identification. ACL. Simons, G. F. and C. D. Fennig. 2018. Ethnologue: Languages of the world, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">21st edition. SIL International. King, S. 2020. From African American Vernacular English\\nto African American </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Language: Rethinking the study of race and language in African Americans\\' speech. Annual\\nReview of Linguistics, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">6:285–300. Solorio, T., E. Blair, S. Maharjan, S. Bethard, M. Diab,\\nM. Ghoneim, A. Hawwari, F. AlGhamdi, J. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Hirschberg, A. Chang, and P. Fung. 2014. Overview for the first\\nshared task on language identification in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">code-switched data. First Workshop on Computational Approaches to Code Switching. Kiss, T. and J. Strunk. 2006. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Unsupervised multilingual\\nsentence boundary detection. Computational Linguistics,\\n32(4):485–525. Thompson, K. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1968. Regular expression search algorithm. CACM, 11(6):419–422. Kleene, S. C. 1951. Representation of events in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">nerve nets\\nand finite automata. Technical Report RM-704, RAND Corporation. RAND Research Memorandum. Wagner, R. A.</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and M. J. Fischer. 1974. The string-to-string\\ncorrection problem. Journal of the ACM, 21:168–173. Is word </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">segmentation necessary for deep learning of Chinese representations? ACL. Weizenbaum, J. 1966. ELIZA - A computer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">program for the\\nstudy of natural language communication between man\\nand machine. CACM, 9(1):36–45. Weizenbaum, J.</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1976. Computer Power and Human Reason: From Judgement to Calculation. W.H. Freeman and\\nCompany. Yasseri, T., A. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Kornai, and J. Kert´esz. 2012. A practical approach to language complexity: a Wikipedia case study. PLoS ONE, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">7(11).'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Header 1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Header 2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Exercises'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'User:\\nI am unhappy. ELIZA: DO YOU THINK COMING HERE WILL HELP YOU NOT TO BE \u001b[0m\n",
       "\u001b[32mUNHAPPY\\nUser:\\nI need some help, that much seems certain. ELIZA: WHAT WOULD IT MEAN TO YOU IF YOU GOT SOME \u001b[0m\n",
       "\u001b[32mHELP\\nUser:\\nPerhaps I could learn to get along with my mother. ELIZA: TELL ME MORE ABOUT YOUR FAMILY\\nUser:\\nMy \u001b[0m\n",
       "\u001b[32mmother takes care of me. ELIZA: WHO ELSE IN YOU FAMILY TAKES CARE OF YOU\\nUser:\\nMy father. ELIZA: YOUR \u001b[0m\n",
       "\u001b[32mFATHER\\nUser:\\nYou are like my father in some ways. Weizenbaum \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1966\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nThe dialogue above is from ELIZA, an early \u001b[0m\n",
       "\u001b[32mnatural language processing system\\nELIZA\\nthat could carry on a limited conversation with a user by imitating the \u001b[0m\n",
       "\u001b[32mresponses of a Rogerian psychotherapist \u001b[0m\u001b[32m(\u001b[0m\u001b[32mWeizenbaum, 1966\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. ELIZA is a surprisingly simple program that uses \u001b[0m\n",
       "\u001b[32mpattern matching to recognize phrases like \"I need X\" and translate them into suitable outputs like \"What would it \u001b[0m\n",
       "\u001b[32mmean to you if you got X?\". This simple technique succeeds in this domain because ELIZA doesn\\'t actually need to \u001b[0m\n",
       "\u001b[32mknow anything to mimic a Rogerian psychotherapist. As Weizenbaum notes, this is one of the few dialogue genres \u001b[0m\n",
       "\u001b[32mwhere listeners can act as if they know nothing of the world. ELIZA\\'s mimicry of human conversation was remarkably\u001b[0m\n",
       "\u001b[32msuccessful: many people who interacted with ELIZA came to believe that it really understood them and their \u001b[0m\n",
       "\u001b[32mproblems, many continued to believe in ELIZA\\'s abilities even after the program\\'s operation was explained to them\u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mWeizenbaum, 1976\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, and even today\\nsuch chatbots are a fun diversion. chatbots\\nOf course modern conversational \u001b[0m\n",
       "\u001b[32magents are much more than a diversion; they can answer questions, book flights, or find restaurants, functions for \u001b[0m\n",
       "\u001b[32mwhich they rely on a much more sophisticated understanding of the user\\'s intent, as we will see in Chapter 15. \u001b[0m\n",
       "\u001b[32mNonetheless, the simple pattern-based methods that powered ELIZA and other chatbots play a crucial role in natural \u001b[0m\n",
       "\u001b[32mlanguage processing. We\\'ll begin with the most important tool for describing text patterns: the regular \u001b[0m\n",
       "\u001b[32mexpression. Regular expressions can be used to specify strings we might want to extract from a document, from \u001b[0m\n",
       "\u001b[32mtransforming \"I need X\" in ELIZA above, to defining strings like $199 or $24.99 for extracting tables of prices \u001b[0m\n",
       "\u001b[32mfrom a document. We\\'ll then turn to a set of tasks collectively called text normalization, in which text \u001b[0m\n",
       "\u001b[32mnormalization regular expressions play an important part.'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"Normalizing\u001b[0m\u001b[32m text means converting it to a more convenient, standard form. For example, most \u001b[0m\n",
       "\u001b[32mof what we are going to do with language relies on first separating out or tokenizing words from running text, the \u001b[0m\n",
       "\u001b[32mtask of tokenization. English words are often separated from each other tokenization by whitespace, but whitespace \u001b[0m\n",
       "\u001b[32mis not always sufficient. New York and rock 'n' roll are sometimes treated as large words despite the fact that \u001b[0m\n",
       "\u001b[32mthey contain spaces, while sometimes we'll need to separate I'm into the two words I and am. For processing tweets \u001b[0m\n",
       "\u001b[32mor texts we'll need to tokenize emoticons like :\u001b[0m\u001b[32m)\u001b[0m\u001b[32m or hashtags like #nlproc.\"\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"Some\u001b[0m\u001b[32m languages, like Japanese, don't have spaces between words, so word tokenization becomes \u001b[0m\n",
       "\u001b[32mmore difficult. Another part of text normalization is lemmatization, the task of determining lemmatization that two\u001b[0m\n",
       "\u001b[32mwords have the same root, despite their surface differences. For example, the words sang, sung, and sings are forms\u001b[0m\n",
       "\u001b[32mof the verb sing. The word sing is the common lemma of these words, and a lemmatizer maps from all of these to \u001b[0m\n",
       "\u001b[32msing. Lemmatization is essential for processing morphologically complex languages like Arabic. Stemming refers to a\u001b[0m\n",
       "\u001b[32msimpler version of lemmatization in which we mainly stemming just strip suffixes from the end of the word. Text \u001b[0m\n",
       "\u001b[32mnormalization also includes sentence segmentation: breaking up a text into individual sentences, using cues like \u001b[0m\n",
       "\u001b[32msentence segmentation periods or exclamation points.\"\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"Finally\u001b[0m\u001b[32m, we'll need to compare words and other strings. We'll introduce a metric called edit \u001b[0m\n",
       "\u001b[32mdistance that measures how similar two strings are based on the number of edits \u001b[0m\u001b[32m(\u001b[0m\u001b[32minsertions, deletions, \u001b[0m\n",
       "\u001b[32msubstitutions\u001b[0m\u001b[32m)\u001b[0m\u001b[32m it takes to change one string into the other. Edit distance is an algorithm with applications \u001b[0m\n",
       "\u001b[32mthroughout language processing, from spelling correction to speech recognition to coreference resolution.\"\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"One\u001b[0m\u001b[32m of the unsung successes in standardization in computer science has been the regular \u001b[0m\n",
       "\u001b[32mexpression \u001b[0m\u001b[32m(\u001b[0m\u001b[32moften shortened to regex\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, a language for specifying text search regular expression strings. This \u001b[0m\n",
       "\u001b[32mpractical language is used in every computer language, word processor, and text processing tools like the Unix \u001b[0m\n",
       "\u001b[32mtools grep or Emacs. Formally, a regular expression is an algebraic notation for characterizing a set of strings. \u001b[0m\n",
       "\u001b[32mRegular expressions are particularly useful for searching in texts, when we have a pattern to search for and a \u001b[0m\n",
       "\u001b[32mcorpus of texts to search through. A regular expression search function corpus will search through the corpus, \u001b[0m\n",
       "\u001b[32mreturning all texts that match the pattern. The corpus can be a single document or a collection. For example, the \u001b[0m\n",
       "\u001b[32mUnix command-line tool grep takes a regular expression and returns every line of the input document that matches \u001b[0m\n",
       "\u001b[32mthe expression. A search can be designed to return every match on a line, if there are more than one, or just the \u001b[0m\n",
       "\u001b[32mfirst match. In the following examples we generally underline the exact part of the pattern that matches the \u001b[0m\n",
       "\u001b[32mregular expression and show only the first match. We'll show regular expressions delimited by slashes but note that\u001b[0m\n",
       "\u001b[32mslashes are not part of the regular expressions. Regular expressions come in many variants.\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.1 Regular Expressions'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"We\u001b[0m\u001b[32m'll be describing extended regular expressions; different regular expression parsers may \u001b[0m\n",
       "\u001b[32monly recognize subsets of these, or treat some expressions slightly differently. Using an online regular expression\u001b[0m\n",
       "\u001b[32mtester is a handy way to test out your expressions and explore these variations.\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.1 Regular Expressions'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'The simplest kind of regular expression is a sequence of simple characters; putting \u001b[0m\n",
       "\u001b[32mcharacters in sequence is called concatenation. To search for woodchuck, we type concatenation\\n/woodchuck/. The \u001b[0m\n",
       "\u001b[32mexpression /Buttercup/ matches any string containing the substring Buttercup; grep with that expression would \u001b[0m\n",
       "\u001b[32mreturn the line I\\'m called little Buttercup. The search string can consist of a single character \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlike /!/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m or a \u001b[0m\n",
       "\u001b[32msequence of characters \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlike /urgl/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m(\u001b[0m\u001b[32msee Fig. 2.1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Regular expressions are case sensitive; lower case /s/ is \u001b[0m\n",
       "\u001b[32mdistinct from upper\\n| Regex                                             \u001b[0m\n",
       "\u001b[32m|\\n|---------------------------------------------------|\\n| /woodchucks/                                      |\\n| \u001b[0m\n",
       "\u001b[32m\"interesting links to woodchucks and lemurs\"      |\\n| /a/                                               |\\n| \"Mary\u001b[0m\n",
       "\u001b[32mAnn stopped by Mona\\'s\"                      |\\n| /!/                                               |\\n| \"You\\'ve \u001b[0m\n",
       "\u001b[32mleft the burglar behind again!\" said Nori |\\ncase /S/ \u001b[0m\u001b[32m(\u001b[0m\u001b[32m/s/ matches a lower case s but not an upper case S\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. This \u001b[0m\n",
       "\u001b[32mmeans that the pattern /woodchucks/ will not match the string Woodchucks. We can solve this problem with the use of\u001b[0m\n",
       "\u001b[32mthe square braces \u001b[0m\u001b[32m[\u001b[0m\u001b[32m and \u001b[0m\u001b[32m]\u001b[0m\u001b[32m. The string of characters inside the braces specifies a disjunction of characters to \u001b[0m\n",
       "\u001b[32mmatch. For example, Fig. 2.2 shows that the pattern /\u001b[0m\u001b[32m[\u001b[0m\u001b[32mwW\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/ matches patterns containing either w or W. | Regex      \u001b[0m\n",
       "\u001b[32m| Match              | Example Patterns   \u001b[0m\n",
       "\u001b[32m|\\n|------------------------|--------------------|--------------------|\\n| /\u001b[0m\u001b[32m[\u001b[0m\u001b[32mwW\u001b[0m\u001b[32m]\u001b[0m\u001b[32moodchuck/         |                \u001b[0m\n",
       "\u001b[32m|                    |\\n| Woodchuck or woodchuck | \"Woodchuck\"        |                    |\\n| /\u001b[0m\u001b[32m[\u001b[0m\u001b[32mabc\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/            \u001b[0m\n",
       "\u001b[32m|                    |                    |\\n| \\'a\\', \\'b\\',              | or                 | \\'c\\'             \u001b[0m\n",
       "\u001b[32m|\\n| /\u001b[0m\u001b[32m[\u001b[0m\u001b[32m1234567890\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/         |                    |                    |\\n| any digit              | \"plenty of 7 to\u001b[0m\n",
       "\u001b[32m5\" |                    |\\nThe regular expression /\u001b[0m\u001b[32m[\u001b[0m\u001b[32m1234567890\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/ specifies any single digit. While such classes of \u001b[0m\n",
       "\u001b[32mcharacters as digits or letters are important building blocks in expressions, they can get awkward \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., it\\'s \u001b[0m\n",
       "\u001b[32minconvenient to specify\\n/\u001b[0m\u001b[32m[\u001b[0m\u001b[32mABCDEFGHIJKLMNOPQRSTUVWXYZ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/\\nto mean \"any capital letter\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. In cases where there is a \u001b[0m\n",
       "\u001b[32mwell-defined sequence associated with a set of characters, the brackets can be used with the dash \u001b[0m\u001b[32m(\u001b[0m\u001b[32m-\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to specify \u001b[0m\n",
       "\u001b[32many one character in a range. The pattern /\u001b[0m\u001b[32m[\u001b[0m\u001b[32m2-5\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/ specifies any one of the characrange ters 2, 3, 4, or 5. The \u001b[0m\n",
       "\u001b[32mpattern /\u001b[0m\u001b[32m[\u001b[0m\u001b[32mb-g\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/ specifies one of the characters b, c, d, e, f, or g. Some other examples are shown in Fig. 2.3. | \u001b[0m\n",
       "\u001b[32mRegex                | Match                                    \u001b[0m\n",
       "\u001b[32m|\\n|----------------------|------------------------------------------|\\n| /\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-Z\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/              |                   \u001b[0m\n",
       "\u001b[32m|\\n| an upper case letter | \"we should call it \\'Drenched Blossoms\\' \" |\\n| /\u001b[0m\u001b[32m[\u001b[0m\u001b[32ma-z\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/              |                 \u001b[0m\n",
       "\u001b[32m|\\n| a lower case letter  | \"my beans were impatient to be hoed!\"    |\\n| /\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/              |                   \u001b[0m\n",
       "\u001b[32m|\\n| a single digit       | \"Chapter 1: Down the Rabbit Hole\"        |\\nThe square braces can also be used to \u001b[0m\n",
       "\u001b[32mspecify what a single character cannot be, by use of the caret ˆ. If the caret ˆ is the first symbol after the open\u001b[0m\n",
       "\u001b[32msquare brace \u001b[0m\u001b[32m[\u001b[0m\u001b[32m, the resulting pattern is negated. For example, the pattern /\u001b[0m\u001b[32m[\u001b[0m\u001b[32mˆa\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/ matches any single character \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mincluding special characters\u001b[0m\u001b[32m)\u001b[0m\u001b[32m except a. This is only true when the caret is the first symbol after the open square\u001b[0m\n",
       "\u001b[32mbrace. If it occurs anywhere else, it usually stands for a caret; Fig.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.1.1 Basic Regular Expression Patterns'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'2.4 shows some examples. | Regex                    | Match \u001b[0m\u001b[32m(\u001b[0m\u001b[32msingle characters\u001b[0m\u001b[32m)\u001b[0m\u001b[32m          \u001b[0m\n",
       "\u001b[32m|\\n|--------------------------|------------------------------------|\\n| /\u001b[0m\u001b[32m[\u001b[0m\u001b[32mˆA-Z\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/                 |                 \u001b[0m\n",
       "\u001b[32m|\\n| not an upper case letter | \"Oyfn pripetchik\"                  |\\n| /\u001b[0m\u001b[32m[\u001b[0m\u001b[32mˆSs\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/                  |                 \u001b[0m\n",
       "\u001b[32m|\\n| neither \\'S\\' nor \\'s\\'      | \"I have no exquisite reason for\\'t\" |\\n| /\u001b[0m\u001b[32m[\u001b[0m\u001b[32mˆ.\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/                   |            \u001b[0m\n",
       "\u001b[32m|\\n| not a period             | \"our resident Djinn\"               |\\n| /\u001b[0m\u001b[32m[\u001b[0m\u001b[32meˆ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/                   |                 \u001b[0m\n",
       "\u001b[32m|\\n| either \\'e\\' or \\'          |                                    |\\n| ˆ                        |              \u001b[0m\n",
       "\u001b[32m|\\n| \\'                        | \"look up ˆ now\"                    |\\n| /aˆb/                    |                \u001b[0m\n",
       "\u001b[32m|\\n| the pattern \\'            |                                    |\\n| aˆb                      |                \u001b[0m\n",
       "\u001b[32m|\\n| \\'                        | \"look up aˆ b now\"                 |\\nHow can we talk about optional elements, \u001b[0m\n",
       "\u001b[32mlike an optional s in woodchuck and woodchucks? We can\\'t use the square brackets, because while they allow us to \u001b[0m\n",
       "\u001b[32msay\\n\"s or S\", they don\\'t allow us to say \"s or nothing\". For this we use the question mark\\n/?/, which means \"the\u001b[0m\n",
       "\u001b[32mpreceding character or nothing\", as shown in Fig. 2.5. | Regex                   | Match       \u001b[0m\n",
       "\u001b[32m|\\n|-------------------------|-------------|\\n| /woodchucks?/           |             |\\n| woodchuck or woodchucks \u001b[0m\n",
       "\u001b[32m| \"woodchuck\" |\\n| /colou?r/               |             |\\n| color or colour         | \"color\"     |\\nWe can think\u001b[0m\n",
       "\u001b[32mof the question mark as meaning \"zero or one instances of the previous character\". That is, it\\'s a way of \u001b[0m\n",
       "\u001b[32mspecifying how many of something that we want, something that is very important in regular expressions. For \u001b[0m\n",
       "\u001b[32mexample, consider the language of certain sheep, which consists of strings that look like the following:\\nbaa! \u001b[0m\n",
       "\u001b[32mbaaa!'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.1.1 Basic Regular Expression Patterns'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'baaaa! baaaaa! ... This language consists of strings with a b, followed by at least two a\\'s,\u001b[0m\n",
       "\u001b[32mfollowed by an exclamation point. The set of operators that allows us to say things like \"some number of as\" are \u001b[0m\n",
       "\u001b[32mbased on the asterisk or , commonly called the Kleene *** \u001b[0m\u001b[32m(\u001b[0m\u001b[32mgen-\\nKleene *\\nerally pronounced \"cleany star\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. The \u001b[0m\n",
       "\u001b[32mKleene star means \"zero or more occurrences of the immediately previous character or regular expression\". So /a/ \u001b[0m\n",
       "\u001b[32mmeans \"any string of zero or more as\". This will match a or aaaaaa, but it will also match the empty string at the \u001b[0m\n",
       "\u001b[32mstart of Off Minor since the string Off Minor starts with zero a\\'s. So the regular expression for matching one or \u001b[0m\n",
       "\u001b[32mmore a is /aa/, meaning one a followed by zero or more as. More complex patterns can also be repeated. So \u001b[0m\n",
       "\u001b[32m/\u001b[0m\u001b[32m[\u001b[0m\u001b[32mab\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/\\nmeans \"zero or more a\\'s or b\\'s\" \u001b[0m\u001b[32m(\u001b[0m\u001b[32mnot \"zero or more right square braces\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. This will match strings like \u001b[0m\n",
       "\u001b[32maaaa or ababab or bbbb. For specifying multiple digits \u001b[0m\u001b[32m(\u001b[0m\u001b[32museful for finding prices\u001b[0m\u001b[32m)\u001b[0m\u001b[32m we can extend /\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/, the \u001b[0m\n",
       "\u001b[32mregular expression for a single digit. An integer \u001b[0m\u001b[32m(\u001b[0m\u001b[32ma string of digits\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is thus\\n/\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mWhy isn\\'t it just \u001b[0m\n",
       "\u001b[32m/\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/?\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nSometimes it\\'s annoying to have to write the regular expression for digits twice, so there is a shorter\u001b[0m\n",
       "\u001b[32mway to specify \"at least one\" of some character. This is the Kleene +, which means \"one or more occurrences of the \u001b[0m\n",
       "\u001b[32mimmediately preceding Kleene +\\ncharacter or regular expression\". Thus, the expression /\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m+/ is the normal way \u001b[0m\n",
       "\u001b[32mto specify \"a sequence of digits\". There are thus two ways to specify the sheep language: /baaa!/ or /baa+!/. One \u001b[0m\n",
       "\u001b[32mvery important special character is the period \u001b[0m\u001b[32m(\u001b[0m\u001b[32m/./\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, a wildcard expression that matches any single character \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mexcept a carriage return\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, as shown in Fig. 2.6. | Regex                 | Match   | Example Matches   \u001b[0m\n",
       "\u001b[32m|\\n|-----------------------|---------|-------------------|\\n| /beg.n/               |         |                   \u001b[0m\n",
       "\u001b[32m|\\n| any character between | beg     | and               |\\nThe wildcard is often used together with the Kleene \u001b[0m\n",
       "\u001b[32mstar to mean \"any string of characters\". For example, suppose we want to find any line in which a particular word, \u001b[0m\n",
       "\u001b[32mfor example, aardvark, appears twice. We can specify this with the regular expression /aardvark.aardvark/. Anchors \u001b[0m\n",
       "\u001b[32mare special characters that anchor regular expressions to particular places anchors in a string. The most common \u001b[0m\n",
       "\u001b[32manchors are the caret ˆ and the dollar sign $. The caret\\nˆ matches the start of a line. The pattern /ˆThe/ matches\u001b[0m\n",
       "\u001b[32mthe word The only at the start of a line. Thus, the caret ˆ has three uses: to match the start of a line, to \u001b[0m\n",
       "\u001b[32mindicate a negation inside of square brackets, and just to mean a caret. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mWhat are the contexts that allow grep or \u001b[0m\n",
       "\u001b[32mPython to know which function a given caret is supposed to have?\u001b[0m\u001b[32m)\u001b[0m\u001b[32m The dollar sign $ matches the end of a line. So \u001b[0m\n",
       "\u001b[32mthe pattern ␣$ is a useful pattern for matching a space at the end of a line, and /ˆThe dog.$/ matches a line that \u001b[0m\n",
       "\u001b[32mcontains only the phrase The dog. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mWe have to use the backslash here since we want the . to mean \"period\" and not \u001b[0m\n",
       "\u001b[32mthe wildcard.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nMatch\\nˆ\\nstart of line\\n$\\nend of line\\n\\\\b\\nword boundary\\n\\\\B\\nnon-word boundary\\nThere are also\u001b[0m\n",
       "\u001b[32mtwo other anchors: \\\\b matches a word boundary, and \\\\B matches a non-boundary. Thus, /\\\\bthe\\\\b/ matches the word \u001b[0m\n",
       "\u001b[32mthe but not the word other. More technically, a \"word\" for the purposes of a regular expression is defined as any \u001b[0m\n",
       "\u001b[32msequence of digits, underscores, or letters; this is based on the definition of \"words\" in programming languages. \u001b[0m\n",
       "\u001b[32mFor example, /\\\\b99\\\\b/ will match the string 99 in There are 99 bottles of beer on the wall \u001b[0m\u001b[32m(\u001b[0m\u001b[32mbecause 99 follows a \u001b[0m\n",
       "\u001b[32mspace\u001b[0m\u001b[32m)\u001b[0m\u001b[32m but not 99 in There are 299 bottles of beer on the wall \u001b[0m\u001b[32m(\u001b[0m\u001b[32msince 99 follows a number\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. But it will match 99 in\u001b[0m\n",
       "\u001b[32m$99 \u001b[0m\u001b[32m(\u001b[0m\u001b[32msince 99 follows a dollar sign \u001b[0m\u001b[32m(\u001b[0m\u001b[32m$\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, which is not a digit, underscore, or letter\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.1.1 Basic Regular Expression Patterns'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Suppose we need to search for texts about pets; perhaps we are particularly interested in \u001b[0m\n",
       "\u001b[32mcats and dogs. In such a case, we might want to search for either the string cat or the string dog. Since we can\\'t\u001b[0m\n",
       "\u001b[32muse the square brackets to search for \"cat or dog\" \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwhy can\\'t we say /\u001b[0m\u001b[32m[\u001b[0m\u001b[32mcatdog\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/?\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, we need a new operator, the \u001b[0m\n",
       "\u001b[32mdisjunction operator, also disjunction called the pipe symbol |. The pattern /cat|dog/ matches either the string \u001b[0m\n",
       "\u001b[32mcat or the string dog. Sometimes we need to use this disjunction operator in the midst of a larger sequence. For \u001b[0m\n",
       "\u001b[32mexample, suppose I want to search for information about pet fish for my cousin David. How can I specify both guppy \u001b[0m\n",
       "\u001b[32mand guppies? We cannot simply say /guppy|ies/, because that would match only the strings guppy and ies. This is \u001b[0m\n",
       "\u001b[32mbecause sequences like guppy take precedence over the disjunction operator |. precedence To make the disjunction \u001b[0m\n",
       "\u001b[32moperator apply only to a specific pattern, we need to use the parenthesis operators \u001b[0m\u001b[32m(\u001b[0m\u001b[32m and \u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Enclosing a pattern in\u001b[0m\n",
       "\u001b[32mparentheses makes it act like a single character for the purposes of neighboring operators like the pipe | and the \u001b[0m\n",
       "\u001b[32mKleene. So the pattern /gupp\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|ies\u001b[0m\u001b[32m)\u001b[0m\u001b[32m/ would specify that we meant the disjunction only to apply to the suffixes y \u001b[0m\n",
       "\u001b[32mand ies. The parenthesis operator \u001b[0m\u001b[32m(\u001b[0m\u001b[32m is also useful when we are using counters like the Kleene. Unlike the | \u001b[0m\n",
       "\u001b[32moperator, the Kleene operator applies by default only to a single character, not to a whole sequence. Suppose we \u001b[0m\n",
       "\u001b[32mwant to match repeated instances of a string. Perhaps we have a line that has column labels of the form Column 1 \u001b[0m\n",
       "\u001b[32mColumn 2 Column 3. The expression /Column␣\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m+␣/ will not match any number of columns; instead, it will match a \u001b[0m\n",
       "\u001b[32msingle column followed by\\nany number of spaces! The star here applies only to the space ␣ that precedes it, not to\u001b[0m\n",
       "\u001b[32mthe whole sequence. With the parentheses, we could write the expression /\u001b[0m\u001b[32m(\u001b[0m\u001b[32mColumn␣\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m+␣\u001b[0m\u001b[32m)\u001b[0m\u001b[32m/ to match the word \u001b[0m\n",
       "\u001b[32mColumn, followed by a number and optional spaces, the whole pattern repeated zero or more times. This idea that one\u001b[0m\n",
       "\u001b[32moperator may take precedence over another, requiring us to sometimes use parentheses to specify what we mean, is \u001b[0m\n",
       "\u001b[32mformalized by the operator precedence hierarchy for regular expressions. The following table gives the order \u001b[0m\n",
       "\u001b[32moperator precedence of RE operator precedence, from highest precedence to lowest precedence. \u001b[0m\n",
       "\u001b[32mParenthesis\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nCounters\\n* + ? \u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\nSequences and anchors\\nthe ˆmy end$\\nDisjunction\\n|\\nThus, because counters \u001b[0m\n",
       "\u001b[32mhave a higher precedence than sequences,\\n/the/ matches theeeee but not thethe. Because sequences have a higher \u001b[0m\n",
       "\u001b[32mprecedence than disjunction, /the|any/ matches the or any but not thany or theny. Patterns can be ambiguous in \u001b[0m\n",
       "\u001b[32manother way. Consider the expression /\u001b[0m\u001b[32m[\u001b[0m\u001b[32ma-z\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/\\nwhen matching against the text once upon a time. Since /\u001b[0m\u001b[32m[\u001b[0m\u001b[32ma-z\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/ \u001b[0m\n",
       "\u001b[32mmatches zero or more letters, this expression could match nothing, or just the first letter o, on, onc, or once. In\u001b[0m\n",
       "\u001b[32mthese cases regular expressions always match the largest string they can;\\nwe say that patterns are greedy, \u001b[0m\n",
       "\u001b[32mexpanding to cover as much of a string as they can. greedy There are, however, ways to enforce non-greedy matching,\u001b[0m\n",
       "\u001b[32musing another meannon-greedy ing of the ? qualifier.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.1.2 Disjunction, Grouping, And Precedence'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'The operator ? is a Kleene star that matches as little text as\\n*? possible. The operator +? \u001b[0m\n",
       "\u001b[32mis a Kleene plus that matches as little text as possible. +?'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.1.2 Disjunction, Grouping, And Precedence'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Suppose we wanted to write a RE to find cases of the English article the. A simple\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mbut \u001b[0m\n",
       "\u001b[32mincorrect\u001b[0m\u001b[32m)\u001b[0m\u001b[32m pattern might be:\\n/the/\\nOne problem is that this pattern will miss the word when it begins a sentence \u001b[0m\n",
       "\u001b[32mand hence is capitalized \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e., The\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.1.3 A Simple Example'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"This\u001b[0m\u001b[32m might lead us to the following pattern:\\n/\u001b[0m\u001b[32m[\u001b[0m\u001b[32mtT\u001b[0m\u001b[32m]\u001b[0m\u001b[32mhe/\\nBut we will still incorrectly return \u001b[0m\n",
       "\u001b[32mtexts with the embedded in other words \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., other or theology\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. So we need to specify that we want instances with\u001b[0m\n",
       "\u001b[32ma word boundary on both sides:\\n/\\\\b\u001b[0m\u001b[32m[\u001b[0m\u001b[32mtT\u001b[0m\u001b[32m]\u001b[0m\u001b[32mhe\\\\b/\\nSuppose we wanted to do this without the use of /\\\\b/. We might \u001b[0m\n",
       "\u001b[32mwant this since\\n/\\\\b/ won't treat underscores and numbers as word boundaries; but we might want to find the in \u001b[0m\n",
       "\u001b[32msome context where it might also have underlines or numbers nearby\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mthe or the25\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. We need to specify that we want\u001b[0m\n",
       "\u001b[32minstances in which there are no alphabetic letters on either side of the the:\\n/\u001b[0m\u001b[32m[\u001b[0m\u001b[32mˆa-zA-Z\u001b[0m\u001b[32m]\u001b[0m\u001b[32m[\u001b[0m\u001b[32mtT\u001b[0m\u001b[32m]\u001b[0m\u001b[32mhe\u001b[0m\u001b[32m[\u001b[0m\u001b[32mˆa-zA-Z\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/\\nBut \u001b[0m\n",
       "\u001b[32mthere is still one more problem with this pattern: it won't find the word the when it begins a line. This is \u001b[0m\n",
       "\u001b[32mbecause the regular expression \u001b[0m\u001b[32m[\u001b[0m\u001b[32mˆa-zA-Z\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, which we used to avoid embedded instances of the, implies that there must\u001b[0m\n",
       "\u001b[32mbe some single\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32malthough non-alphabetic\u001b[0m\u001b[32m)\u001b[0m\u001b[32m character before the the. We can avoid this by specifying that before the\u001b[0m\n",
       "\u001b[32mthe we require either the beginning-of-line or a non-alphabetic character, and the same at the end of the \u001b[0m\n",
       "\u001b[32mline:\\n/\u001b[0m\u001b[32m(\u001b[0m\u001b[32mˆ|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mˆa-zA-Z\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32mtT\u001b[0m\u001b[32m]\u001b[0m\u001b[32mhe\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mˆa-zA-Z\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|$\u001b[0m\u001b[32m)\u001b[0m\u001b[32m/\\nThe process we just went through was based on fixing two kinds of \u001b[0m\n",
       "\u001b[32merrors: false positives, strings that we incorrectly matched like other or there, and false negafalse positives \u001b[0m\n",
       "\u001b[32mtives, strings that we incorrectly missed, like The. Addressing these two kinds of false negatives errors comes up \u001b[0m\n",
       "\u001b[32magain and again in implementing speech and language processing systems.\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.1.3 A Simple Example'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"Reducing\u001b[0m\u001b[32m the overall error rate for an application thus involves two antagonistic efforts:\\n-\u001b[0m\n",
       "\u001b[32mIncreasing precision \u001b[0m\u001b[32m(\u001b[0m\u001b[32mminimizing false positives\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n- Increasing recall \u001b[0m\u001b[32m(\u001b[0m\u001b[32mminimizing false negatives\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nWe'll come \u001b[0m\n",
       "\u001b[32mback to precision and recall with more precise definitions in Chapter 4.\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.1.3 A Simple Example'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Figure 2.8 shows some aliases for common ranges, which can be used mainly to save typing. \u001b[0m\n",
       "\u001b[32mBesides the Kleene * and Kleene + we can also use explicit numbers as counters, by enclosing them in curly \u001b[0m\n",
       "\u001b[32mbrackets. The regular expression /\u001b[0m\u001b[32m{\u001b[0m\u001b[32m3\u001b[0m\u001b[32m}\u001b[0m\u001b[32m/ means \"exactly 3 occurrences of the previous character or expression\". So \u001b[0m\n",
       "\u001b[32m/a.\u001b[0m\u001b[32m{\u001b[0m\u001b[32m24\u001b[0m\u001b[32m}\u001b[0m\u001b[32mz/ will match a followed by 24 dots followed by z \u001b[0m\u001b[32m(\u001b[0m\u001b[32mbut not a followed by 23 or 25 dots followed by a z\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. | \u001b[0m\n",
       "\u001b[32mRegex                       | Expansion    |\\n|-----------------------------|--------------|\\n| \\\\d                \u001b[0m\n",
       "\u001b[32m| \u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m        |\\n| any digit                   | Party        |\\n| ␣                           |              |\\n|\u001b[0m\n",
       "\u001b[32mof                          |              |\\n| ␣                           |              |\\n| 5                  \u001b[0m\n",
       "\u001b[32m|              |\\n| \\\\D                          | \u001b[0m\u001b[32m[\u001b[0m\u001b[32mˆ0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m       |\\n| any non-digit               | Blue         \u001b[0m\n",
       "\u001b[32m|\\n| ␣                           |              |\\n| moon                        |              |\\n| \\\\w           \u001b[0m\n",
       "\u001b[32m| \u001b[0m\u001b[32m[\u001b[0m\u001b[32ma-zA-Z0-9_\u001b[0m\u001b[32m]\u001b[0m\u001b[32m |\\n| any alphanumeric/underscore | Daiyu        |\\n| \\\\W                          | \u001b[0m\u001b[32m[\u001b[0m\u001b[32mˆ\\\\w\u001b[0m\u001b[32m]\u001b[0m\u001b[32m        \u001b[0m\n",
       "\u001b[32m|\\n| a non-alphanumeric          | !!!! |\\n| \\\\s                          | \u001b[0m\u001b[32m[\u001b[0m\u001b[32m␣\\\\r\\\\t\\\\n\\\\f\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  |\\n| whitespace \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mspace, tab\u001b[0m\u001b[32m)\u001b[0m\u001b[32m     | in Concord   |\\n| \\\\S                          | \u001b[0m\u001b[32m[\u001b[0m\u001b[32mˆ\\\\s\u001b[0m\u001b[32m]\u001b[0m\u001b[32m        |\\n| Non-whitespace              \u001b[0m\n",
       "\u001b[32m| in           |\\n| ␣                           |              |\\n| Concord                     |              |\\nA\u001b[0m\n",
       "\u001b[32mrange of numbers can also be specified. So /\u001b[0m\u001b[32m{\u001b[0m\u001b[32mn,m\u001b[0m\u001b[32m}\u001b[0m\u001b[32m/ specifies from n to m occurrences of the previous char or \u001b[0m\n",
       "\u001b[32mexpression, and /\u001b[0m\u001b[32m{\u001b[0m\u001b[32mn,\u001b[0m\u001b[32m}\u001b[0m\u001b[32m/ means at least n occurrences of the previous expression. REs for counting are summarized in \u001b[0m\n",
       "\u001b[32mFig. 2.9. | Regex                                                       | Match   \u001b[0m\n",
       "\u001b[32m|\\n|-------------------------------------------------------------|---------|\\n| *                                  \u001b[0m\n",
       "\u001b[32m|         |\\n| zero or more occurrences of the previous char or expression |         |\\n| +                        \u001b[0m\n",
       "\u001b[32m|         |\\n| one or more occurrences of the previous char or expression  |         |\\n| ? |         |\\n| zero or \u001b[0m\n",
       "\u001b[32mone occurrence of the previous char or expression   |         |\\n| \u001b[0m\u001b[32m{\u001b[0m\u001b[32mn\u001b[0m\u001b[32m}\u001b[0m\u001b[32m                                             \u001b[0m\n",
       "\u001b[32m|         |\\n| exactly                                                     | n       |\\n| \u001b[0m\u001b[32m{\u001b[0m\u001b[32mn,m\u001b[0m\u001b[32m}\u001b[0m\u001b[32m                    \u001b[0m\n",
       "\u001b[32m|         |\\n| from                                                        | n       |\\n| \u001b[0m\u001b[32m{\u001b[0m\u001b[32mn,\u001b[0m\u001b[32m}\u001b[0m\u001b[32m                     \u001b[0m\n",
       "\u001b[32m|         |\\n| at least                                                    | n       |\\n| \u001b[0m\u001b[32m{\u001b[0m\u001b[32m,m\u001b[0m\u001b[32m}\u001b[0m\u001b[32m                     \u001b[0m\n",
       "\u001b[32m|         |\\n| up to                                                       | m       |\\nFinally, certain special \u001b[0m\n",
       "\u001b[32mcharacters are referred to by special notation based on the backslash \u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m(\u001b[0m\u001b[32msee Fig. 2.10\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. The most common of these \u001b[0m\n",
       "\u001b[32mare the newline character newline\\n\\\\n and the tab character \\\\t. To refer to characters that are special \u001b[0m\n",
       "\u001b[32mthemselves \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlike\\n., , \u001b[0m\u001b[32m[\u001b[0m\u001b[32m, and \u001b[0m\u001b[32m)\u001b[0m\u001b[32m, precede them with a backslash, \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e., /./, /*/, /\u001b[0m\u001b[32m[\u001b[0m\u001b[32m/, and /\\\\/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. | Regex           \u001b[0m\n",
       "\u001b[32m| Match                                  |\\n|-----------------|----------------------------------------|\\n| *      \u001b[0m\n",
       "\u001b[32m|                                        |\\n| an asterisk \"\" | \"KAPLA*N\"                          |\\n| . |         \u001b[0m\n",
       "\u001b[32m|\\n| a period \".\"    | \"Dr.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.1.4 More Operators'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Livingston, I presume\"            |\\n| \\\\? |                                        |\\n| a \u001b[0m\n",
       "\u001b[32mquestion mark | \"Why don\\'t they come and lend a hand?\" |\\n| \\\\n              |                                    \u001b[0m\n",
       "\u001b[32m|\\n| a newline       |                                        |\\n| \\\\t              |                              \u001b[0m\n",
       "\u001b[32m|\\n| a tab           |                                        |'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.1.4 More Operators'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Let\\'s try out a more significant example of the power of REs. Suppose we want to build an \u001b[0m\n",
       "\u001b[32mapplication to help a user buy a computer on the Web. The user might want \"any machine with at least 6 GHz and 500 \u001b[0m\n",
       "\u001b[32mGB of disk space for less than $1000\". To do this kind of retrieval, we first need to be able to look for \u001b[0m\n",
       "\u001b[32mexpressions like 6\\nGHz or 500 GB or Mac or $999.99. In the rest of this section we\\'ll work out some simple \u001b[0m\n",
       "\u001b[32mregular expressions for this task.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.1.5 A More Complex Example'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"First\u001b[0m\u001b[32m, let's complete our regular expression for prices. Here's a regular expression for a \u001b[0m\n",
       "\u001b[32mdollar sign followed by a string of digits:\\n/$\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m+/\\nNote that the $ character has a different function here \u001b[0m\n",
       "\u001b[32mthan the end-of-line function we discussed earlier. Most regular expression parsers are smart enough to realize \u001b[0m\n",
       "\u001b[32mthat $ here doesn't mean end-of-line. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAs a thought experiment, think about how regex parsers might figure out the \u001b[0m\n",
       "\u001b[32mfunction of $ from the context.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nNow we just need to deal with fractions of dollars. We'll add a decimal point and\u001b[0m\n",
       "\u001b[32mtwo digits afterwards:\\n/$\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m+.\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/\\nThis pattern only allows $199.99 but not $199. We need to make the \u001b[0m\n",
       "\u001b[32mcents optional and to make sure we're at a word boundary:\\n/\u001b[0m\u001b[32m(\u001b[0m\u001b[32mˆ|\\\\W\u001b[0m\u001b[32m)\u001b[0m\u001b[32m$\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m+\u001b[0m\u001b[32m(\u001b[0m\u001b[32m.\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m?\\\\b/\\nOne last catch! This \u001b[0m\n",
       "\u001b[32mpattern allows prices like $199999.99 which would be far too expensive! We need to limit the \u001b[0m\n",
       "\u001b[32mdollars:\\n/\u001b[0m\u001b[32m(\u001b[0m\u001b[32mˆ|\\\\W\u001b[0m\u001b[32m)\u001b[0m\u001b[32m$\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m{\u001b[0m\u001b[32m0,3\u001b[0m\u001b[32m}\u001b[0m\u001b[32m(\u001b[0m\u001b[32m.\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m?\\\\b/\\nFurther fixes \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlike avoiding matching a dollar sign with no price \u001b[0m\n",
       "\u001b[32mafter it\u001b[0m\u001b[32m)\u001b[0m\u001b[32m are left as an exercise for the reader. How about disk space?\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.1.5 A More Complex Example'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'We\\'ll need to allow for optional fractions again \u001b[0m\u001b[32m(\u001b[0m\u001b[32m5.5 GB\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\nnote the use of ? for making the\u001b[0m\n",
       "\u001b[32mfinal s optional, and the use of /␣/ to mean \"zero or more spaces\" since there might always be extra spaces lying \u001b[0m\n",
       "\u001b[32maround:\\n/\\\\b\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m+\u001b[0m\u001b[32m(\u001b[0m\u001b[32m.\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m+\u001b[0m\u001b[32m)\u001b[0m\u001b[32m?␣\u001b[0m\u001b[32m(\u001b[0m\u001b[32mGB|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mGg\u001b[0m\u001b[32m]\u001b[0m\u001b[32migabytes?\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\\\b/\\nModifying this regular expression so that it only matches \u001b[0m\n",
       "\u001b[32mmore than 500 GB is left as an exercise for the reader.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.1.5 A More Complex Example'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'An important use of regular expressions is in substitutions. For example, the \u001b[0m\n",
       "\u001b[32msubstisubstitution\\ntution operator s/regexp1/pattern/ used in Python and in Unix commands like vim or sed allows a\u001b[0m\n",
       "\u001b[32mstring characterized by a regular expression to be replaced by another string:\\ns/colour/color/\\nIt is often useful\u001b[0m\n",
       "\u001b[32mto be able to refer to a particular subpart of the string matching the first pattern. For example, suppose we \u001b[0m\n",
       "\u001b[32mwanted to put angle brackets around all integers in a text, for example, changing the 35 boxes to the \u001b[0m\u001b[32m<\u001b[0m\u001b[32m35\u001b[0m\u001b[32m> boxes. \u001b[0m\n",
       "\u001b[32mWe\\'d like a way to refer to the integer we\\'ve found so that we can easily add the brackets. To do this, we put \u001b[0m\n",
       "\u001b[32mparentheses \u001b[0m\u001b[32m(\u001b[0m\u001b[32m and \u001b[0m\u001b[32m)\u001b[0m\u001b[32m around the first pattern and use the number operator \\\\1 in the second pattern to refer back. \u001b[0m\n",
       "\u001b[32mHere\\'s how it looks:\\ns/\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0-9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m+\u001b[0m\u001b[32m)\u001b[0m\u001b[32m/<\\\\1\u001b[0m\u001b[32m>\u001b[0m\u001b[32m/\\nThe parenthesis and number operators can also specify that a certain \u001b[0m\n",
       "\u001b[32mstring or expression must occur twice in the text. For example, suppose we are looking for the pattern \"the Xer \u001b[0m\n",
       "\u001b[32mthey were, the Xer they will be\", where we want to constrain the two X\\'s to be the same string. We do this by \u001b[0m\n",
       "\u001b[32msurrounding the first X with the parenthesis operator, and replacing the second X with the number operator \\\\1, as \u001b[0m\n",
       "\u001b[32mfollows:\\n/the \u001b[0m\u001b[32m(\u001b[0m\u001b[32m.\u001b[0m\u001b[32m)\u001b[0m\u001b[32mer they were, the \\\\1er they will be/\\nHere the \\\\1 will be replaced by whatever string matched \u001b[0m\n",
       "\u001b[32mthe first item in parentheses. So this will match the bigger they were, the bigger they will be but not the bigger \u001b[0m\n",
       "\u001b[32mthey were, the faster they will be. This use of parentheses to store a pattern in memory is called a capture group.\u001b[0m\n",
       "\u001b[32mcapture group\\nEvery time a capture group is used \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e., parentheses surround a pattern\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, the resulting match is \u001b[0m\n",
       "\u001b[32mstored in a numbered register. If you match two different sets of\\nregister\\nparentheses, \\\\2 means whatever \u001b[0m\n",
       "\u001b[32mmatched the second capture group. Thus\\n/the \u001b[0m\u001b[32m(\u001b[0m\u001b[32m.\u001b[0m\u001b[32m)\u001b[0m\u001b[32mer they \u001b[0m\u001b[32m(\u001b[0m\u001b[32m.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, the \\\\1er we \\\\2/\\nwill match the faster they ran, the\u001b[0m\n",
       "\u001b[32mfaster we ran but not the faster they ran, the faster we ate. Similarly, the third capture group is stored in \\\\3, \u001b[0m\n",
       "\u001b[32mthe fourth is \\\\4, and so on. Parentheses thus have a double function in regular expressions; they are used to \u001b[0m\n",
       "\u001b[32mgroup terms for specifying the order in which operators should apply, and they are used to capture something in a \u001b[0m\n",
       "\u001b[32mregister. Occasionally we might want to use parentheses for grouping, but don\\'t want to capture the resulting \u001b[0m\n",
       "\u001b[32mpattern in a register. In that case we use a non-capturing group, which is specified by putting the special \u001b[0m\n",
       "\u001b[32mnon-capturing group commands ?: after the open parenthesis, in the form \u001b[0m\u001b[32m(\u001b[0m\u001b[32m?: pattern \u001b[0m\u001b[32m)\u001b[0m\u001b[32m. /\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?:some|a few\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mpeople|cats\u001b[0m\u001b[32m)\u001b[0m\u001b[32m like some \\\\1/\\nwill match some cats like some cats but not some cats like some some. Substitutions \u001b[0m\n",
       "\u001b[32mand capture groups are very useful in implementing simple chatbots like ELIZA \u001b[0m\u001b[32m(\u001b[0m\u001b[32mWeizenbaum, 1966\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Recall that ELIZA\u001b[0m\n",
       "\u001b[32msimulates a Rogerian psychologist by carrying on conversations like the following:\\nUser1:\\nMen are all alike. \u001b[0m\n",
       "\u001b[32mELIZA1: IN WHAT WAY\\nUser2:\\nThey\\'re always bugging us about something or other. ELIZA2: CAN YOU THINK OF A \u001b[0m\n",
       "\u001b[32mSPECIFIC EXAMPLE User3:\\nWell, my boyfriend made me come here. ELIZA3: YOUR BOYFRIEND MADE YOU COME HERE User4:\\nHe\u001b[0m\n",
       "\u001b[32msays I\\'m depressed much of the time. ELIZA4: I AM SORRY TO HEAR YOU ARE DEPRESSED\\nELIZA works by having a series \u001b[0m\n",
       "\u001b[32mor cascade of regular expression substitutions each of which matches and changes some part of the input lines. \u001b[0m\n",
       "\u001b[32mAfter the input is uppercased, substitutions change all instances of MY to YOUR, and I\\'M to YOU\\nARE, and so on. \u001b[0m\n",
       "\u001b[32mThat way when ELIZA repeats back part of the user utterance, it will seem to be referring correctly to the user. \u001b[0m\n",
       "\u001b[32mThe next set of substitutions matches and replaces other patterns in the input. Here are some examples:\\ns/. YOU \u001b[0m\n",
       "\u001b[32mARE \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdepressed|sad\u001b[0m\u001b[32m)\u001b[0m\u001b[32m ./I AM SORRY TO HEAR YOU ARE \\\\1/\\ns/. YOU ARE \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdepressed|sad\u001b[0m\u001b[32m)\u001b[0m\u001b[32m ./WHY DO YOU THINK YOU ARE \u001b[0m\n",
       "\u001b[32m\\\\1/\\ns/. all ./IN WHAT WAY/\\ns/. always .*/CAN YOU THINK OF A SPECIFIC EXAMPLE/\\nSince multiple substitutions can \u001b[0m\n",
       "\u001b[32mapply to a given input, substitutions are assigned\\na rank and applied in order. Creating patterns is the topic of \u001b[0m\n",
       "\u001b[32mExercise 2.3, and we\\nreturn to the details of the ELIZA architecture in Chapter 15.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.1.6 Substitution, Capture Groups, And Eliza'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Finally, there will be times when we need to predict the future: look ahead in the text to \u001b[0m\n",
       "\u001b[32msee if some pattern matches, but not yet advance the pointer we always keep to where we are in the text, so that we\u001b[0m\n",
       "\u001b[32mcan then deal with the pattern if it occurs, but if it doesn\\'t we can check for something else instead. These \u001b[0m\n",
       "\u001b[32mlookahead assertions make use of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32m? syntax that we saw in the previlookahead ous section for non-capture \u001b[0m\n",
       "\u001b[32mgroups. The operator \u001b[0m\u001b[32m(\u001b[0m\u001b[32m?= pattern\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is true if pattern occurs, but is zero-width, i.e. the match pointer doesn\\'t \u001b[0m\n",
       "\u001b[32madvance. The operator zero-width\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?! pattern\u001b[0m\u001b[32m)\u001b[0m\u001b[32m only returns true if a pattern does not match, but again is \u001b[0m\n",
       "\u001b[32mzero-width and doesn\\'t advance the pointer. Negative lookahead is commonly used when we are parsing some complex \u001b[0m\n",
       "\u001b[32mpattern but want to rule out a special case. For example suppose we want to match, at the beginning of a line, any \u001b[0m\n",
       "\u001b[32msingle word that doesn\\'t start with \"Volcano\". We can use negative lookahead to do \u001b[0m\n",
       "\u001b[32mthis:\\n/ˆ\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?!Volcano\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-Za-z\u001b[0m\u001b[32m]\u001b[0m\u001b[32m+/'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.1.7 Lookahead Assertions'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"Before\u001b[0m\u001b[32m we talk about processing words, we need to decide what counts as a word. Let's start \u001b[0m\n",
       "\u001b[32mby looking at one particular corpus \u001b[0m\u001b[32m(\u001b[0m\u001b[32mplural corpora\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, a computer-readable corpus corpora collection of text or \u001b[0m\n",
       "\u001b[32mspeech. For example the Brown corpus is a million-word collection of samples from 500 written English texts from \u001b[0m\n",
       "\u001b[32mdifferent genres \u001b[0m\u001b[32m(\u001b[0m\u001b[32mnewspaper, fiction, non-fiction, academic, etc.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, assembled at Brown University in \u001b[0m\n",
       "\u001b[32m1963–64\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mKuˇcera and Francis, 1967\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. How many words are in the following Brown sentence?\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.2 Words'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"He\u001b[0m\u001b[32m stepped out into the hall, was delighted to encounter a water brother. This sentence has \u001b[0m\n",
       "\u001b[32m13 words if we don't count punctuation marks as words, 15\\nif we count punctuation.\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.2 Words'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Whether we treat period \u001b[0m\u001b[32m(\u001b[0m\u001b[32m\".\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, comma \u001b[0m\u001b[32m(\u001b[0m\u001b[32m\",\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, and so on as words depends on the task. \u001b[0m\n",
       "\u001b[32mPunctuation is critical for finding boundaries of things \u001b[0m\u001b[32m(\u001b[0m\u001b[32mcommas, periods, colons\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and for identifying some aspects\u001b[0m\n",
       "\u001b[32mof meaning \u001b[0m\u001b[32m(\u001b[0m\u001b[32mquestion marks, exclamation marks, quotation marks\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. For some tasks, like part-of-speech tagging or \u001b[0m\n",
       "\u001b[32mparsing or speech synthesis, we sometimes treat punctuation marks as if they were separate words. The Switchboard \u001b[0m\n",
       "\u001b[32mcorpus of American English telephone conversations between strangers was collected in the early 1990s; it contains \u001b[0m\n",
       "\u001b[32m2430 conversations averaging 6 minutes each, totaling 240 hours of speech and about 3 million words \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGodfrey et \u001b[0m\n",
       "\u001b[32mal., 1992\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Such corpora of spoken language introduce other complications with regard to defining words. Let\\'s \u001b[0m\n",
       "\u001b[32mlook at one utterance from Switchboard; an utterance is the spoken correlate of a sentence:\\nutterance I do uh \u001b[0m\n",
       "\u001b[32mmain- mainly business data processing This utterance has two kinds of disfluencies. The broken-off word main- is \u001b[0m\n",
       "\u001b[32mdisfluency called a fragment. Words like uh and um are called fillers or filled pauses. Should fragment filled \u001b[0m\n",
       "\u001b[32mpause we consider these to be words? Again, it depends on the application. If we are building a speech \u001b[0m\n",
       "\u001b[32mtranscription system, we might want to eventually strip out the disfluencies. But we also sometimes keep \u001b[0m\n",
       "\u001b[32mdisfluencies around. Disfluencies like uh or um are actually helpful in speech recognition in predicting the \u001b[0m\n",
       "\u001b[32mupcoming word, because they may signal that the speaker is restarting the clause or idea, and so for speech \u001b[0m\n",
       "\u001b[32mrecognition they are treated as regular words. Because people use different disfluencies they can also be a cue to \u001b[0m\n",
       "\u001b[32mspeaker identification. In fact Clark and Fox Tree \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2002\u001b[0m\u001b[32m)\u001b[0m\u001b[32m showed that uh and um have different meanings. What do \u001b[0m\n",
       "\u001b[32myou think they are?'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.2 Words'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Perhaps most important, in thinking about what is a word, we need to distinguish two ways of \u001b[0m\n",
       "\u001b[32mtalking about words that will be useful throughout the book. Word types word type\\nare the number of distinct words\u001b[0m\n",
       "\u001b[32min a corpus; if the set of words in the vocabulary\\nis V, the number of types is the vocabulary size |V|. Word \u001b[0m\n",
       "\u001b[32minstances are the total\\nword instance\\nnumber N of running words.1\\nIf we ignore punctuation, the following Brown \u001b[0m\n",
       "\u001b[32msentence has 16 instances and 14\\ntypes:\\nThey picnicked by the pool, then lay back on the grass and looked at the \u001b[0m\n",
       "\u001b[32mstars. We still have decisions to make!'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.2 Words'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"For\u001b[0m\u001b[32m example, should we consider a capitalized string \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlike They\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and one that is \u001b[0m\n",
       "\u001b[32muncapitalized \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlike they\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to be the same word type? The answer is that it depends on the task! They and they might \u001b[0m\n",
       "\u001b[32mbe lumped together as the same type in some tasks, like speech recognition, where we might just care about getting \u001b[0m\n",
       "\u001b[32mthe words in order and don't care about the formatting, while for other tasks, such as deciding whether a \u001b[0m\n",
       "\u001b[32mparticular word is a noun or verb \u001b[0m\u001b[32m(\u001b[0m\u001b[32mpart-of-speech tagging\u001b[0m\u001b[32m)\u001b[0m\u001b[32m or whether a word is a name of a person or location \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mnamed-entity tagging\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, capitalization is a useful feature and is retained. Sometimes we keep around two versions \u001b[0m\n",
       "\u001b[32mof a particular NLP model, one with capitalization and one without capitalization. How many words are there in \u001b[0m\n",
       "\u001b[32mEnglish? When we speak about the number of words in the language, we are generally referring to word types. Fig. \u001b[0m\n",
       "\u001b[32m2.11 shows the rough numbers of types and instances computed from some English corpora. | Corpus                   \u001b[0m\n",
       "\u001b[32m| Instances =              |\\n|-------------------------------------|--------------------------|\\n| N              \u001b[0m\n",
       "\u001b[32m|                          |\\n| Types =                             |                          |\\n| |              \u001b[0m\n",
       "\u001b[32m|                          |\\n| V                                   |                          |\\n| |              \u001b[0m\n",
       "\u001b[32m|                          |\\n| Shakespeare                         | 884 thousand 31 thousand |\\n| Brown corpus   \u001b[0m\n",
       "\u001b[32m| 1 million 38 thousand    |\\n| Switchboard telephone conversations | 2.4 million 20 thousand  |\\n| COCA           \u001b[0m\n",
       "\u001b[32m| 440 million              |\\n| Google n-grams                      | 1 trillion               |\\nThe larger the \u001b[0m\n",
       "\u001b[32mcorpora we look at, the more word types we find, and in fact this relationship between the number of types |V| and \u001b[0m\n",
       "\u001b[32mnumber of instances N is called Herdan's Law \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHerdan, 1960\u001b[0m\u001b[32m)\u001b[0m\u001b[32m or Heaps' Law \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHeaps, 1978\u001b[0m\u001b[32m)\u001b[0m\u001b[32m after its discoverers \u001b[0m\n",
       "\u001b[32mHerdan's Law Heaps' Law\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32min linguistics and information retrieval respectively\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. It is shown in Eq.\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.2 Words'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"2\u001b[0m\u001b[32m.1, where k and β are positive constants, and 0 < β < 1. $|V|=kN^\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\\\beta\u001b[0m\u001b[32m}\u001b[0m\u001b[32m$ \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2.1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nThe value \u001b[0m\n",
       "\u001b[32mof β depends on the corpus size and the genre, but at least for the large corpora in Fig. 2.11, β ranges from .67 \u001b[0m\n",
       "\u001b[32mto .75. Roughly then we can say that the vocabulary size for a text goes up significantly faster than the square \u001b[0m\n",
       "\u001b[32mroot of its length in words. It's sometimes useful to make a further distinction.\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.2 Words'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"Consider\u001b[0m\u001b[32m inflected forms like cats versus cat. We say these two words are different wordforms\u001b[0m\n",
       "\u001b[32mbut have the same lemma. A lemma is a set of lexical forms having the same stem, the same lemma major \u001b[0m\n",
       "\u001b[32mpart-of-speech, and the same word sense. The wordform is the full inflected wordform or derived form of the word. \u001b[0m\n",
       "\u001b[32mThe two wordforms cat and cats thus have the same lemma, which we can represent as cat. For morphologically complex\u001b[0m\n",
       "\u001b[32mlanguages like Arabic, we often need to deal with lemmatization. For most tasks in English, however, wordforms are \u001b[0m\n",
       "\u001b[32msufficient, and when we talk about words in this book we almost always mean wordsforms \u001b[0m\u001b[32m(\u001b[0m\u001b[32malthough we will discuss \u001b[0m\n",
       "\u001b[32mbasic algorithms for lemmatization and the related task of stemming below in Section 2.6\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. One of the situations \u001b[0m\n",
       "\u001b[32meven in English where we talk about lemmas is when we measure the number of words in a dictionary. Dictionary \u001b[0m\n",
       "\u001b[32mentries or boldface forms are a very rough approximation to \u001b[0m\u001b[32m(\u001b[0m\u001b[32man upper bound on\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the number of lemmas \u001b[0m\u001b[32m(\u001b[0m\u001b[32msince some \u001b[0m\n",
       "\u001b[32mlemmas have multiple boldface forms\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. The 1989 edition of the Oxford English Dictionary had 615,000 entries. \u001b[0m\n",
       "\u001b[32mFinally, we should note that in practice, for many NLP applications \u001b[0m\u001b[32m(\u001b[0m\u001b[32mfor example for neural language modeling\u001b[0m\u001b[32m)\u001b[0m\u001b[32m we \u001b[0m\n",
       "\u001b[32mdon't actually use words as our internal unit of representation at all! We instead tokenize the input strings into \u001b[0m\n",
       "\u001b[32mtokens, which can be words but can also be only parts of words. We'll return to this tokenization question when we \u001b[0m\n",
       "\u001b[32mintroduce the BPE algorithm in Section 2.5.2.\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.2 Words'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"Words\u001b[0m\u001b[32m don't appear out of nowhere. Any particular piece of text that we study is produced by \u001b[0m\n",
       "\u001b[32mone or more specific speakers or writers, in a specific dialect of a specific language, at a specific time, in a \u001b[0m\n",
       "\u001b[32mspecific place, for a specific function. Perhaps the most important dimension of variation is the language. NLP \u001b[0m\n",
       "\u001b[32malgorithms are most useful when they apply across many languages. The world has 7097\\nlanguages at the time of this\u001b[0m\n",
       "\u001b[32mwriting, according to the online Ethnologue catalog\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mSimons and Fennig, 2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. It is important to test algorithms \u001b[0m\n",
       "\u001b[32mon more than one language, and particularly on languages with different properties; by contrast there is an \u001b[0m\n",
       "\u001b[32munfortunate current tendency for NLP algorithms to be developed or tested just on English \u001b[0m\u001b[32m(\u001b[0m\u001b[32mBender, 2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Even when\u001b[0m\n",
       "\u001b[32malgorithms are developed beyond English, they tend to be developed for the official languages of large \u001b[0m\n",
       "\u001b[32mindustrialized nations \u001b[0m\u001b[32m(\u001b[0m\u001b[32mChinese, Spanish, Japanese, German etc.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, but we don't want to limit tools to just these \u001b[0m\n",
       "\u001b[32mfew languages. Furthermore, most languages also have multiple varieties, often spoken in different regions or by \u001b[0m\n",
       "\u001b[32mdifferent social groups. Thus, for example, if we're processing text that uses features of African American English\u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mAAE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m or AAE\\nAfrican American Vernacular English \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAAVE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m—the variations of English used by millions of people in \u001b[0m\n",
       "\u001b[32mAfrican American communities \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKing 2020\u001b[0m\u001b[32m)\u001b[0m\u001b[32m—we must use\\nNLP tools that function with features of those varieties. \u001b[0m\n",
       "\u001b[32mTwitter posts might use features often used by speakers of African American English, such as constructions \u001b[0m\n",
       "\u001b[32mlike\\niont \u001b[0m\u001b[32m(\u001b[0m\u001b[32mI don't in Mainstream American English \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMAE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, or talmbout corresponding\\nMAE\\nto MAE talking about, \u001b[0m\n",
       "\u001b[32mboth examples that influence word segmentation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mBlodgett et al. 2016, Jones 2015\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.3 Corpora'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"It\u001b[0m\u001b[32m's also quite common for speakers or writers to use multiple languages in a\\nsingle \u001b[0m\n",
       "\u001b[32mcommunicative act, a phenomenon called code switching. Code switching\\ncode switching\\nis enormously common across \u001b[0m\n",
       "\u001b[32mthe world; here are examples showing Spanish and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mtransliterated\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Hindi code switching with English \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSolorio et al.\u001b[0m\n",
       "\u001b[32m2014, Jurgens et al.\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.3 Corpora'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'2017\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2.2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nPor primera vez veo a @username actually being hateful! it was \u001b[0m\n",
       "\u001b[32mbeautiful:\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mFor the first time I get to see @username actually being hateful! it was beautiful:\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2.3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ndost \u001b[0m\n",
       "\u001b[32mtha or ra- hega ... dont wory ... but dherya rakhe\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"he was and will remain a friend ... don\\'t worry ...'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.3 Corpora'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'but have faith\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\nAnother dimension of variation is the genre. The text that our algorithms \u001b[0m\n",
       "\u001b[32mmust process might come from newswire, fiction or non-fiction books, scientific articles, Wikipedia, or religious \u001b[0m\n",
       "\u001b[32mtexts. It might come from spoken genres like telephone conversations, business meetings, police body-worn cameras, \u001b[0m\n",
       "\u001b[32mmedical interviews, or transcripts of television shows or movies. It might come from work situations like doctors\\'\u001b[0m\n",
       "\u001b[32mnotes, legal text, or parliamentary or congressional proceedings. Text also reflects the demographic \u001b[0m\n",
       "\u001b[32mcharacteristics of the writer \u001b[0m\u001b[32m(\u001b[0m\u001b[32mor speaker\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: their age, gender, race, socioeconomic class can all influence the \u001b[0m\n",
       "\u001b[32mlinguistic properties of the text we are processing. And finally, time matters too. Language changes over time, and\u001b[0m\n",
       "\u001b[32mfor some languages we have good corpora of texts from different historical periods. Because language is so \u001b[0m\n",
       "\u001b[32msituated, when developing computational models for language processing from a corpus, it\\'s important to consider \u001b[0m\n",
       "\u001b[32mwho produced the language, in what context, for what purpose. How can a user of a dataset know all these details? \u001b[0m\n",
       "\u001b[32mThe best way is for the corpus creator to build a datasheet \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGebru et al., datasheet\\n2020\u001b[0m\u001b[32m)\u001b[0m\u001b[32m or data statement \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mBender et al., 2021\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for each corpus. A datasheet specifies properties of a dataset like:\\nMotivation: Why was the\u001b[0m\n",
       "\u001b[32mcorpus collected, by whom, and who funded it? Situation: When and in what situation was the text written/spoken?'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.3 Corpora'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"For\u001b[0m\u001b[32m example,\\nwas there a task? Was the language originally spoken conversation, edited text,\u001b[0m\n",
       "\u001b[32msocial media communication, monologue vs. dialogue? Language variety: What language \u001b[0m\u001b[32m(\u001b[0m\u001b[32mincluding dialect/region\u001b[0m\u001b[32m)\u001b[0m\u001b[32m was \u001b[0m\n",
       "\u001b[32mthe corpus in? Speaker demographics: What was, e.g., the age or gender of the text's authors? Collection process: \u001b[0m\n",
       "\u001b[32mHow big is the data? If it is a subsample how was it sampled? Was the data collected with consent? How was the data\u001b[0m\n",
       "\u001b[32mpre-processed, and what metadata is available? Annotation process: What are the annotations, what are the \u001b[0m\n",
       "\u001b[32mdemographics of the\\nannotators, how were they trained, how was the data annotated? Distribution: Are there \u001b[0m\n",
       "\u001b[32mcopyright or other intellectual property restrictions?\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.3 Corpora'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"Before\u001b[0m\u001b[32m almost any natural language processing of a text, the text has to be normalized, a \u001b[0m\n",
       "\u001b[32mtask called text normalization. At least three tasks are commonly applied as text normalization part of any \u001b[0m\n",
       "\u001b[32mnormalization process:\\n1. Tokenizing \u001b[0m\u001b[32m(\u001b[0m\u001b[32msegmenting\u001b[0m\u001b[32m)\u001b[0m\u001b[32m words 2. Normalizing word formats 3. Segmenting sentences\\nIn \u001b[0m\n",
       "\u001b[32mthe next sections we walk through each of these tasks, but we'll first start with an easy, if somewhat naive \u001b[0m\n",
       "\u001b[32mversion of word tokenization and normalization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mand frequency computation\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that can be accomplished for English \u001b[0m\n",
       "\u001b[32msolely in a single UNIX command-line, inspired by Church \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1994\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. We'll make use of some Unix commands: tr, used to \u001b[0m\n",
       "\u001b[32msystematically change particular characters in the input; sort, which sorts input lines in alphabetical order; and \u001b[0m\n",
       "\u001b[32muniq, which collapses and counts adjacent identical lines. For example let's begin with the 'complete words' of \u001b[0m\n",
       "\u001b[32mShakespeare in one file, sh.txt. We can use tr to tokenize the words by changing every sequence of nonalphabetic \u001b[0m\n",
       "\u001b[32mcharacters to a newline \u001b[0m\u001b[32m(\u001b[0m\u001b[32m'A-Za-z' means alphabetic and the -c option complements to non-alphabet, so together they \u001b[0m\n",
       "\u001b[32mmean to change every non-alphabetic character into a newline. The -s \u001b[0m\u001b[32m(\u001b[0m\u001b[32m'squeeze'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m option is used to replace the \u001b[0m\n",
       "\u001b[32mresult of multiple consecutive changes into a single output, so a series of non-alphabetic characters in a row \u001b[0m\n",
       "\u001b[32mwould all be 'squeezed' into a single newline\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\ntr -sc 'A-Za-z' '\\\\n' < sh.txt\\nThe output of this command will \u001b[0m\n",
       "\u001b[32mbe:\\nTHE\\nSONNETS\\nby\\nWilliam\\nShakespeare\\nFrom\\nfairest\\ncreatures\\nWe\\n... Now that there is one word per line,\u001b[0m\n",
       "\u001b[32mwe can sort the lines, and pass them to uniq\\n-c which will collapse and count them:\\ntr -sc 'A-Za-z' '\\\\n' < \u001b[0m\n",
       "\u001b[32msh.txt | sort | uniq -c\\nwith the following output:\\n1945 A\\n72 AARON\\n19 ABBESS\\n25 Aaron\\n6 Abate\\n1 Abates\\n5 \u001b[0m\n",
       "\u001b[32mAbbess\\n6 Abbey\\n3 Abbot\\n... Alternatively, we can collapse all the upper case to lower case:\\ntr -sc 'A-Za-z' \u001b[0m\n",
       "\u001b[32m'\\\\n' < sh.txt | tr A-Z a-z | sort | uniq -c\\nwhose output is\\n14725 a\\n97 aaron\\n1 abaissiez\\n10 abandon\\n2 \u001b[0m\n",
       "\u001b[32mabandoned\\n2 abase\\n1 abash\\n14 abate\\n3 abated\\n3 abatement\\n... Now we can sort again to find the frequent words.\u001b[0m\n",
       "\u001b[32mThe -n option to sort means\\nto sort numerically rather than alphabetically, and the -r option means to sort \u001b[0m\n",
       "\u001b[32min\\nreverse order \u001b[0m\u001b[32m(\u001b[0m\u001b[32mhighest-to-lowest\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\ntr -sc 'A-Za-z' '\\\\n' < sh.txt | tr A-Z a-z | sort | uniq -c | sort -n \u001b[0m\n",
       "\u001b[32m-r\\nThe results show that the most frequent words in Shakespeare, as in any other\\ncorpus, are the short function \u001b[0m\n",
       "\u001b[32mwords like articles, pronouns, prepositions:\\n27378 the 26084 and 22538 i 19771 to 17481 of 14725 a 13826 you 12489\u001b[0m\n",
       "\u001b[32mmy 11318 that 11112 in\\n... Unix tools of this sort can be very handy in building quick word count statistics for \u001b[0m\n",
       "\u001b[32many corpus in English.\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.4 Simple Unix Tools For Word Tokenization'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'While in some versions of Unix these command-line tools also correctly handle Unicode \u001b[0m\n",
       "\u001b[32mcharacters and so can be used for many languages, in general for handling most languages outside English we use \u001b[0m\n",
       "\u001b[32mmore sophisticated tokenization algorithms.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.4 Simple Unix Tools For Word Tokenization'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'The simple UNIX tools above were fine for getting rough word statistics but more \u001b[0m\n",
       "\u001b[32msophisticated algorithms are generally necessary for tokenization, the task of segtokenization menting running text\u001b[0m\n",
       "\u001b[32minto words. There are roughly two classes of tokenization algorithms. In top-down tokenization, we define a \u001b[0m\n",
       "\u001b[32mstandard and implement rules to implement that kind of tokenization. In bottom-up tokenization, we use simple \u001b[0m\n",
       "\u001b[32mstatistics of letter sequences to break up words into subword tokens.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.5 Word Tokenization'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'While the Unix command sequence just removed all the numbers and punctuation, for most NLP \u001b[0m\n",
       "\u001b[32mapplications we\\'ll need to keep these in our tokenization. We often want to break off punctuation as a separate \u001b[0m\n",
       "\u001b[32mtoken; commas are a useful piece of information for parsers, periods help indicate sentence boundaries. But we\\'ll \u001b[0m\n",
       "\u001b[32moften want to keep the punctuation that occurs word internally, in examples like m.p.h., Ph.D., AT&T, and cap\\'n. \u001b[0m\n",
       "\u001b[32mSpecial characters and numbers will need to be kept in prices \u001b[0m\u001b[32m(\u001b[0m\u001b[32m$45.55\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and dates \u001b[0m\u001b[32m(\u001b[0m\u001b[32m01/02/06\u001b[0m\u001b[32m)\u001b[0m\u001b[32m; we don\\'t want to \u001b[0m\n",
       "\u001b[32msegment that price into separate tokens of \"45\" and \"55\". And there are URLs \u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://www.stanford.edu\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, Twitter \u001b[0m\n",
       "\u001b[32mhashtags \u001b[0m\u001b[32m(\u001b[0m\u001b[32m#nlproc\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, or email addresses \u001b[0m\u001b[32m(\u001b[0m\u001b[32msomeone@cs.colorado.edu\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Number expressions introduce other complications \u001b[0m\n",
       "\u001b[32mas well; while commas normally appear at word boundaries, commas are used inside numbers in English, every three \u001b[0m\n",
       "\u001b[32mdigits: 555,500.50. Languages, and hence tokenization requirements, differ on this; many continental European \u001b[0m\n",
       "\u001b[32mlanguages like Spanish, French, and German, by contrast, use a comma to mark the decimal point, and spaces \u001b[0m\u001b[32m(\u001b[0m\u001b[32mor \u001b[0m\n",
       "\u001b[32msometimes periods\u001b[0m\u001b[32m)\u001b[0m\u001b[32m where English puts commas, for example, 555 500,50. A tokenizer can also be used to expand \u001b[0m\n",
       "\u001b[32mclitic contractions that are marked by clitic apostrophes, for example, converting what\\'re to the two tokens what \u001b[0m\n",
       "\u001b[32mare, and we\\'re to we are. A clitic is a part of a word that can\\'t stand on its own, and can only occur when it is\u001b[0m\n",
       "\u001b[32mattached to another word. Some such contractions occur in other alphabetic languages, including articles and \u001b[0m\n",
       "\u001b[32mpronouns in French \u001b[0m\u001b[32m(\u001b[0m\u001b[32mj\\'ai, l\\'homme\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Depending on the application, tokenization algorithms may also tokenize \u001b[0m\n",
       "\u001b[32mmultiword expressions like New York or rock \\'n\\' roll as a single token, which requires a multiword expression \u001b[0m\n",
       "\u001b[32mdictionary of some sort. Tokenization is thus intimately tied up with named entity recognition, the task of \u001b[0m\n",
       "\u001b[32mdetecting names, dates, and organizations \u001b[0m\u001b[32m(\u001b[0m\u001b[32mChapter 8\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. One commonly used tokenization standard is known as the Penn\u001b[0m\n",
       "\u001b[32mTreebank tokenization standard, used for the parsed corpora \u001b[0m\u001b[32m(\u001b[0m\u001b[32mtreebanks\u001b[0m\u001b[32m)\u001b[0m\u001b[32m released by the Lin-\\nPenn Treebank guistic\u001b[0m\n",
       "\u001b[32mData Consortium \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLDC\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, the source of many useful datasets. This standard separates out clitics \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdoesn\\'t becomes \u001b[0m\n",
       "\u001b[32mdoes plus n\\'t\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, keeps hyphenated words together, and separates out all punctuation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mto save space we\\'re showing \u001b[0m\n",
       "\u001b[32mvisible spaces\\n\\' \\' between tokens, although newlines is a more common output\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\nInput:\\n\"The San Francisco-based\u001b[0m\n",
       "\u001b[32mrestaurant,\" they said,\\n\"doesn\\'t charge $10\". Output: \" The San Francisco-based restaurant , \" they said ,\\n\" \u001b[0m\n",
       "\u001b[32mdoes n\\'t charge $ 10 \" .'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.5.1 Top-Down \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRule-Based\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Tokenization'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'In practice, since tokenization needs to be run before any other language processing, it \u001b[0m\n",
       "\u001b[32mneeds to be very fast.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.5.1 Top-Down \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRule-Based\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Tokenization'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'The standard method for tokenization is therefore to use deterministic algorithms based on \u001b[0m\n",
       "\u001b[32mregular expressions compiled into very efficient finite state automata. For example, Fig. 2.12 shows an example of \u001b[0m\n",
       "\u001b[32ma basic regular expression that can be used to tokenize English with the nltk.regexp tokenize function of the \u001b[0m\n",
       "\u001b[32mPython-based Natural Language Toolkit \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNLTK\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m(\u001b[0m\u001b[32mBird et al. 2009; https://www.nltk.org\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.5.1 Top-Down \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRule-Based\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Tokenization'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"text\u001b[0m\u001b[32m = 'That U.S.A. poster-print costs $12.40...'\\npattern = r'''\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\n\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.5.1 Top-Down \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRule-Based\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Tokenization'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\u001b[33mpage_content\u001b[0m=\u001b[32m'... \u001b[0m\u001b[32m(\u001b[0m\u001b[32m?:\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-Z\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m+'\u001b[0m, \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m'set flag to allow verbose regexps'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\u001b[33mpage_content\u001b[0m=\u001b[32m'... | \\\\w+\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?:-\\\\w+\u001b[0m\u001b[32m)\u001b[0m\u001b[32m*'\u001b[0m, \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m'abbreviations, e.g. U.S.A.'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'... | \\\\$?\\\\d+\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?:.\\\\d+\u001b[0m\u001b[32m)\u001b[0m\u001b[32m?%?'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m'words with optional internal hyphens'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\u001b[33mpage_content\u001b[0m=\u001b[32m'... | ...'\u001b[0m, \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m'currency, percentages, e.g. $12.40, 82%'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\u001b[33mpage_content\u001b[0m=\u001b[32m'... | \u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m[\u001b[0m\u001b[32m.,;\"\\'?\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:_`-\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m'ellipsis'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"... '''\\n\\n\\n\\nnltk.regexp_tokenize\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtext, pattern\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m'That', 'U.S.A.', 'poster-print', \u001b[0m\n",
       "\u001b[32m'costs', '$12.40', '...'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\nCarefully designed deterministic algorithms can deal with the ambiguities that arise, \u001b[0m\n",
       "\u001b[32msuch as the fact that the apostrophe needs to be tokenized differently when used as a genitive marker \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas in the \u001b[0m\n",
       "\u001b[32mbook's cover\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, a quotative as in 'The other class', she said, or in clitics like they're. Word tokenization is more\u001b[0m\n",
       "\u001b[32mcomplex in languages like written Chinese, Japanese, and Thai, which do not use spaces to mark potential \u001b[0m\n",
       "\u001b[32mword-boundaries.\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m'these are separate tokens; includes \u001b[0m\u001b[32m]\u001b[0m\u001b[32m, \u001b[0m\u001b[32m[\u001b[0m\u001b[32m'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'In Chinese, for example, words are composed of characters \u001b[0m\u001b[32m(\u001b[0m\u001b[32mcalled hanzi in Chinese\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Each \u001b[0m\n",
       "\u001b[32mhanzi\\ncharacter generally represents a single unit of meaning \u001b[0m\u001b[32m(\u001b[0m\u001b[32mcalled a morpheme\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and is\\npronounceable as a \u001b[0m\n",
       "\u001b[32msingle syllable. Words are about 2.4 characters long on average. But deciding what counts as a word in Chinese is \u001b[0m\n",
       "\u001b[32mcomplex. For example, consider the following sentence: \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2.4\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n姚明进入总决赛\\ny´ao m´ıng jın ru zˇong ju´e \u001b[0m\n",
       "\u001b[32ms`ai\\n\"Yao Ming reaches the finals\"\\nAs Chen et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2017\u001b[0m\u001b[32m)\u001b[0m\u001b[32m point out, this could be treated as 3 words \u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'Chinese \u001b[0m\n",
       "\u001b[32mTreebank\\' segmentation\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2.5\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n姚明\\nYaoMing\\n进入\\nreaches\\n总决赛\\nfinals\\nor as 5 words \u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'Peking \u001b[0m\n",
       "\u001b[32mUniversity\\' segmentation\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2.6\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n姚\\nYao\\n明\\nMing\\n进入\\nreaches\\n总\\noverall\\n决赛\\nfinals\\nFinally, it is \u001b[0m\n",
       "\u001b[32mpossible in Chinese simply to ignore words altogether and use characters as the basic elements, treating the \u001b[0m\n",
       "\u001b[32msentence as a series of 7 \u001b[0m\n",
       "\u001b[32mcharacters:\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2.7\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n姚\\nYao\\n明\\nMing\\n进\\nenter\\n入\\nenter\\n总\\noverall\\n决\\ndecision\\n赛\\ngame\\nIn fact, for most\u001b[0m\n",
       "\u001b[32mChinese NLP tasks it turns out to work better to take characters rather than words as input, since characters are \u001b[0m\n",
       "\u001b[32mat a reasonable semantic level for most applications, and since most word standards, by contrast, result in a huge \u001b[0m\n",
       "\u001b[32mvocabulary with large numbers of very rare words \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLi et al., 2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. However, for Japanese and Thai the character is\u001b[0m\n",
       "\u001b[32mtoo small a unit, and so algorithms for word segmentation are required. These can also be useful for Chinese word \u001b[0m\n",
       "\u001b[32msegmentation in the rare situations where word rather than character boundaries are required. The standard \u001b[0m\n",
       "\u001b[32msegmentation algorithms for these languages use neural sequence models trained via supervised machine learning on \u001b[0m\n",
       "\u001b[32mhand-segmented training sets; we\\'ll introduce sequence models in Chapter 8 and Chapter 9. '\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m'these are separate tokens; includes \u001b[0m\u001b[32m]\u001b[0m\u001b[32m, \u001b[0m\u001b[32m[\u001b[0m\u001b[32m'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"There\u001b[0m\u001b[32m is a third option to tokenizing text, one that is most commonly used by large language \u001b[0m\n",
       "\u001b[32mmodels. Instead of defining tokens as words \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwhether delimited by spaces or more complex algorithms\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, or as \u001b[0m\n",
       "\u001b[32mcharacters \u001b[0m\u001b[32m(\u001b[0m\u001b[32mas in Chinese\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, we can use our data to automatically tell us what the tokens should be. This is \u001b[0m\n",
       "\u001b[32mespecially useful in dealing with unknown words, an important problem in language processing. As we will see in the\u001b[0m\n",
       "\u001b[32mnext chapter, NLP algorithms often learn some facts about language from one corpus \u001b[0m\u001b[32m(\u001b[0m\u001b[32ma training corpus\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and then use\u001b[0m\n",
       "\u001b[32mthese facts to make decisions about a separate test corpus and its language. Thus if our training corpus contains, \u001b[0m\n",
       "\u001b[32msay the words low, new, newer, but not lower, then if the word lower appears in our test corpus, our system will \u001b[0m\n",
       "\u001b[32mnot know what to do with it. To deal with this unknown word problem, modern tokenizers automatically induce sets of\u001b[0m\n",
       "\u001b[32mtokens that include tokens smaller than words, called subwords. Subsubwords words can be arbitrary substrings, or \u001b[0m\n",
       "\u001b[32mthey can be meaning-bearing units like the morphemes -est or -er. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mA morpheme is the smallest meaning-bearing unit \u001b[0m\n",
       "\u001b[32mof a language; for example the word unlikeliest has the morphemes un-, likely, and -est.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nIn modern tokenization \u001b[0m\n",
       "\u001b[32mschemes, most tokens are words, but some tokens are frequently occurring morphemes or other subwords like -er. \u001b[0m\n",
       "\u001b[32mEvery unseen word like lower can thus be represented by some sequence of known subword units, such as low and er, \u001b[0m\n",
       "\u001b[32mor even as a sequence of individual letters if necessary. Most tokenization schemes have two parts: a token \u001b[0m\n",
       "\u001b[32mlearner, and a token segmenter. The token learner takes a raw training corpus \u001b[0m\u001b[32m(\u001b[0m\u001b[32msometimes roughly preseparated into \u001b[0m\n",
       "\u001b[32mwords, for example by whitespace\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and induces a vocabulary, a set of tokens. The token segmenter takes a raw test \u001b[0m\n",
       "\u001b[32msentence and segments it into the tokens in the vocabulary. Two algorithms are widely used: byte-pair \u001b[0m\n",
       "\u001b[32mencoding\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mSennrich et al., 2016\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, and unigram language modeling \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKudo, 2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, There is also a SentencePiece \u001b[0m\n",
       "\u001b[32mlibrary that includes implementations of both of these \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKudo and Richardson, 2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, and people often use the name \u001b[0m\n",
       "\u001b[32mSentencePiece to simply mean unigram language modeling tokenization. In this section we introduce the simplest of \u001b[0m\n",
       "\u001b[32mthe three, the byte-pair encoding or BPE algorithm \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSennrich et al., 2016\u001b[0m\u001b[32m)\u001b[0m\u001b[32m; see Fig. 2.13. The BPE token learner \u001b[0m\n",
       "\u001b[32mbegins BPE\\nwith a vocabulary that is just the set of all individual characters. It then examines the training \u001b[0m\n",
       "\u001b[32mcorpus, chooses the two symbols that are most frequently adjacent \u001b[0m\u001b[32m(\u001b[0m\u001b[32msay 'A', 'B'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, adds a new merged symbol 'AB' to \u001b[0m\n",
       "\u001b[32mthe vocabulary, and replaces every adjacent 'A' 'B' in the corpus with the new 'AB'. It continues to count and \u001b[0m\n",
       "\u001b[32mmerge, creating new longer and longer character strings, until k merges have been done creating\\nk novel tokens; k \u001b[0m\n",
       "\u001b[32mis thus a parameter of the algorithm. The resulting vocabulary\\nconsists of the original set of characters plus k \u001b[0m\n",
       "\u001b[32mnew symbols. The algorithm is usually run inside words \u001b[0m\u001b[32m(\u001b[0m\u001b[32mnot merging across word boundaries\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\nso the input corpus \u001b[0m\n",
       "\u001b[32mis first white-space-separated to give a set of strings, each corresponding to the characters of a word, plus a \u001b[0m\n",
       "\u001b[32mspecial end-of-word symbol\\n, and its\\ncounts. Let's see its operation on the following tiny input corpus of 18 \u001b[0m\n",
       "\u001b[32mword tokens with counts for each word \u001b[0m\u001b[32m(\u001b[0m\u001b[32mthe word low appears 5 times, the word newer 6 times,\\nand so on\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, which \u001b[0m\n",
       "\u001b[32mwould have a starting vocabulary of 11 letters:\\n| corpus      | vocabulary                     \u001b[0m\n",
       "\u001b[32m|\\n|-------------|--------------------------------|\\n| 5           |                                |\\n| l o w     \u001b[0m\n",
       "\u001b[32m| , d, e, i, l, n, o, r, s, t, w |\\n| 2           |                                |\\n| l o w e s t |              \u001b[0m\n",
       "\u001b[32m|\\n| 6           |                                |\\n| n e w e r   |                                |\\n| 3         \u001b[0m\n",
       "\u001b[32m|                                |\\n| w i d e r   |                                |\\n| 2           |              \u001b[0m\n",
       "\u001b[32m|\\n| n e w       |                                |\\nThe BPE algorithm first counts all pairs of adjacent symbols: \u001b[0m\n",
       "\u001b[32mthe most frequent is the pair e r because it occurs in newer \u001b[0m\u001b[32m(\u001b[0m\u001b[32mfrequency of 6\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and wider \u001b[0m\u001b[32m(\u001b[0m\u001b[32mfrequency of\\n3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for a \u001b[0m\n",
       "\u001b[32mtotal of 9 occurrences.2 We then merge these symbols, treating er as one symbol, and count again:\\n| corpus      | \u001b[0m\n",
       "\u001b[32mvocabulary                         |\\n|-------------|------------------------------------|\\n| 5           |        \u001b[0m\n",
       "\u001b[32m|\\n| l o w       | , d, e, i, l, n, o, r, s, t, w, er |\\n| 2           |                                    |\\n| l \u001b[0m\n",
       "\u001b[32mo w e s t |                                    |\\n| 6           |                                    |\\n| n e w er \u001b[0m\n",
       "\u001b[32m|                                    |\\n| 3           |                                    |\\n| w i d er    |      \u001b[0m\n",
       "\u001b[32m|\\n| 2           |                                    |\\n| n e w       |                                    |\\nNow \u001b[0m\n",
       "\u001b[32mthe most frequent pair is er\\n, which we merge; our system has learned that there should be a token for word-final \u001b[0m\n",
       "\u001b[32mer, represented as er :\\n| corpus      |\\n|-------------|\\n| 5           |\\n| l o w       |\\n| ,           |\\n| d  \u001b[0m\n",
       "\u001b[32m|\\n| ,           |\\n| e           |\\n| ,           |\\n| i           |\\n| ,           |\\n| l           |\\n| ,       \u001b[0m\n",
       "\u001b[32m|\\n| n           |\\n| ,           |\\n| o           |\\n| ,           |\\n| r           |\\n| ,           |\\n| s       \u001b[0m\n",
       "\u001b[32m|\\n| ,           |\\n| t           |\\n| ,           |\\n| w           |\\n| ,           |\\n| er          |\\n| ,       \u001b[0m\n",
       "\u001b[32m|\\n| er          |\\n| 2           |\\n| l o w e s t |\\n| 6           |\\n| n e w er    |\\n| 3           |\\n| w i d er\u001b[0m\n",
       "\u001b[32m|\\n| 2           |\\n| n e w       |\\nNext n e \u001b[0m\u001b[32m(\u001b[0m\u001b[32mtotal count of 8\u001b[0m\u001b[32m)\u001b[0m\u001b[32m get merged to ne:\\n| corpus      \u001b[0m\n",
       "\u001b[32m|\\n|-------------|\\n| 5           |\\n| l o w       |\\n| ,           |\\n| d           |\\n| ,           |\\n| e       \u001b[0m\n",
       "\u001b[32m|\\n| ,           |\\n| i           |\\n| ,           |\\n| l           |\\n| ,           |\\n| n           |\\n| ,       \u001b[0m\n",
       "\u001b[32m|\\n| o           |\\n| ,           |\\n| r           |\\n| ,           |\\n| s           |\\n| ,           |\\n| t       \u001b[0m\n",
       "\u001b[32m|\\n| ,           |\\n| w           |\\n| ,           |\\n| er          |\\n| ,           |\\n| er          |\\n| ,       \u001b[0m\n",
       "\u001b[32m|\\n| ne          |\\n| 2           |\\n| l o w e s t |\\n| 6           |\\n| ne w er     |\\n| 3           |\\n| w i d er\u001b[0m\n",
       "\u001b[32m|\\n| 2           |\\n| ne w        |\\nIf we continue, the next merges are:\\n| merge      | current vocabulary   \u001b[0m\n",
       "\u001b[32m|\\n|------------|----------------------|\\n| \u001b[0m\u001b[32m(\u001b[0m\u001b[32mne, w\u001b[0m\u001b[32m)\u001b[0m\u001b[32m    |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| d          |                      |\\n| ,          |                      |\\n| e          |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| i          |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| l          |                      |\\n| ,          |                      |\\n| n          |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| o          |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| r          |                      |\\n| ,          |                      |\\n| s          |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| t          |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| w          |                      |\\n| ,          |                      |\\n| er         |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| er         |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| ne         |                      |\\n| ,          |                      |\\n| new        |                    \u001b[0m\n",
       "\u001b[32m|\\n| \u001b[0m\u001b[32m(\u001b[0m\u001b[32ml, o\u001b[0m\u001b[32m)\u001b[0m\u001b[32m     |                      |\\n| ,          |                      |\\n| d          |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| e          |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| i          |                      |\\n| ,          |                      |\\n| l          |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| n          |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| o          |                      |\\n| ,          |                      |\\n| r          |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| s          |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| t          |                      |\\n| ,          |                      |\\n| w          |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| er         |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| er         |                      |\\n| ,          |                      |\\n| ne         |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| new        |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| lo         |                      |\\n| \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlo, w\u001b[0m\u001b[32m)\u001b[0m\u001b[32m    |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| d          |                      |\\n| ,          |                      |\\n| e          |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| i          |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| l          |                      |\\n| ,          |                      |\\n| n          |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| o          |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| r          |                      |\\n| ,          |                      |\\n| s          |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| t          |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| w          |                      |\\n| ,          |                      |\\n| er         |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| er         |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| ne         |                      |\\n| ,          |                      |\\n| new        |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| lo         |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| low        |                      |\\n| \u001b[0m\u001b[32m(\u001b[0m\u001b[32mnew, er \u001b[0m\u001b[32m)\u001b[0m\u001b[32m |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| d          |                      |\\n| ,          |                      |\\n| e          |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| i          |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| l          |                      |\\n| ,          |                      |\\n| n          |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| o          |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| r          |                      |\\n| ,          |                      |\\n| s          |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| t          |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| w          |                      |\\n| ,          |                      |\\n| er         |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| er         |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| ne         |                      |\\n| ,          |                      |\\n| new        |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| lo         |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| low        |                      |\\n| ,          |                      |\\n| newer      |                    \u001b[0m\n",
       "\u001b[32m|\\n| \u001b[0m\u001b[32m(\u001b[0m\u001b[32mlow,      | \u001b[0m\u001b[32m)\u001b[0m\u001b[32m                    |\\n| ,          |                      |\\n| d          |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| e          |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| i          |                      |\\n| ,          |                      |\\n| l          |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| n          |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| o          |                      |\\n| ,          |                      |\\n| r          |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| s          |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| t          |                      |\\n| ,          |                      |\\n| w          |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| er         |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| er         |                      |\\n| ,          |                      |\\n| ne         |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| new        |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| lo         |                      |\\n| ,          |                      |\\n| low        |                    \u001b[0m\n",
       "\u001b[32m|\\n| ,          |                      |\\n| newer      |                      |\\n| ,          |                    \u001b[0m\n",
       "\u001b[32m|\\n| low        |                      |\\nOnce we've learned our vocabulary, the token segmenter is used to \u001b[0m\n",
       "\u001b[32mtokenize a test sentence. The token segmenter just runs on the test data the merges we have learned from the \u001b[0m\n",
       "\u001b[32mtraining data, greedily, in the order we learned them.\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'Header 1'\u001b[0m: \u001b[32m'these are separate tokens; includes \u001b[0m\u001b[32m]\u001b[0m\u001b[32m, \u001b[0m\u001b[32m[\u001b[0m\u001b[32m'\u001b[0m,\n",
       "            \u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.5.2 Byte-Pair Encoding: A Bottom-Up Tokenization Algorithm'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"\u001b[0m\u001b[32m(\u001b[0m\u001b[32mThus the frequencies in the test data don't play a role, just the frequencies in the \u001b[0m\n",
       "\u001b[32mtraining data\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. So first we segment each test sentence word into characters.\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'Header 1'\u001b[0m: \u001b[32m'these are separate tokens; includes \u001b[0m\u001b[32m]\u001b[0m\u001b[32m, \u001b[0m\u001b[32m[\u001b[0m\u001b[32m'\u001b[0m,\n",
       "            \u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.5.2 Byte-Pair Encoding: A Bottom-Up Tokenization Algorithm'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Then we apply the first rule: replace every instance of e r in the test corpus with er, and \u001b[0m\n",
       "\u001b[32mthen the second rule: replace every instance of er in the test corpus with er , and so on. function BYTE-PAIR \u001b[0m\n",
       "\u001b[32mENCODING\u001b[0m\u001b[32m(\u001b[0m\u001b[32mstrings C, number of merges k\u001b[0m\u001b[32m)\u001b[0m\u001b[32m returns vocab V\\nV←all unique characters in C'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'Header 1'\u001b[0m: \u001b[32m'these are separate tokens; includes \u001b[0m\u001b[32m]\u001b[0m\u001b[32m, \u001b[0m\u001b[32m[\u001b[0m\u001b[32m'\u001b[0m,\n",
       "            \u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.5.2 Byte-Pair Encoding: A Bottom-Up Tokenization Algorithm'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'tNEW ←tL + tR'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m'merge tokens k times tL, tR ←Most frequent pair of adjacent tokens in C'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'By the end, if the test corpus contained the character sequence n e w e r\\n, it would be \u001b[0m\n",
       "\u001b[32mtokenized as a full word. But the characters of a new \u001b[0m\u001b[32m(\u001b[0m\u001b[32munknown\u001b[0m\u001b[32m)\u001b[0m\u001b[32m word like l o w e r would be merged into the two \u001b[0m\n",
       "\u001b[32mtokens low er . Of course in real settings BPE is run with many thousands of merges on a very large input corpus. \u001b[0m\n",
       "\u001b[32mThe result is that most words will be represented as full symbols, and only the very rare words \u001b[0m\u001b[32m(\u001b[0m\u001b[32mand unknown words\u001b[0m\u001b[32m)\u001b[0m\n",
       "\u001b[32mwill have to be represented by their parts.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m'and update the corpus return V'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Word normalization is the task of putting words/tokens in a standard format. The \u001b[0m\n",
       "\u001b[32mnormalization simplest case of word normalization is case folding. Mapping everything to lower case folding case \u001b[0m\n",
       "\u001b[32mmeans that Woodchuck and woodchuck are represented identically, which is very helpful for generalization in many \u001b[0m\n",
       "\u001b[32mtasks, such as information retrieval or speech recognition. For sentiment analysis and other text classification \u001b[0m\n",
       "\u001b[32mtasks, information extraction, and machine translation, by contrast, case can be quite helpful and case folding is \u001b[0m\n",
       "\u001b[32mgenerally not done. This is because maintaining the difference between, for example, US the country and us the \u001b[0m\n",
       "\u001b[32mpronoun can outweigh the advantage in generalization that case folding would have provided for other words. Systems\u001b[0m\n",
       "\u001b[32mthat use BPE or other kinds of bottom-up tokenization may do no further word normalization. In other NLP systems, \u001b[0m\n",
       "\u001b[32mwe may want to do further normalizations, like choosing a single normal form for words with multiple forms like USA\u001b[0m\n",
       "\u001b[32mand US or uh-huh and uhhuh. This standardization may be valuable, despite the spelling information that is lost in \u001b[0m\n",
       "\u001b[32mthe normalization process. For information retrieval or information extraction about the US, we might want to see \u001b[0m\n",
       "\u001b[32minformation from documents whether they mention the US or the USA.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'Header 1'\u001b[0m: \u001b[32m'and update the corpus return V'\u001b[0m,\n",
       "            \u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.6 Word Normalization, Lemmatization And Stemming'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'For other natural language processing situations we also want two morphologically different \u001b[0m\n",
       "\u001b[32mforms of a word to behave similarly. For example in web search, someone may type the string woodchucks but a useful\u001b[0m\n",
       "\u001b[32msystem might want to also return pages that mention woodchuck with no s. This is especially common in \u001b[0m\n",
       "\u001b[32mmorphologically complex languages like Polish, where for example the word Warsaw has different endings when it is \u001b[0m\n",
       "\u001b[32mthe subject \u001b[0m\u001b[32m(\u001b[0m\u001b[32mWarszawa\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, or after a preposition like \"in Warsaw\" \u001b[0m\u001b[32m(\u001b[0m\u001b[32mw Warszawie\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, or \"to Warsaw\" \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdo Warszawy\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, and so\u001b[0m\n",
       "\u001b[32mon. Lemmatization is the task lemmatization of determining that two words have the same root, despite their surface\u001b[0m\n",
       "\u001b[32mdifferences. The words am, are, and is have the shared lemma be; the words dinner and dinners both have the lemma \u001b[0m\n",
       "\u001b[32mdinner. Lemmatizing each of these forms to the same lemma will let us find all mentions of words in Polish like \u001b[0m\n",
       "\u001b[32mWarsaw. The lemmatized form of a sentence like He is reading detective stories would thus be He be read detective \u001b[0m\n",
       "\u001b[32mstory. How is lemmatization done?'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m'and update the corpus return V'\u001b[0m, \u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.6.1 Lemmatization'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'The most sophisticated methods for lemmatization involve complete morphological parsing of \u001b[0m\n",
       "\u001b[32mthe word. Morphology is the study of the way words are built up from smaller meaning-bearing units called \u001b[0m\n",
       "\u001b[32mmorphemes. morpheme Two broad classes of morphemes can be distinguished: stems—the central morstem pheme of the \u001b[0m\n",
       "\u001b[32mword, supplying the main meaning—and affixes—adding \"additional\"\\naffix meanings of various kinds. So, for example,\u001b[0m\n",
       "\u001b[32mthe word fox consists of one morpheme\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mthe morpheme fox\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and the word cats consists of two: the morpheme cat and \u001b[0m\n",
       "\u001b[32mthe morpheme -s. A morphological parser takes a word like cats and parses it into the two morphemes cat and s, or \u001b[0m\n",
       "\u001b[32mparses a Spanish word like amaren \u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'if in the future they would love\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m into the morpheme amar \\'to love\\', and \u001b[0m\n",
       "\u001b[32mthe morphological features\\n3PL and future subjunctive.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m'and update the corpus return V'\u001b[0m, \u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.6.1 Lemmatization'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"Lemmatization\u001b[0m\u001b[32m algorithms can be complex. For this reason we sometimes make use of a simpler \u001b[0m\n",
       "\u001b[32mbut cruder method, which mainly consists of chopping off wordfinal affixes. This naive version of morphological \u001b[0m\n",
       "\u001b[32manalysis is called stemming. For stemming example, the Porter stemmer, a widely used stemming algorithm \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPorter, \u001b[0m\n",
       "\u001b[32m1980\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, Porter stemmer when applied to the following paragraph:\\nThis was not the map we found in Billy Bones's \u001b[0m\n",
       "\u001b[32mchest, but an accurate copy, complete in all things-names and heights and soundings-with the single exception of \u001b[0m\n",
       "\u001b[32mthe red crosses and the written notes. produces the following stemmed output:\\nThi wa not the map we found in Billi\u001b[0m\n",
       "\u001b[32mBone s chest but an accur copi complet in all thing name and height and sound with the singl except of the red \u001b[0m\n",
       "\u001b[32mcross and the written note The algorithm is based on series of rewrite rules run in series: the output of each pass\u001b[0m\n",
       "\u001b[32mis fed as input to the next pass. Here are some sample rules \u001b[0m\u001b[32m(\u001b[0m\u001b[32mmore details can be found at \u001b[0m\n",
       "\u001b[32mhttps://tartarus.org/martin/PorterStemmer/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\nATIONAL → ATE \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., relational → relate\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nING → ϵ\\nif the stem \u001b[0m\n",
       "\u001b[32mcontains a vowel \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., motoring → motor\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nSSES → SS\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., grasses → grass\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nSimple stemmers can be useful in \u001b[0m\n",
       "\u001b[32mcases where we need to collapse across different variants of the same lemma. Nonetheless, they do tend to commit \u001b[0m\n",
       "\u001b[32merrors of both over- and under-generalizing, as shown in the table below \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKrovetz, 1993\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:\\n| Errors of Commission  \u001b[0m\n",
       "\u001b[32m| Errors of Omission   |\\n|------------------------|----------------------|\\n| organization organ     | European \u001b[0m\n",
       "\u001b[32mEurope      |\\n| doing                  | doe                  |\\n| numerical              | numerous             \u001b[0m\n",
       "\u001b[32m|\\n| policy                 | police               |\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m'and update the corpus return V'\u001b[0m, \u001b[32m'Header 2'\u001b[0m: \u001b[32m'Stemming: The Porter Stemmer'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Sentence segmentation is another important step in text processing. The most usesentence \u001b[0m\n",
       "\u001b[32msegmentation ful cues for segmenting a text into sentences are punctuation, like periods, question marks, and \u001b[0m\n",
       "\u001b[32mexclamation points. Question marks and exclamation points are relatively unambiguous markers of sentence \u001b[0m\n",
       "\u001b[32mboundaries.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m'and update the corpus return V'\u001b[0m, \u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.7 Sentence Segmentation'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Periods, on the other hand, are more ambiguous. The period character \".\" is ambiguous between\u001b[0m\n",
       "\u001b[32ma sentence boundary marker and a marker of abbreviations like Mr. or Inc. The previous sentence that you just read \u001b[0m\n",
       "\u001b[32mshowed an even more complex case of this ambiguity, in which the final period of Inc. marked both an abbreviation \u001b[0m\n",
       "\u001b[32mand the sentence boundary marker. For this reason, sentence tokenization and word tokenization may be addressed \u001b[0m\n",
       "\u001b[32mjointly. In general, sentence tokenization methods work by first deciding \u001b[0m\u001b[32m(\u001b[0m\u001b[32mbased on rules or machine learning\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mwhether a period is part of the word or is a sentence-boundary marker. An abbreviation dictionary can help \u001b[0m\n",
       "\u001b[32mdetermine whether the period is part of a commonly used abbreviation; the dictionaries can be hand-built or \u001b[0m\n",
       "\u001b[32mmachinelearned \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKiss and Strunk, 2006\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, as can the final sentence splitter. In the Stanford CoreNLP toolkit \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mManning et al., 2014\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, for example sentence splitting is rule-based, a deterministic consequence of tokenization; \u001b[0m\n",
       "\u001b[32ma sentence ends when a sentence-ending punctuation \u001b[0m\u001b[32m(\u001b[0m\u001b[32m., !, or ?\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is not already grouped with other characters into a\u001b[0m\n",
       "\u001b[32mtoken \u001b[0m\u001b[32m(\u001b[0m\u001b[32msuch as for an abbreviation or number\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, optionally followed by additional final quotes or brackets.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m'and update the corpus return V'\u001b[0m, \u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.7 Sentence Segmentation'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"Much\u001b[0m\u001b[32m of natural language processing is concerned with measuring how similar two strings are. \u001b[0m\n",
       "\u001b[32mFor example in spelling correction, the user typed some erroneous string—let's say graffe–and we want to know what \u001b[0m\n",
       "\u001b[32mthe user meant. The user probably intended a word that is similar to graffe. Among candidate similar words, the \u001b[0m\n",
       "\u001b[32mword giraffe, which differs by only one letter from graffe, seems intuitively to be more similar than, say grail or\u001b[0m\n",
       "\u001b[32mgraf, which differ in more letters. Another example comes from coreference, the task of deciding whether two \u001b[0m\n",
       "\u001b[32mstrings such as the following refer to the same entity:\\nStanford President Marc Tessier-Lavigne Stanford \u001b[0m\n",
       "\u001b[32mUniversity President Marc Tessier-Lavigne Again, the fact that these two strings are very similar \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdiffering by \u001b[0m\n",
       "\u001b[32monly one word\u001b[0m\u001b[32m)\u001b[0m\u001b[32m seems like useful evidence for deciding that they might be coreferent. Edit distance gives us a way \u001b[0m\n",
       "\u001b[32mto quantify both of these intuitions about string similarity. More formally, the minimum edit distance between two \u001b[0m\n",
       "\u001b[32mstrings is defined minimum edit distance as the minimum number of editing operations \u001b[0m\u001b[32m(\u001b[0m\u001b[32moperations like insertion, \u001b[0m\n",
       "\u001b[32mdeletion, substitution\u001b[0m\u001b[32m)\u001b[0m\u001b[32m needed to transform one string into another. The gap between intention and execution, for \u001b[0m\n",
       "\u001b[32mexample, is 5 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdelete an i, substitute e for n, substitute x for t, insert c, substitute u for n\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. It's much easier\u001b[0m\n",
       "\u001b[32mto see this by looking at the most important visualization for string distances, an alignment alignment between the\u001b[0m\n",
       "\u001b[32mtwo strings, shown in Fig. 2.14. Given two sequences, an alignment is a correspondence between substrings of the \u001b[0m\n",
       "\u001b[32mtwo sequences. Thus, we say I aligns with the empty string, N with E, and so on. Beneath the aligned strings is \u001b[0m\n",
       "\u001b[32manother representation; a series of symbols expressing an operation list for converting the top string into the \u001b[0m\n",
       "\u001b[32mbottom string: d for deletion, s for substitution, i for insertion. We can also assign a particular cost or weight \u001b[0m\n",
       "\u001b[32mto each of these operations. The Levenshtein distance between two sequences is the simplest weighting factor in \u001b[0m\n",
       "\u001b[32mwhich each of the three operations has a cost of 1 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLevenshtein, 1966\u001b[0m\u001b[32m)\u001b[0m\u001b[32m—we assume that the substitution of a letter \u001b[0m\n",
       "\u001b[32mfor itself, for example, t for t, has zero cost. The Levenshtein distance between intention and execution is 5. \u001b[0m\n",
       "\u001b[32mLevenshtein also proposed an alternative version of his metric in which each insertion or deletion has a cost of 1 \u001b[0m\n",
       "\u001b[32mand substitutions are not allowed. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mThis is equivalent to allowing substitution, but giving each substitution a \u001b[0m\n",
       "\u001b[32mcost of 2 since any substitution can be represented by one insertion and one deletion\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Using this version, the \u001b[0m\n",
       "\u001b[32mLevenshtein distance between intention and execution is 8.\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m'and update the corpus return V'\u001b[0m, \u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.8 Minimum Edit Distance'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"How\u001b[0m\u001b[32m do we find the minimum edit distance? We can think of this as a search task, in which we \u001b[0m\n",
       "\u001b[32mare searching for the shortest path—a sequence of edits—from one string to another. i n t e n t i o \u001b[0m\n",
       "\u001b[32mn\\ndel\\nins\\nsubst\\nn t e n t i o n\\ni n t e c n t i o n\\ni n x e n t i o n\\nThe space of all possible edits is \u001b[0m\n",
       "\u001b[32menormous, so we can't search naively. However, lots of distinct edit paths will end up in the same state \u001b[0m\u001b[32m(\u001b[0m\u001b[32mstring\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32mso rather than recomputing all those paths, we could just remember the shortest path to a state each time we saw \u001b[0m\n",
       "\u001b[32mit. We can do this by using dynamic programming. Dynamic programming is the name for a class of algorithms, first \u001b[0m\n",
       "\u001b[32mintroduced by Bellman \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1957\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, that apply a table-driven method to solve problems by combining solutions to \u001b[0m\n",
       "\u001b[32msubproblems. Some of the most commonly used algorithms in natural language processing make use of dynamic \u001b[0m\n",
       "\u001b[32mprogramming, such as the Viterbi algorithm \u001b[0m\u001b[32m(\u001b[0m\u001b[32mChapter 8\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and the CKY algorithm for parsing \u001b[0m\u001b[32m(\u001b[0m\u001b[32mChapter 17\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. The \u001b[0m\n",
       "\u001b[32mintuition of a dynamic programming problem is that a large problem can be solved by properly combining the \u001b[0m\n",
       "\u001b[32msolutions to various subproblems. Consider the shortest path of transformed words that represents the minimum edit \u001b[0m\n",
       "\u001b[32mdistance between the strings intention and execution shown in Fig. 2.16. Imagine some string \u001b[0m\u001b[32m(\u001b[0m\u001b[32mperhaps it is \u001b[0m\n",
       "\u001b[32mexention\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that is in this optimal path \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwhatever it is\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. The intuition of dynamic programming is that if exention \u001b[0m\n",
       "\u001b[32mis in the optimal\\ni n t e n t i o n\\ndelete i\\nn t e n t i o n\\nsubstitute n by e\\ne t e n t i o n\\nsubstitute t \u001b[0m\n",
       "\u001b[32mby x\\ne x e n t i o n\\ninsert u\\ne x e n u t i o n\\nsubstitute n by c\\ne x e c u t i o n\\noperation list, then the \u001b[0m\n",
       "\u001b[32moptimal sequence must also include the optimal path from intention to exention. Why?\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'Header 1'\u001b[0m: \u001b[32m'and update the corpus return V'\u001b[0m,\n",
       "            \u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.8.1 The Minimum Edit Distance Algorithm'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"If\u001b[0m\u001b[32m there were a shorter path from intention to exention, then we could use it instead, \u001b[0m\n",
       "\u001b[32mresulting in a shorter overall path, and the optimal sequence wouldn't be optimal, thus leading to a contradiction.\u001b[0m\n",
       "\u001b[32mThe minimum edit distance algorithm was named by Wagner and Fischer\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m1974\u001b[0m\u001b[32m)\u001b[0m\u001b[32m but independently discovered by many \u001b[0m\n",
       "\u001b[32mpeople \u001b[0m\u001b[32m(\u001b[0m\u001b[32msee the Historical Notes section of Chapter 8\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Let's first define the minimum edit distance between two \u001b[0m\n",
       "\u001b[32mstrings. Given two strings, the source string X of length n, and target string Y of length m, we'll define D\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi, j\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mas the edit distance between X\u001b[0m\u001b[32m[\u001b[0m\u001b[32m1..i\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and Y\u001b[0m\u001b[32m[\u001b[0m\u001b[32m1.. j\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, i.e., the first i characters of X and the first j characters of \u001b[0m\n",
       "\u001b[32mY. The edit distance between X and Y is thus D\u001b[0m\u001b[32m[\u001b[0m\u001b[32mn,m\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. We'll use dynamic programming to compute D\u001b[0m\u001b[32m[\u001b[0m\u001b[32mn,m\u001b[0m\u001b[32m]\u001b[0m\u001b[32m bottom up, \u001b[0m\n",
       "\u001b[32mcombining solutions to subproblems. In the base case, with a source substring of length i but an empty target \u001b[0m\n",
       "\u001b[32mstring, going from i characters to 0 requires i deletes. With a target substring of length j but an empty source \u001b[0m\n",
       "\u001b[32mgoing from 0 characters to j characters requires j inserts. Having computed D\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi, j\u001b[0m\u001b[32m]\u001b[0m\u001b[32m for small i, j we then compute \u001b[0m\n",
       "\u001b[32mlarger D\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi, j\u001b[0m\u001b[32m]\u001b[0m\u001b[32m based on previously computed smaller values. The value of D\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi, j\u001b[0m\u001b[32m]\u001b[0m\u001b[32m is computed by taking the minimum \u001b[0m\n",
       "\u001b[32mof the three possible paths through the matrix which arrive there:\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2.8\u001b[0m\u001b[32m)\u001b[0m\u001b[32m D\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi, j\u001b[0m\u001b[32m]\u001b[0m\u001b[32m = min \\uf8f1 \\uf8f2 D\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi−1, \u001b[0m\n",
       "\u001b[32mj\u001b[0m\u001b[32m]\u001b[0m\u001b[32m+del-cost\u001b[0m\u001b[32m(\u001b[0m\u001b[32msource\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m D\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi, j −1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m+ins-cost\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtarget\u001b[0m\u001b[32m[\u001b[0m\u001b[32mj\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m D\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi−1, j −1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m+sub-cost\u001b[0m\u001b[32m(\u001b[0m\u001b[32msource\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,target\u001b[0m\u001b[32m[\u001b[0m\u001b[32mj\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \\uf8f3 If we \u001b[0m\n",
       "\u001b[32massume the version of Levenshtein distance in which the insertions and deletions each have a cost of $1$ \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32min-cost\u001b[0m\u001b[32m(\u001b[0m\u001b[32m$\\\\cdot$\u001b[0m\u001b[32m)\u001b[0m\u001b[32m = del-cost\u001b[0m\u001b[32m(\u001b[0m\u001b[32m$\\\\cdot$\u001b[0m\u001b[32m)\u001b[0m\u001b[32m = $1$\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, and substitutions have a cost of $2$ \u001b[0m\u001b[32m(\u001b[0m\u001b[32mexcept substitution of \u001b[0m\n",
       "\u001b[32midentical letters have zero cost\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, the computation for $D\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi,j\u001b[0m\u001b[32m]\u001b[0m\u001b[32m$ \u001b[0m\n",
       "\u001b[32mbecomes:\\n$$D\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi,j\u001b[0m\u001b[32m]\u001b[0m\u001b[32m=\\\\min\\\\left\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\\\begin\u001b[0m\u001b[32m{\u001b[0m\u001b[32marray\u001b[0m\u001b[32m}\u001b[0m\u001b[32m{\u001b[0m\u001b[32mll\u001b[0m\u001b[32m}\u001b[0m\u001b[32mD\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi-1,j\u001b[0m\u001b[32m]\u001b[0m\u001b[32m+1\\\\ D\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi,j-1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m+1\\\\ \u001b[0m\n",
       "\u001b[32mD\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi-1,j-1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m+\\\\left\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\\\begin\u001b[0m\u001b[32m{\u001b[0m\u001b[32marray\u001b[0m\u001b[32m}\u001b[0m\u001b[32m{\u001b[0m\u001b[32mll\u001b[0m\u001b[32m}\u001b[0m\u001b[32m2;&\\\\mbox\u001b[0m\u001b[32m{\u001b[0m\u001b[32mif\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\\\;\\\\;source\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\\\neq target\u001b[0m\u001b[32m[\u001b[0m\u001b[32mj\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\\\ \u001b[0m\n",
       "\u001b[32m0;&\\\\mbox\u001b[0m\u001b[32m{\u001b[0m\u001b[32mif\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\\\;\\\\;source\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi\u001b[0m\u001b[32m]\u001b[0m\u001b[32m=target\u001b[0m\u001b[32m[\u001b[0m\u001b[32mj\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\\\end\u001b[0m\u001b[32m{\u001b[0m\u001b[32marray\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\\\right.\\\\end\u001b[0m\u001b[32m{\u001b[0m\u001b[32marray\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\\\right.\\\\tag\u001b[0m\u001b[32m{\u001b[0m\u001b[32m2.9\u001b[0m\u001b[32m}\u001b[0m\u001b[32m$$\\nThe algorithm is \u001b[0m\n",
       "\u001b[32msummarized in Fig. 2.17; Fig.\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'Header 1'\u001b[0m: \u001b[32m'and update the corpus return V'\u001b[0m,\n",
       "            \u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.8.1 The Minimum Edit Distance Algorithm'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'2.18 shows the results of applying the algorithm to the distance between intention and \u001b[0m\n",
       "\u001b[32mexecution with the version of Levenshtein in Eq. 2.9. Alignment Knowing the minimum edit distance is useful for \u001b[0m\n",
       "\u001b[32malgorithms like finding potential spelling error corrections. But the edit distance algorithm is important in \u001b[0m\n",
       "\u001b[32manother way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two \u001b[0m\n",
       "\u001b[32mstrings is useful throughout speech and'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'Header 1'\u001b[0m: \u001b[32m'and update the corpus return V'\u001b[0m,\n",
       "            \u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.8.1 The Minimum Edit Distance Algorithm'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'n←LENGTH\u001b[0m\u001b[32m(\u001b[0m\u001b[32msource\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nm←LENGTH\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtarget\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nCreate a distance matrix D\u001b[0m\u001b[32m[\u001b[0m\u001b[32mn+1,m+1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m # Initialization: the\u001b[0m\n",
       "\u001b[32mzeroth row and column is the distance from the empty string D\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0,0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m = 0\\nfor each row i from 1 to n do \u001b[0m\n",
       "\u001b[32mD\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi,0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m←D\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi-1,0\u001b[0m\u001b[32m]\u001b[0m\u001b[32m + del-cost\u001b[0m\u001b[32m(\u001b[0m\u001b[32msource\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor each column j from 1 to m do D\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0,j\u001b[0m\u001b[32m]\u001b[0m\u001b[32m←D\u001b[0m\u001b[32m[\u001b[0m\u001b[32m0,j-1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m + ins-cost\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtarget\u001b[0m\u001b[32m[\u001b[0m\u001b[32mj\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'Header 1'\u001b[0m: \u001b[32m'and update the corpus return V'\u001b[0m,\n",
       "            \u001b[32m'Header 2'\u001b[0m: \u001b[32m'Function Min-Edit-Distance\u001b[0m\u001b[32m(\u001b[0m\u001b[32mSource, *Target*\u001b[0m\u001b[32m)\u001b[0m\u001b[32m **Returns** Min-Distance'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'for each row i from 1 to n do for each column j from 1 to m do D\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi,j\u001b[0m\u001b[32m]\u001b[0m\u001b[32m←MIN\u001b[0m\u001b[32m(\u001b[0m\u001b[32m D\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi−1, j\u001b[0m\u001b[32m]\u001b[0m\u001b[32m + \u001b[0m\n",
       "\u001b[32mdel-cost\u001b[0m\u001b[32m(\u001b[0m\u001b[32msource\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, D\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi−1, j−1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m + sub-cost\u001b[0m\u001b[32m(\u001b[0m\u001b[32msource\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,target\u001b[0m\u001b[32m[\u001b[0m\u001b[32mj\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, D\u001b[0m\u001b[32m[\u001b[0m\u001b[32mi, j−1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m + ins-cost\u001b[0m\u001b[32m(\u001b[0m\u001b[32mtarget\u001b[0m\u001b[32m[\u001b[0m\u001b[32mj\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m'Recurrence relation:'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\u001b[33mpage_content\u001b[0m=\u001b[32m'Src\\\\Tar'\u001b[0m, \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m'Termination return D\u001b[0m\u001b[32m[\u001b[0m\u001b[32mn,m\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"e\u001b[0m\u001b[32m\\nx\\ne\\nc\\nu\\nt\\ni\\no\\nn\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\ni\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n6\\n7\\n8\\nn\\n\u001b[0m\n",
       "\u001b[32m2\\n3\\n4\\n5\\n6\\n7\\n8\\n7\\n8\\n7\\nt\\n3\\n4\\n5\\n6\\n7\\n8\\n7\\n8\\n9\\n8\\ne\\n4\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n9\\nn\\n5\\n4\\n5\\n6\\n7\\n\u001b[0m\n",
       "\u001b[32m8\\n9\\n10\\n11\\n10\\nt\\n6\\n5\\n6\\n7\\n8\\n9\\n8\\n9\\n10\\n11\\ni\\n7\\n6\\n7\\n8\\n9\\n10\\n9\\n8\\n9\\n10\\no\\n8\\n7\\n8\\n9\\n10\\n11\\n10\\n\u001b[0m\n",
       "\u001b[32m9\\n8\\n9\\nn\\n9\\n8\\n9\\n10\\n11\\n12\\n11\\n10\\n9\\n8\\nlanguage processing. In speech recognition, minimum edit distance \u001b[0m\n",
       "\u001b[32malignment is used to compute the word error rate \u001b[0m\u001b[32m(\u001b[0m\u001b[32mChapter 16\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Alignment plays a role in machine translation, in \u001b[0m\n",
       "\u001b[32mwhich sentences in a parallel corpus \u001b[0m\u001b[32m(\u001b[0m\u001b[32ma corpus with a text in two languages\u001b[0m\u001b[32m)\u001b[0m\u001b[32m need to be matched to each other. To \u001b[0m\n",
       "\u001b[32mextend the edit distance algorithm to produce an alignment, we can start by visualizing an alignment as a path \u001b[0m\n",
       "\u001b[32mthrough the edit distance matrix. Figure 2.19 shows this path with boldfaced cells. Each boldfaced cell represents \u001b[0m\n",
       "\u001b[32man alignment of a pair of letters in the two strings. If two boldfaced cells occur in the same row, there will be \u001b[0m\n",
       "\u001b[32man insertion in going from the source to the target; two boldfaced cells in the same column indicate a deletion. \u001b[0m\n",
       "\u001b[32mFigure 2.19 also shows the intuition of how to compute this alignment path. The computation proceeds in two steps. \u001b[0m\n",
       "\u001b[32mIn the first step, we augment the minimum edit distance algorithm to store backpointers in each cell. The \u001b[0m\n",
       "\u001b[32mbackpointer from a cell points to the previous cell \u001b[0m\u001b[32m(\u001b[0m\u001b[32mor cells\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that we came from in entering the current cell. \u001b[0m\n",
       "\u001b[32mWe've shown a schematic of these backpointers in Fig. 2.19. Some cells have multiple backpointers because the \u001b[0m\n",
       "\u001b[32mminimum extension could have come from multiple previous cells. In the second step, we perform a backtrace. In a \u001b[0m\n",
       "\u001b[32mbacktrace, we start backtrace from the last cell \u001b[0m\u001b[32m(\u001b[0m\u001b[32mat the final row and column\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, and follow the pointers back \u001b[0m\n",
       "\u001b[32mthrough the dynamic programming matrix. Each complete path between the final cell and the initial cell is a minimum\u001b[0m\n",
       "\u001b[32mdistance alignment. Exercise 2.7 asks you to modify the minimum edit distance algorithm to store the pointers and \u001b[0m\n",
       "\u001b[32mcompute the backtrace to output an alignment. e\\nx\\ne\\nc\\nu\\nt\\ni\\no\\nn\\n0\\n← 1\\n← 2\\n← 3\\n← 4\\n← 5\\n← 6\\n← 7\\n← \u001b[0m\n",
       "\u001b[32m8\\n← 9\\ni\\n↑ 1\\n↖←↑ 2\\n↖←↑ 3\\n↖←↑ 4\\n↖←↑ 5\\n↖←↑ 6\\n↖←↑ 7\\n↖ 6\\n← 7\\n← 8\\nn\\n↑ 2\\n↖←↑ 3\\n↖←↑ 4\\n↖←↑ 5\\n↖←↑ 6\\n↖←↑ \u001b[0m\n",
       "\u001b[32m7\\n↖←↑ 8\\n↑ 7\\n↖←↑ 8\\n↖ 7\\nt\\n↑ 3\\n↖←↑ 4\\n↖←↑ 5\\n↖←↑ 6\\n↖←↑ 7\\n↖←↑ 8\\n↖ 7\\n←↑ 8\\n↖←↑ 9\\n↑ 8\\ne\\n↑ 4\\n↖ 3\\n← 4\\n↖← \u001b[0m\n",
       "\u001b[32m5\\n← 6\\n← 7\\n←↑ 8\\n↖←↑ 9\\n↖←↑ 10\\n↑ 9\\nn\\n↑ 5\\n↑ 4\\n↖←↑ 5\\n↖←↑ 6\\n↖←↑ 7\\n↖←↑ 8\\n↖←↑ 9\\n↖←↑ 10\\n↖←↑ 11 ↖↑ 10\\nt\\n↑ \u001b[0m\n",
       "\u001b[32m6\\n↑ 5\\n↖←↑ 6\\n↖←↑ 7\\n↖←↑ 8\\n↖←↑ 9\\n↖ 8\\n← 9\\n← 10 ←↑ 11\\ni\\n↑ 7\\n↑ 6\\n↖←↑ 7\\n↖←↑ 8\\n↖←↑ 9\\n↖←↑ 10\\n↑ 9\\n↖ 8\\n← \u001b[0m\n",
       "\u001b[32m9\\n← 10\\no\\n↑ 8\\n↑ 7\\n↖←↑ 8\\n↖←↑ 9\\n↖←↑ 10\\n↖←↑ 11\\n↑ 10\\n↑ 9\\n↖ 8\\n← 9\\nn\\n↑ 9\\n↑ 8\\n↖←↑ 9\\n↖←↑ 10\\n↖←↑ 11\\n↖←↑ \u001b[0m\n",
       "\u001b[32m12\\n↑ 11\\n↑ 10\\n↑ 9\\n↖ 8\\nWhile we worked our example with simple Levenshtein distance, the algorithm in Fig.\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'2.17 allows arbitrary weights on the operations. For spelling correction, for example, \u001b[0m\n",
       "\u001b[32msubstitutions are more likely to happen between letters that are next to each other on the keyboard. The Viterbi \u001b[0m\n",
       "\u001b[32malgorithm is a probabilistic extension of minimum edit distance. Instead of computing the \"minimum edit distance\" \u001b[0m\n",
       "\u001b[32mbetween two strings, Viterbi computes the \"maximum probability alignment\" of one string with another. We\\'ll \u001b[0m\n",
       "\u001b[32mdiscuss this more in Chapter 8.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"This\u001b[0m\u001b[32m chapter introduced a fundamental tool in language processing, the regular expression, \u001b[0m\n",
       "\u001b[32mand showed how to perform basic text normalization tasks including word segmentation and normalization, sentence \u001b[0m\n",
       "\u001b[32msegmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings.\u001b[0m\n",
       "\u001b[32mHere's a summary of the main points we covered about these ideas:\\n- The regular expression language is a powerful \u001b[0m\n",
       "\u001b[32mtool for pattern-matching. - Basic operations in regular expressions include concatenation of symbols,\\ndisjunction\u001b[0m\n",
       "\u001b[32mof symbols \u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, |, and .\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, counters \u001b[0m\u001b[32m(\u001b[0m\u001b[32m, +, and \u001b[0m\u001b[32m{\u001b[0m\u001b[32mn,m\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, anchors\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mˆ, $\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and precedence operators \u001b[0m\u001b[32m(\u001b[0m\u001b[32m(\u001b[0m\u001b[32m,\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. - Word \u001b[0m\n",
       "\u001b[32mtokenization and normalization are generally done by cascades of\\nsimple regular expression substitutions or finite\u001b[0m\n",
       "\u001b[32mautomata. - The Porter algorithm is a simple and efficient way to do stemming, stripping\\noff affixes. It does not \u001b[0m\n",
       "\u001b[32mhave high accuracy but may be useful for some tasks.\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m''\u001b[0m, \u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.9 Summary'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'- The minimum edit distance between two strings is the minimum number of\\noperations it takes\u001b[0m\n",
       "\u001b[32mto edit one into the other. Minimum edit distance can be\\ncomputed by dynamic programming, which also results in an\u001b[0m\n",
       "\u001b[32malignment* of\\nthe two strings.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m''\u001b[0m, \u001b[32m'Header 2'\u001b[0m: \u001b[32m'2.9 Summary'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Kleene 1951; 1956 first defined regular expressions and the finite automaton, based on the \u001b[0m\n",
       "\u001b[32mMcCulloch-Pitts neuron. Ken Thompson was one of the first to build regular expressions compilers into editors for \u001b[0m\n",
       "\u001b[32mtext searching \u001b[0m\u001b[32m(\u001b[0m\u001b[32mThompson, 1968\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. His editor ed included a command \"g/regular expression/p\", or Global Regular \u001b[0m\n",
       "\u001b[32mExpression Print, which later became the Unix grep utility. Text normalization algorithms have been applied since \u001b[0m\n",
       "\u001b[32mthe beginning of the field. One of the earliest widely used stemmers was Lovins \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1968\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Stemming was also applied \u001b[0m\n",
       "\u001b[32mearly to the digital humanities, by Packard \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1973\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, who built an affix-stripping morphological parser for Ancient \u001b[0m\n",
       "\u001b[32mGreek. Currently a wide variety of code for tokenization and normalization is available, such as the Stanford \u001b[0m\n",
       "\u001b[32mTokenizer \u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://nlp.stanford.edu/software/tokenizer.shtml\u001b[0m\u001b[32m)\u001b[0m\u001b[32m or specialized tokenizers for Twitter \u001b[0m\u001b[32m(\u001b[0m\u001b[32mO\\'Connor et \u001b[0m\n",
       "\u001b[32mal., 2010\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, or for sentiment \u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttp: //sentiment.christopherpotts.net/tokenizing.html\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. See Palmer \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2012\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfor a \u001b[0m\n",
       "\u001b[32msurvey of text preprocessing. NLTK is an essential tool that offers both useful Python libraries \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mhttps://www.nltk.org\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and textbook descriptions \u001b[0m\u001b[32m(\u001b[0m\u001b[32mBird et al.,\\n2009\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of many algorithms including text \u001b[0m\n",
       "\u001b[32mnormalization and corpus interfaces. For more on Herdan\\'s law and Heaps\\' Law, see Herdan \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1960, p.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m''\u001b[0m, \u001b[32m'Header 2'\u001b[0m: \u001b[32m'Bibliographical And Historical Notes'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'28\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, Heaps\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m1978\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, Egghe \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2007\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and Baayen \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2001\u001b[0m\u001b[32m)\u001b[0m\u001b[32m; Yasseri et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2012\u001b[0m\u001b[32m)\u001b[0m\u001b[32m discuss the \u001b[0m\n",
       "\u001b[32mrelationship with other measures of linguistic complexity. For more on edit distance, see the excellent Gusfield \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32m1997\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Our example measuring the edit distance from \\'intention\\' to \\'execution\\' was adapted from Kruskal \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32m1983\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. There are various publicly available packages to compute edit distance, including Unix diff and the NIST \u001b[0m\n",
       "\u001b[32msclite program \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNIST, 2005\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. In his autobiography Bellman \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1984\u001b[0m\u001b[32m)\u001b[0m\u001b[32m explains how he originally came up with the term \u001b[0m\n",
       "\u001b[32mdynamic programming:\\n\"...The 1950s were not good years for mathematical research. \u001b[0m\u001b[32m[\u001b[0m\u001b[32mthe\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\nSecretary of Defense \u001b[0m\n",
       "\u001b[32m...had a pathological fear and hatred of the word, research... I decided therefore to use the word, \"programming\". \u001b[0m\n",
       "\u001b[32mI\\nwanted to get across the idea that this was dynamic, this was multistage... I thought, let\\'s ... take a word \u001b[0m\n",
       "\u001b[32mthat has an absolutely precise meaning, namely dynamic... it\\'s impossible to use the word, dynamic, in a \u001b[0m\n",
       "\u001b[32mpejorative sense. Try thinking of some combination that will possibly give it a pejorative meaning. It\\'s \u001b[0m\n",
       "\u001b[32mimpossible. Thus, I thought dynamic programming was a good name. It was something not even a Congressman could \u001b[0m\n",
       "\u001b[32mobject to.\"'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m''\u001b[0m, \u001b[32m'Header 2'\u001b[0m: \u001b[32m'Bibliographical And Historical Notes'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'2.1\\nWrite regular expressions for the following languages. 1. the set of all alphabetic \u001b[0m\n",
       "\u001b[32mstrings; 2. the set of all lower case alphabetic strings ending in a b;\\n3. the set of all strings from the \u001b[0m\n",
       "\u001b[32malphabet a,b such that each a is immediately preceded by and immediately followed by a b;\\n2.2\\nWrite regular \u001b[0m\n",
       "\u001b[32mexpressions for the following languages. By \"word\", we mean an alphabetic string separated from other words by \u001b[0m\n",
       "\u001b[32mwhitespace, any relevant\\npunctuation, line breaks, and so forth. 1. the set of all strings with two consecutive \u001b[0m\n",
       "\u001b[32mrepeated words \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \"Humbert Humbert\" and \"the the\" but not \"the bug\" or \"the big bug\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\n2. all strings that \u001b[0m\n",
       "\u001b[32mstart at the beginning of the line with an integer and that\\nend at the end of the line with a word;\\n3. all \u001b[0m\n",
       "\u001b[32mstrings that have both the word grotto and the word raven in them\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mbut not, e.g., words like grottos that merely \u001b[0m\n",
       "\u001b[32mcontain the word grotto\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;\\n4. write a pattern that places the first word of an English sentence in a\\nregister. \u001b[0m\n",
       "\u001b[32mDeal with punctuation. 2.3\\nImplement an ELIZA-like program, using substitutions such as those described on page \u001b[0m\n",
       "\u001b[32m10. You might want to choose a different domain than a Rogerian psychologist, although keep in mind that you would \u001b[0m\n",
       "\u001b[32mneed a domain in which your program can legitimately engage in a lot of simple repetition. 2.4\\nCompute the edit \u001b[0m\n",
       "\u001b[32mdistance \u001b[0m\u001b[32m(\u001b[0m\u001b[32musing insertion cost 1, deletion cost 1, substitution cost 1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of \"leda\" to \"deal\". Show your work \u001b[0m\u001b[32m(\u001b[0m\u001b[32musing \u001b[0m\n",
       "\u001b[32mthe edit distance grid\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. 2.5\\nFigure out whether drive is closer to brief or to divers and what the edit distance \u001b[0m\n",
       "\u001b[32mis to each. You may use any version of distance that you like. 2.6\\nNow implement a minimum edit distance algorithm\u001b[0m\n",
       "\u001b[32mand use your hand-computed results to check your code. 2.7\\nAugment the minimum edit distance algorithm to output \u001b[0m\n",
       "\u001b[32man alignment; you will need to store pointers and add a stage to compute the backtrace. Baayen, R.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m''\u001b[0m, \u001b[32m'Header 2'\u001b[0m: \u001b[32m'Exercises'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'H. 2001. Word frequency distributions. Springer. Bellman, R. 1957. Dynamic Programming. \u001b[0m\n",
       "\u001b[32mPrinceton University Press. Kleene, S. C.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m''\u001b[0m, \u001b[32m'Header 2'\u001b[0m: \u001b[32m'Exercises'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'1956. Representation of events in nerve nets\\nand finite automata. In C. Shannon and J. \u001b[0m\n",
       "\u001b[32mMcCarthy, editors, Automata Studies, pages 3–41. Princeton University Press. Bellman, R. 1984. Eye of the \u001b[0m\n",
       "\u001b[32mHurricane: an autobiography. World Scientific Singapore. Krovetz, R. 1993. Viewing morphology as an inference \u001b[0m\n",
       "\u001b[32mprocess. SIGIR-93. Bender, E. M. 2019. The #BenderRule: On naming the languages we study and why it matters. Blog \u001b[0m\n",
       "\u001b[32mpost. Kruskal, J. B. 1983. An overview of sequence comparison. In D. Sankoff and J.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m''\u001b[0m, \u001b[32m'Header 2'\u001b[0m: \u001b[32m'Exercises'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'B. Kruskal, editors, Time\\nWarps, String Edits, and Macromolecules:\\nThe Theory and Practice \u001b[0m\n",
       "\u001b[32mof Sequence Comparison, pages 1–44. Addison-Wesley.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m''\u001b[0m, \u001b[32m'Header 2'\u001b[0m: \u001b[32m'Exercises'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"Bender\u001b[0m\u001b[32m, E. M., B. Friedman, and A. McMillan-Major. 2021. A guide for writing data statements \u001b[0m\n",
       "\u001b[32mfor natural language processing. Available at http://techpolicylab.uw. edu/data-statements/. Bird, S., E. Klein, \u001b[0m\n",
       "\u001b[32mand E. Loper. 2009. Natural Language\\nProcessing with Python. O'Reilly. Kudo, T. 2018. Subword regularization: \u001b[0m\n",
       "\u001b[32mImproving neural\\nnetwork translation models with multiple subword candidates. ACL. Blodgett, S. L., L. Green, and \u001b[0m\n",
       "\u001b[32mB. O'Connor. 2016. Demographic dialectal variation in social media: A case study of African-American English. \u001b[0m\n",
       "\u001b[32mEMNLP. Kudo, T. and J. Richardson. 2018. SentencePiece: A simple\\nand language independent subword tokenizer and \u001b[0m\n",
       "\u001b[32mdetokenizer for neural text processing. EMNLP. Bostrom, K. and G. Durrett.\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m''\u001b[0m, \u001b[32m'Header 2'\u001b[0m: \u001b[32m'Exercises'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m\"2020\u001b[0m\u001b[32m. Byte pair encoding is\\nsuboptimal for language model pretraining. Findings of EMNLP. \u001b[0m\n",
       "\u001b[32mKuˇcera, H. and W. N. Francis. 1967. Computational Analysis of Present-Day American English. Brown University \u001b[0m\n",
       "\u001b[32mPress, Providence, RI. Chen, X., Z. Shi, X. Qiu, and X. Huang. 2017. Adversarial multi-criteria learning for \u001b[0m\n",
       "\u001b[32mChinese word segmentation. ACL. Levenshtein, V. I. 1966. Binary codes capable of correcting deletions, insertions, \u001b[0m\n",
       "\u001b[32mand reversals. Cybernetics and Control Theory, 10\u001b[0m\u001b[32m(\u001b[0m\u001b[32m8\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:707–710. Original in Doklady\\nAkademii Nauk SSSR 163\u001b[0m\u001b[32m(\u001b[0m\u001b[32m4\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: \u001b[0m\n",
       "\u001b[32m845–848 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1965\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Church, K. W. 1994. Unix for Poets. Slides from 2nd EL-\\nSNET Summer School and unpublished paper \u001b[0m\n",
       "\u001b[32mms. Li, X., Y. Meng, X. Sun, Q. Han, A. Yuan, and J. Li. 2019. Clark, H. H. and J. E. Fox Tree. 2002. Using uh and \u001b[0m\n",
       "\u001b[32mum in\\nspontaneous speaking. Cognition, 84:73–111. Lovins, J. B. 1968. Development of a stemming algorithm. \u001b[0m\n",
       "\u001b[32mMechanical Translation and Computational Linguistics,\\n11\u001b[0m\u001b[32m(\u001b[0m\u001b[32m1–2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:9–13. Egghe, L. 2007. Untangling Herdan's law and \u001b[0m\n",
       "\u001b[32mHeaps'\\nlaw: Mathematical and informetric arguments. JASIST,\\n58\u001b[0m\u001b[32m(\u001b[0m\u001b[32m5\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:702–709. Manning, C. D., M. Surdeanu, J. Bauer,\u001b[0m\n",
       "\u001b[32mJ. Finkel, S. Bethard,\\nand D. McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. ACL. \u001b[0m\n",
       "\u001b[32mGebru, T., J. Morgenstern, B. Vecchione, J. W. Vaughan,\\nH. Wallach, H. Daum´e III, and K. Crawford. 2020. \u001b[0m\n",
       "\u001b[32mDatasheets for datasets. ArXiv. NIST. 2005. Speech recognition scoring toolkit \u001b[0m\u001b[32m(\u001b[0m\u001b[32msctk\u001b[0m\u001b[32m)\u001b[0m\u001b[32m version 2.1. \u001b[0m\n",
       "\u001b[32mhttp://www.nist.gov/speech/tools/. Godfrey, J., E. Holliman, and J. McDaniel. 1992. SWITCH-\\nBOARD: Telephone \u001b[0m\n",
       "\u001b[32mspeech corpus for research and development. ICASSP. O'Connor, B., M. Krieger, and D. Ahn. 2010. \u001b[0m\n",
       "\u001b[32mTweetmotif:\\nExploratory search and topic summarization for twitter. ICWSM. Gusfield, D. 1997. Algorithms on \u001b[0m\n",
       "\u001b[32mStrings, Trees, and Sequences: Computer Science and Computational Biology. Cambridge University Press. Packard, D. \u001b[0m\n",
       "\u001b[32mW. 1973. Computer-assisted morphological\\nanalysis of ancient Greek. COLING. Heaps, H. S. 1978. Information \u001b[0m\n",
       "\u001b[32mretrieval. Computational and\\ntheoretical aspects. Academic Press. Herdan, G. 1960. Type-token mathematics. Mouton.\u001b[0m\n",
       "\u001b[32mPalmer, D. 2012. Text preprocessing. In N. Indurkhya and\\nF. J. Damerau, editors, Handbook of Natural Language \u001b[0m\n",
       "\u001b[32mProcessing, pages 9–30. CRC Press. Porter, M. F.\"\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m''\u001b[0m, \u001b[32m'Header 2'\u001b[0m: \u001b[32m'Exercises'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'1980. An algorithm for suffix stripping. Program, 14\u001b[0m\u001b[32m(\u001b[0m\u001b[32m3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:130–137. Jones, T. 2015. Toward a \u001b[0m\n",
       "\u001b[32mdescription of African American\\nVernacular English dialect regions using \"Black Twitter\". American Speech, \u001b[0m\n",
       "\u001b[32m90\u001b[0m\u001b[32m(\u001b[0m\u001b[32m4\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:403–440. Sennrich, R., B. Haddow, and A. Birch. 2016. Neural machine translation of rare words with subword \u001b[0m\n",
       "\u001b[32munits. ACL. Jurgens, D., Y. Tsvetkov, and D. Jurafsky. 2017. Incorporating dialectal variability for socially \u001b[0m\n",
       "\u001b[32mequitable language identification. ACL. Simons, G. F. and C. D. Fennig. 2018. Ethnologue: Languages of the world, \u001b[0m\n",
       "\u001b[32m21st edition. SIL International. King, S. 2020. From African American Vernacular English\\nto African American \u001b[0m\n",
       "\u001b[32mLanguage: Rethinking the study of race and language in African Americans\\' speech. Annual\\nReview of Linguistics, \u001b[0m\n",
       "\u001b[32m6:285–300. Solorio, T., E. Blair, S. Maharjan, S. Bethard, M. Diab,\\nM. Ghoneim, A. Hawwari, F. AlGhamdi, J. \u001b[0m\n",
       "\u001b[32mHirschberg, A. Chang, and P. Fung. 2014. Overview for the first\\nshared task on language identification in \u001b[0m\n",
       "\u001b[32mcode-switched data. First Workshop on Computational Approaches to Code Switching. Kiss, T. and J. Strunk. 2006. \u001b[0m\n",
       "\u001b[32mUnsupervised multilingual\\nsentence boundary detection. Computational Linguistics,\\n32\u001b[0m\u001b[32m(\u001b[0m\u001b[32m4\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:485–525. Thompson, K. \u001b[0m\n",
       "\u001b[32m1968. Regular expression search algorithm. CACM, 11\u001b[0m\u001b[32m(\u001b[0m\u001b[32m6\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:419–422. Kleene, S. C. 1951. Representation of events in \u001b[0m\n",
       "\u001b[32mnerve nets\\nand finite automata. Technical Report RM-704, RAND Corporation. RAND Research Memorandum. Wagner, R. A.\u001b[0m\n",
       "\u001b[32mand M. J. Fischer. 1974. The string-to-string\\ncorrection problem. Journal of the ACM, 21:168–173. Is word \u001b[0m\n",
       "\u001b[32msegmentation necessary for deep learning of Chinese representations? ACL. Weizenbaum, J. 1966. ELIZA - A computer \u001b[0m\n",
       "\u001b[32mprogram for the\\nstudy of natural language communication between man\\nand machine. CACM, 9\u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:36–45. Weizenbaum, J.\u001b[0m\n",
       "\u001b[32m1976. Computer Power and Human Reason: From Judgement to Calculation. W.H. Freeman and\\nCompany. Yasseri, T., A. \u001b[0m\n",
       "\u001b[32mKornai, and J. Kert´esz. 2012. A practical approach to language complexity: a Wikipedia case study. PLoS ONE, \u001b[0m\n",
       "\u001b[32m7\u001b[0m\u001b[32m(\u001b[0m\u001b[32m11\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'Header 1'\u001b[0m: \u001b[32m''\u001b[0m, \u001b[32m'Header 2'\u001b[0m: \u001b[32m'Exercises'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community import embeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from markdown import markdown\n",
    "import re\n",
    "\n",
    "def markdown_to_text(markdown_string):\n",
    "    html = markdown(markdown_string)\n",
    "    html = re.sub(r'<pre>(.*?)</pre>', ' ', html)\n",
    "    html = re.sub(r'<code>(.*?)</code >', ' ', html)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = ''.join(soup.findAll(string=True))\n",
    "    return text\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "text = str()\n",
    "with open(\"../data/cleaned/2.md\", \"r\") as file:\n",
    "    text = file.read()\n",
    "    text = text.strip()\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    paragraphs = [markdown_to_text(para).replace(\"\\n\", \" \") for para in paragraphs]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(text)\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings(), breakpoint_threshold_type=\"interquartile\"\n",
    ")\n",
    "\n",
    "list_of_strings = [markdown_to_text(split.page_content) for split in md_header_splits]\n",
    "metadatas = [split.metadata for split in md_header_splits]\n",
    "\n",
    "docs = text_splitter.create_documents(list_of_strings, metadatas=metadatas)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30c017e7-1361-4bed-b98a-bedba0f31fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=docs,\n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Create a multiple choice question (MCQ) and solution that covers the following topic: {topic}\n",
    "\n",
    "Only make use of the textbook content below: {context} \n",
    "\"\"\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a lecturer for an advanced undergraduate natural language processing course.\n",
    "Your goal is to create a multiple choice exam question that comprehensively evaluates\n",
    "students' understanding of natural language processing concepts,\n",
    "their ability to apply theoretical knowledge to practical situations,\n",
    "and their capacity for critical analysis and problem-solving in complex scenarios.\n",
    "\n",
    "The source textbook for this course is \"Speech and Language Processing\" (3rd ed., 2022)\n",
    "by Dan Jurafsky and James H. Martin. The questions should be constructed from the content \n",
    "retrieved from the textbook. \n",
    "\n",
    "For each question, you should:\n",
    "- Provide a detailed solution that explains the thought process, reasoning,\n",
    "  and step-by-step approach required to arrive at the correct answer. \n",
    "  \n",
    "- The solution should demonstrate a deep understanding of the underlying\n",
    "  concepts and their practical applications. The solution must be deduced from the knowledge \n",
    "  obtained from the textbook and you should explain how. \n",
    "\n",
    "The question itself should meet the following criteria:\n",
    "- Be a multiple choice question (MCQ) with 5 choices in markdown format:\n",
    "  1. Choice 1\n",
    "  2. Choice 2\n",
    "  3. Choice 3\n",
    "  4. Choice 4\n",
    "  5. Choice 5\n",
    "\n",
    "- Should utilize the content given to you, which includes relevant textbook material. \n",
    "\n",
    "- A student reading the textbook should be able to figure out the answer for the question. Don't go \n",
    "  beyond the materials of the textbook at all. \n",
    "\n",
    "- Incorporate both theoretical concepts and practical applications of natural language\n",
    "  processing topics covered in the course.\n",
    "\n",
    "- Be of a high difficulty level, challenging students to apply their knowledge in novel\n",
    "  and complex scenarios, rather than relying on rote memorization or simple recall.\n",
    "\n",
    "- Require a unique synthesis of ideas from multiple topics, concepts, and sources,\n",
    "  going beyond questions commonly found in standard textbooks. \n",
    "\n",
    "- Have choices that are challenging and non-obvious, making the correct answer difficult\n",
    "  to deduce without a deep understanding of the concepts and their practical implications.\n",
    "\n",
    "- Your output should only be in markdown format, with the following headers:\n",
    "  ## Question\n",
    "  ## Solution\n",
    "  ## Reasoning\n",
    "\n",
    "- Inline equations should use the markdown format: $a = b + c$\n",
    "- Block equations should use the markdown format: $$a = b + c$$\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-0125-preview\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", user_prompt)\n",
    "])\n",
    "\n",
    "chain = (\n",
    "    { \"topic\": RunnablePassthrough(), \"context\": retriever | format_docs } | prompt | llm | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1394eee-8207-40e9-b590-842dd77ee1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = str(chain.invoke(\"Text Normalization for Lemmatization\"))\n",
    "with open(\"../responsebuffers/test.md\", \"w\") as file:\n",
    "    file.write(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43b3bdb4-65e4-4cd6-89d2-eeeb27332aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07 - 10\n",
      "10 - 10\n",
      "16 - 10\n",
      "06 - 10\n",
      "17 - 10\n",
      "02 - 10\n",
      "05 - 10\n",
      "18 - 10\n",
      "19 - 9\n",
      "04 - 10\n",
      "09 - 20\n",
      "21 - 10\n",
      "03 - 10\n",
      "08 - 11\n",
      "20 - 10\n",
      "15 - 10\n",
      "01 - 10\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from itertools import accumulate\n",
    "\n",
    "DIRECTORY = \"../responsebuffers/zeroshot-vanilla-gpt4\"\n",
    "\n",
    "testfilter = lambda x: x != \"test\"\n",
    "directories = list(filter(testfilter, os.listdir(DIRECTORY)))\n",
    "\n",
    "for directory in directories:\n",
    "    print(f\"{directory} - {len(os.listdir(os.path.join(DIRECTORY, directory)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b981a6c-06bd-461d-a746-ad813a61ebd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
