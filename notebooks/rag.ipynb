{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a42720-bbad-4a0f-896c-c87d508ec862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"fyp-rag-experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff8547c-ae36-4f75-a971-fd5c80ab9e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ede46152-22a7-45dc-a128-38009a8e1b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "#### INDEXING ####\n",
    "from bs4 import BeautifulSoup\n",
    "from markdown import markdown\n",
    "import re\n",
    "\n",
    "def markdown_to_text(markdown_string):\n",
    "    html = markdown(markdown_string)\n",
    "    html = re.sub(r'<pre>(.*?)</pre>', ' ', html)\n",
    "    html = re.sub(r'<code>(.*?)</code >', ' ', html)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = ''.join(soup.findAll(string=True))\n",
    "    return text\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "texts = list()\n",
    "for file_path in os.listdir(\"../data/cleaned\"):\n",
    "    if not file_path.endswith(\".md\"):\n",
    "        continue\n",
    "\n",
    "    with open(\"../data/cleaned/\" + file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "        text = text.strip()\n",
    "        texts.append(text)\n",
    "\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deaa59b5-6e3e-4b81-b1aa-2e8d3179ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "document_chunks = list()\n",
    "for text in texts:\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    md_header_splits = markdown_splitter.split_text(text)\n",
    "    \n",
    "    text_splitter = SemanticChunker(\n",
    "        OpenAIEmbeddings(), breakpoint_threshold_type=\"interquartile\"\n",
    "    )\n",
    "    \n",
    "    list_of_strings = [markdown_to_text(split.page_content) for split in md_header_splits]\n",
    "    metadatas = [split.metadata for split in md_header_splits]\n",
    "    \n",
    "    document_chunks.extend(text_splitter.create_documents(list_of_strings, metadatas=metadatas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbba1a31-05fd-49b1-9fa6-4f9a0aedce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     document_chunks, \n",
    "#     embedding=OpenAIEmbeddings(), \n",
    "#     persist_directory=\"./slp_vectordb\"\n",
    "# )\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=OpenAIEmbeddings(), \n",
    "    persist_directory=\"./slp_vectordb\"\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "755b604b-42ea-4e35-8570-a20a1a876948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[Document(page_content='Sequence labelling tasks, such as part-of-speech tagging or BIO-based named entity recognition, follow the same basic classification approach. Here, the final output vector corresponding to each input token is passed to a classifier that produces a softmax distribution over the possible set of tags. Again, assuming a simple classifier consisting of a single feedforward layer followed by a softmax, the set of weights to be learned for this additional layer is WK ∈ Rk×dh, where k is the number of possible tags for the task. As with RNNs, a greedy approach, where the argmax tag for each token is taken as a likely answer, can be used to generate the final output tag sequence. Fig. 11.11 illustrates an example of this approach. yi = softmax(WKzi)\\n(11.14)\\nti = argmaxk(yi)\\n(11.15)\\nAlternatively, the distribution over labels provided by the softmax for each input token can be passed to a conditional random field (CRF) layer which can take global tag-level transitions into account. A complication with this approach arises from the use of subword tokenization such as WordPiece, SentencePiece Unigram LM or Byte Pair Encoding.', metadata={'Header 2': '11.4.3 Sequence Labelling'}), Document(page_content='Sequence labelling tasks, such as part-of-speech tagging or BIO-based named entity recognition, follow the same basic classification approach. Here, the final output vector corresponding to each input token is passed to a classifier that produces a softmax distribution over the possible set of tags. Again, assuming a simple classifier consisting of a single feedforward layer followed by a softmax, the set of weights to be learned for this additional layer is WK ∈ Rk×dh, where k is the number of possible tags for the task. As with RNNs, a greedy approach, where the argmax tag for each token is taken as a likely answer, can be used to generate the final output tag sequence. Fig. 11.11 illustrates an example of this approach. yi = softmax(WKzi)\\n(11.14)\\nti = argmaxk(yi)\\n(11.15)\\nAlternatively, the distribution over labels provided by the softmax for each input token can be passed to a conditional random field (CRF) layer which can take global tag-level transitions into account. A complication with this approach arises from the use of subword tokenization such as WordPiece, SentencePiece Unigram LM or Byte Pair Encoding.', metadata={'Header 2': '11.4.3 Sequence Labelling'}), Document(page_content=\"A simple neural approach to SRL is to treat it as a sequence labeling task like namedentity recognition, using the BIO approach. Let's assume that we are given the predicate and the task is just detecting and labeling spans. Recall that with BIO tagging, we have a begin and end tag for each possible role (B-ARG0, I-ARG0; B-\\nARG1, I-ARG1, and so on), plus an outside tag O. As with all the taggers, the goal is to compute the highest probability tag sequence ˆy, given the input sequence of words w:\\n$${\\\\hat{y}}\\\\ =\\\\ {\\\\underset{y\\\\in T}{\\\\operatorname{argmax}}}\\\\,P(\\\\mathbf{y}|\\\\mathbf{w})$$\\nFig. 20.6 shows a sketch of a standard algorithm from He et al.\", metadata={'Header 2': '20.6.2 A Neural Algorithm For Semantic Role Labeling'}), Document(page_content='Supervised training data for tasks like named entity recognition (NER) is typically in the form of BIO tags associated with text segmented at the word level. For example the following sentence containing two named entities:\\n[LOC Mt. Sanitas ] is in [LOC Sunshine Canyon] . would have the following set of per-word BIO tags. (11.16) Mt. B-LOC\\nSanitas\\nI-LOC\\nis\\nO\\nin\\nO\\nSunshine\\nB-LOC\\nCanyon\\nI-LOC\\n. O\\nUnfortunately, the WordPiece tokenization for this sentence yields the following sequence of tokens which doesn\\'t align directly with BIO tags in the ground truth annotation:\\n\\'Mt\\', \\'.\\', \\'San\\', \\'##itas\\', \\'is\\', \\'in\\', \\'Sunshine\\', \\'Canyon\\' \\'.\\'\\nTo deal with this misalignment, we need a way to assign BIO tags to subword tokens during training and a corresponding way to recover word-level tags from subwords during decoding. For training, we can just assign the gold-standard tag associated with each word to all of the subword tokens derived from it. For decoding, the simplest approach is to use the argmax BIO tag associated with the first subword token of a word. Thus, in our example, the BIO tag assigned to \"Mt\" would be assigned to \"Mt.\" and the tag assigned to \"San\" would be assigned to \"Sanitas\", effectively ignoring the information in the tags assigned to \".\" and \"##itas\". More complex approaches combine the distribution of tag probabilities across the subwords in an attempt to find an optimal word-level tag.', metadata={'Header 2': '11.4.3 Sequence Labelling'})]\n"
     ]
    }
   ],
   "source": [
    "test_retrieval = retriever.get_relevant_documents(\"POS Tagging\")\n",
    "print(len(test_retrieval))\n",
    "print(test_retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a7d36e3-be0e-4cd2-9514-bb9552fbce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = str()\n",
    "with open(\"../prompts/rag-user-prompt.txt\", \"r\") as file:\n",
    "    user_prompt = file.read()\n",
    "\n",
    "system_prompt = str()\n",
    "with open(\"../prompts/rag-system-prompt.txt\", \"r\") as file:\n",
    "    system_prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8694ba8-8176-4894-91f1-c2285ef93d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-0125-preview\")\n",
    "\n",
    "local_llm = ChatOllama(\n",
    "    model=\"gemma\",\n",
    "    num_ctx=4096,\n",
    "    top_p=0.9,\n",
    "    top_k=100\n",
    ")\n",
    "\n",
    "def get_unique_union(documents):\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    to_return = [loads(doc) for doc in unique_docs]\n",
    "    return to_return\n",
    "\n",
    "def prepare_context(docs):\n",
    "    docs = get_unique_union([docs])\n",
    "    references = lambda metadata: \"\\n\".join(f\"{k} {v}\" for k, v in metadata.items())\n",
    "    \n",
    "    formatter = lambda content, metadata: f\"\"\"\n",
    "    {content} \n",
    "    \n",
    "    References from the textbook: \n",
    "    {references(metadata)}\n",
    "    \"\"\"\n",
    "\n",
    "    formatted_content = \"\\n\\n\".join(formatter(doc.page_content, doc.metadata) for doc in docs)\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    You are a professional english data cleaner. Your role is to read the documents extracted from a book,\n",
    "    clean it without degrading the quality of the text. \n",
    "\n",
    "    You should follow these guidelines: \n",
    "    - Pay utmost emphasis on preserving the actual ground truth of the text. It is completely fine to repeat\n",
    "      the given sentences verbatim if they fit. \n",
    "    - The given text is in markdown. Don't omit any necessary tables, inline equations or block equations. \n",
    "    - Keep the references format as is. Right after the document's content.  \n",
    "    - Your response should be formatted as follows:\n",
    "      ## Context 1\n",
    "         ...\n",
    "         References: ...\n",
    "      ## Context 2\n",
    "         ...\n",
    "         References: ...\n",
    "      ## Context 3\n",
    "         ...\n",
    "         References: ...\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = \"\"\"\n",
    "    Clean the following text without compromising on truth: \n",
    "\n",
    "    {context}\n",
    "    \"\"\"\n",
    "\n",
    "    clean_text_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", user_prompt)\n",
    "    ])\n",
    "    \n",
    "    chain = (\n",
    "        { \"context\": RunnablePassthrough() }\n",
    "        | clean_text_prompt \n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    response = str(chain.invoke(formatted_content))\n",
    "    return response\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", user_prompt)\n",
    "])\n",
    "\n",
    "main_chain = (\n",
    "    { \"topics\": RunnablePassthrough(), \"context\": retriever | prepare_context } \n",
    "    | prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d888d31-8775-41ca-841c-79c72a163bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Question\n",
      "Given the definitions and concepts outlined in the provided textbook content, which of the following statements best illustrates the concept of \"structural ambiguity\" in the context of natural language processing and context-free grammars (CFGs)?\n",
      "\n",
      "1. Structural ambiguity occurs when a grammar cannot generate any string of terminal symbols from the start symbol.\n",
      "2. It happens when a single string of terminal symbols can be derived using different sequences of rule applications, leading to multiple possible parse trees.\n",
      "3. Structural ambiguity arises solely from the limitations of the CKY algorithm when parsing sentences in natural language.\n",
      "4. It is the result of having too many non-terminal symbols in a CFG, leading to unclear derivation paths for strings.\n",
      "5. Structural ambiguity is related to the inability of a context-free grammar to be converted into Chomsky Normal Form (CNF).\n",
      "\n",
      "## Solution\n",
      "The correct answer is: 2. It happens when a single string of terminal symbols can be derived using different sequences of rule applications, leading to multiple possible parse trees.\n",
      "\n",
      "## Reasoning\n",
      "Structural ambiguity in natural language processing refers to the phenomenon where a single sequence of words (a string of terminal symbols) can be assigned more than one syntactic structure or interpretation. This ambiguity is significant because it impacts how sentences are parsed and understood by both humans and computational systems. In the context of context-free grammars (CFGs), this ambiguity manifests when a single string of terminal symbols can be derived from the start symbol through different sequences of rule applications, potentially resulting in multiple valid parse trees for the same sentence. This is directly related to the concept mentioned in Context 2, which highlights that structural ambiguity presents a significant challenge for parsers, because it complicates the process of determining the correct syntactic structure of sentences.\n",
      "\n",
      "Choices 1, 3, 4, and 5 do not accurately represent the concept of structural ambiguity as defined in the provided contexts. Specifically:\n",
      "- Choice 1 is incorrect because structural ambiguity is not about the inability to generate strings but about generating the same string in multiple ways.\n",
      "- Choice 3 is incorrect because structural ambiguity exists independently of any specific parsing algorithm, such as the CKY algorithm.\n",
      "- Choice 4 misinterprets the source of ambiguity; it is not the number of non-terminal symbols that causes structural ambiguity but the nature of the grammar rules and their applications.\n",
      "- Choice 5 is incorrect because structural ambiguity is a property of the language or grammar itself, not a result of conversion processes like converting a CFG into CNF.\n",
      "\n",
      "Therefore, option 2 is the best representation of structural ambiguity as it aligns with the definitions and implications discussed in the provided textbook content, particularly emphasizing the challenges it poses for syntactic parsing and interpretation within the framework of context-free grammars.\n"
     ]
    }
   ],
   "source": [
    "response = str(main_chain.invoke(\"\"\"\n",
    "Context-Free Grammars\n",
    "\"\"\"))\n",
    "print(response)\n",
    "with open(\"../responsebuffers/test.md\", \"w\") as file:\n",
    "    file.write(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec2bccc-8a5d-4485-b6a7-9913d4a7f06b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
