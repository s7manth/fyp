{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a42720-bbad-4a0f-896c-c87d508ec862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"fyp-rag-experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff8547c-ae36-4f75-a971-fd5c80ab9e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ede46152-22a7-45dc-a128-38009a8e1b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "#### INDEXING ####\n",
    "from bs4 import BeautifulSoup\n",
    "from markdown import markdown\n",
    "import re\n",
    "\n",
    "def markdown_to_text(markdown_string):\n",
    "    html = markdown(markdown_string)\n",
    "    html = re.sub(r'<pre>(.*?)</pre>', ' ', html)\n",
    "    html = re.sub(r'<code>(.*?)</code >', ' ', html)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = ''.join(soup.findAll(string=True))\n",
    "    return text\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "texts = list()\n",
    "for file_path in os.listdir(\"../data/cleaned\"):\n",
    "    if not file_path.endswith(\".md\"):\n",
    "        continue\n",
    "\n",
    "    with open(\"../data/cleaned/\" + file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "        text = text.strip()\n",
    "        texts.append(text)\n",
    "\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "deaa59b5-6e3e-4b81-b1aa-2e8d3179ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "document_chunks = list()\n",
    "for text in texts:\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    md_header_splits = markdown_splitter.split_text(text)\n",
    "    \n",
    "    text_splitter = SemanticChunker(\n",
    "        OpenAIEmbeddings(), breakpoint_threshold_type=\"interquartile\"\n",
    "    )\n",
    "    \n",
    "    list_of_strings = [markdown_to_text(split.page_content) for split in md_header_splits]\n",
    "    metadatas = [split.metadata for split in md_header_splits]\n",
    "    \n",
    "    document_chunks.extend(text_splitter.create_documents(list_of_strings, metadatas=metadatas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbba1a31-05fd-49b1-9fa6-4f9a0aedce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     document_chunks, \n",
    "#     embedding=OpenAIEmbeddings(), \n",
    "#     persist_directory=\"./slp_vectordb\"\n",
    "# )\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=OpenAIEmbeddings(), \n",
    "    persist_directory=\"./slp_vectordb\"\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "755b604b-42ea-4e35-8570-a20a1a876948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[Document(page_content=\"Some languages, like Japanese, don't have spaces between words, so word tokenization becomes more difficult. Another part of text normalization is lemmatization, the task of determining lemmatization that two words have the same root, despite their surface differences. For example, the words sang, sung, and sings are forms of the verb sing. The word sing is the common lemma of these words, and a lemmatizer maps from all of these to sing. Lemmatization is essential for processing morphologically complex languages like Arabic. Stemming refers to a simpler version of lemmatization in which we mainly stemming just strip suffixes from the end of the word. Text normalization also includes sentence segmentation: breaking up a text into individual sentences, using cues like sentence segmentation periods or exclamation points.\"), Document(page_content='For other natural language processing situations we also want two morphologically different forms of a word to behave similarly. For example in web search, someone may type the string woodchucks but a useful system might want to also return pages that mention woodchuck with no s. This is especially common in morphologically complex languages like Polish, where for example the word Warsaw has different endings when it is the subject (Warszawa), or after a preposition like \"in Warsaw\" (w Warszawie), or \"to Warsaw\" (do Warszawy), and so on. Lemmatization is the task lemmatization of determining that two words have the same root, despite their surface differences. The words am, are, and is have the shared lemma be; the words dinner and dinners both have the lemma dinner. Lemmatizing each of these forms to the same lemma will let us find all mentions of words in Polish like Warsaw. The lemmatized form of a sentence like He is reading detective stories would thus be He be read detective story. How is lemmatization done?', metadata={'Header 1': 'and update the corpus return V', 'Header 2': '2.6.1 Lemmatization'}), Document(page_content=\"Lemmatization algorithms can be complex. For this reason we sometimes make use of a simpler but cruder method, which mainly consists of chopping off wordfinal affixes. This naive version of morphological analysis is called stemming. For stemming example, the Porter stemmer, a widely used stemming algorithm (Porter, 1980), Porter stemmer when applied to the following paragraph:\\nThis was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things-names and heights and soundings-with the single exception of the red crosses and the written notes. produces the following stemmed output:\\nThi wa not the map we found in Billi Bone s chest but an accur copi complet in all thing name and height and sound with the singl except of the red cross and the written note The algorithm is based on series of rewrite rules run in series: the output of each pass is fed as input to the next pass. Here are some sample rules (more details can be found at https://tartarus.org/martin/PorterStemmer/):\\nATIONAL → ATE (e.g., relational → relate)\\nING → ϵ\\nif the stem contains a vowel (e.g., motoring → motor)\\nSSES → SS\\n(e.g., grasses → grass)\\nSimple stemmers can be useful in cases where we need to collapse across different variants of the same lemma. Nonetheless, they do tend to commit errors of both over- and under-generalizing, as shown in the table below (Krovetz, 1993):\\n| Errors of Commission   | Errors of Omission   |\\n|------------------------|----------------------|\\n| organization organ     | European Europe      |\\n| doing                  | doe                  |\\n| numerical              | numerous             |\\n| policy                 | police               |\", metadata={'Header 1': 'and update the corpus return V', 'Header 2': 'Stemming: The Porter Stemmer'}), Document(page_content=\"Consider inflected forms like cats versus cat. We say these two words are different wordforms but have the same lemma. A lemma is a set of lexical forms having the same stem, the same lemma major part-of-speech, and the same word sense. The wordform is the full inflected wordform or derived form of the word. The two wordforms cat and cats thus have the same lemma, which we can represent as cat. For morphologically complex languages like Arabic, we often need to deal with lemmatization. For most tasks in English, however, wordforms are sufficient, and when we talk about words in this book we almost always mean wordsforms (although we will discuss basic algorithms for lemmatization and the related task of stemming below in Section 2.6). One of the situations even in English where we talk about lemmas is when we measure the number of words in a dictionary. Dictionary entries or boldface forms are a very rough approximation to (an upper bound on) the number of lemmas (since some lemmas have multiple boldface forms). The 1989 edition of the Oxford English Dictionary had 615,000 entries. Finally, we should note that in practice, for many NLP applications (for example for neural language modeling) we don't actually use words as our internal unit of representation at all! We instead tokenize the input strings into tokens, which can be words but can also be only parts of words. We'll return to this tokenization question when we introduce the BPE algorithm in Section 2.5.2.\", metadata={'Header 2': '2.2 Words'})]\n"
     ]
    }
   ],
   "source": [
    "test_retrieval = retriever.get_relevant_documents(\"Lemmatization\")\n",
    "print(len(test_retrieval))\n",
    "print(test_retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a7d36e3-be0e-4cd2-9514-bb9552fbce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = str()\n",
    "with open(\"../prompts/rag-user-prompt.txt\", \"r\") as file:\n",
    "    user_prompt = file.read()\n",
    "\n",
    "system_prompt = str()\n",
    "with open(\"../prompts/rag-system-prompt.txt\", \"r\") as file:\n",
    "    system_prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8694ba8-8176-4894-91f1-c2285ef93d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-0125-preview\")\n",
    "\n",
    "local_llm = ChatOllama(\n",
    "    model=\"gemma\",\n",
    "    num_ctx=4096,\n",
    "    top_p=0.9,\n",
    "    top_k=100\n",
    ")\n",
    "\n",
    "def get_unique_union(documents):\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    to_return = [loads(doc) for doc in unique_docs]\n",
    "    return to_return\n",
    "\n",
    "def prepare_context(docs):\n",
    "    docs = get_unique_union([docs])\n",
    "    references = lambda metadata: \"\\n\".join(f\"{k} {v}\" for k, v in metadata.items())\n",
    "    \n",
    "    formatter = lambda content, metadata: f\"\"\"\n",
    "    {content} \n",
    "    \n",
    "    References from the textbook: \n",
    "    {references(metadata)}\n",
    "    \"\"\"\n",
    "\n",
    "    formatted_content = \"\\n\\n\".join(formatter(doc.page_content, doc.metadata) for doc in docs)\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    You are a professional english data cleaner. Your role is to read the documents extracted from a book,\n",
    "    clean it without degrading the quality of the text. \n",
    "\n",
    "    You should follow these guidelines: \n",
    "    - Pay utmost emphasis on preserving the actual ground truth of the text. It is completely fine to repeat\n",
    "      the given sentences verbatim if they fit. \n",
    "    - The given text is in markdown. Don't omit any necessary tables, inline equations or block equations. \n",
    "    - Keep the references format as is. Right after the document's content.  \n",
    "    - Your response should be formatted as follows:\n",
    "      ## Context 1\n",
    "         ...\n",
    "         References: ...\n",
    "      ## Context 2\n",
    "         ...\n",
    "         References: ...\n",
    "      ## Context 3\n",
    "         ...\n",
    "         References: ...\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = \"\"\"\n",
    "    Clean the following text without compromising on truth: \n",
    "\n",
    "    {context}\n",
    "    \"\"\"\n",
    "\n",
    "    clean_text_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", user_prompt)\n",
    "    ])\n",
    "    \n",
    "    chain = (\n",
    "        { \"context\": RunnablePassthrough() }\n",
    "        | clean_text_prompt \n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    response = str(chain.invoke(formatted_content))\n",
    "    return response\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", user_prompt)\n",
    "])\n",
    "\n",
    "main_chain = (\n",
    "    { \"topics\": RunnablePassthrough(), \"context\": retriever | prepare_context } \n",
    "    | prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d888d31-8775-41ca-841c-79c72a163bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Question\n",
      "In the context of Kneser-Ney smoothing, especially the modified Kneser-Ney version, consider the scenario where you are tasked with estimating the probabilities of various bigrams in a large corpus. Knowing that modified Kneser-Ney smoothing uses different discounts for n-grams with counts of 1, 2, and three or more, which of the following best describes the purpose of using three different discounts ($d_1$, $d_2$, and $d_{3+}$) in the algorithm?\n",
      "\n",
      "1. To ensure that all n-grams with a count higher than three contribute equally to the probability estimation.\n",
      "2. To increase the computational efficiency of the algorithm by reducing the complexity of calculations for frequent n-grams.\n",
      "3. To provide a more nuanced approach to discounting, reflecting the varying degrees of reliability of n-grams based on their frequency.\n",
      "4. To allocate more probability mass to unigrams, thereby simplifying the model to behave more like a unigram model.\n",
      "5. To guarantee that the probabilities of all bigrams sum to 1, ensuring a valid probability distribution.\n",
      "\n",
      "## Solution\n",
      "3. To provide a more nuanced approach to discounting, reflecting the varying degrees of reliability of n-grams based on their frequency.\n",
      "\n",
      "## Reasoning\n",
      "The modified Kneser-Ney smoothing algorithm uses three different discounts: $d_1$, $d_2$, and $d_{3+}$, for n-grams with counts of 1, 2, and three or more, respectively (Context 2). This differentiation is crucial because it addresses the varying predictive reliability of n-grams based on how frequently they appear in the corpus. N-grams that occur only once (i.e., hapax legomena) are treated differently from those that occur twice, and both are treated differently from those that occur three times or more. The rationale behind this is to reflect the inherent uncertainty and variability in the predictive value of n-grams based on their frequency of occurrence. More frequent n-grams are generally more reliable indicators of linguistic patterns than less frequent ones. Therefore, by using different discounts for different frequency bands, the modified Kneser-Ney smoothing provides a more nuanced and effective approach to discounting, which helps in better modeling the probability distributions of words in natural language (Context 2). \n",
      "\n",
      "This approach does not aim to simplify the model to behave more like a unigram model (Choice 4), nor does it specifically aim to ensure computational efficiency (Choice 2) or guarantee that the probabilities of all bigrams sum to 1, as normalization to ensure a valid probability distribution is a separate concern (Choice 5). The purpose of using different discounts is primarily to reflect the varying degrees of reliability of n-grams based on their frequency, hence making the model more accurate and reflective of natural language usage (Choice 3).\n"
     ]
    }
   ],
   "source": [
    "response = str(main_chain.invoke(\"\"\"\n",
    "Kneser-Ney Smoothing\n",
    "\"\"\"))\n",
    "print(response)\n",
    "with open(\"../responsebuffers/test.md\", \"w\") as file:\n",
    "    file.write(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec2bccc-8a5d-4485-b6a7-9913d4a7f06b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
