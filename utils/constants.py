topics = {
    "01": """
    - Regular Expressions
    - Text Normalization
    - Edit Distance
    - Words
    - Corpora
    - Simple Unix Tools for Word Tokenization
    - Word Tokenization
    - Word Normalization
    - Lemmatization
    - Stemming
    - Sentence Segmentation
    """,
    "02": """
    - N-gram Language Models
    - N-Grams
    - Evaluating Language Models: Training and Test Sets
    - Evaluating Language Models: Perplexity
    - Sampling sentences from a language model
    - Generalization and Zeros
    - Smoothing
    - Huge Language Models and Stupid Backoff
    - Kneser-Ney Smoothing
    - Perplexityâ€™s Relation to Entropy
    """,
    "03": """
    - Naive Bayes Classifiers
    - Training the Naive Bayes Classifier
    - Optimizing for Sentiment Analysis
    - Naive Bayes for other text classification tasks
    - Naive Bayes as a Language Model
    - Evaluation: Precision, Recall, F-measure
    - Test sets and Cross-validation
    - Statistical Significance Testing
    - Avoiding Harms in Classification
    """,
    "04": """
    - Logistic Regression
    - The sigmoid function
    - Classification with Logistic Regression
    - Multinomial logistic regression
    - Learning in Logistic Regression
    - The cross-entropy loss function
    - Gradient Descent
    - Regularization
    - Learning in Multinomial Logistic Regression
    - Interpreting models
    - Advanced: Deriving the Gradient Equation
    """,
    "05": """
    - Vector Semantics and Embeddings
    - Lexical Semantics
    - Words and Vectors
    - Cosine for measuring similarity
    - TF-IDF: Weighing terms in the vector
    - Pointwise Mutual Information (PMI)
    - Applications of the tf-idf or PPMI vector models
    - Word2vec
    - Visualizing Embeddings
    - Semantic properties of embeddings
    - Bias and Embeddings
    - Evaluating Vector Models
    """,
    "06": """
    - Neural Networks
    - The XOR problem
    - Feed forward Neural Networks
    - Feed forward networks for NLP: Classification
    - Training Neural Nets
    - Feed forward Neural Language Modeling
    - Training the neural language model
    """,
    "07": """
    - Transformers and Large Language Models
    - The Transformer: A Self-Attention Network
    - Multi-head Attention
    - Transformer Blocks
    - The Residual Stream view of the Transformer Block
    - The input: embeddings for token and position
    - The Language Modeling Head
    - Large Language Models with Transformers
    - Large Language Models: Generation by Sampling
    - Large Language Models: Training Transformers
    - Potential Harms from Language Models
    """,
    "08": """
    - Fine-Tuning and Masked Language Models
    - Bidirectional Transformer Encoders
    - Training Bidirectional Encoders
    - Contextual Embeddings
    - Fine-Tuning Language Models
    - Advanced: Span-based Masking
    """,
    "09": """
    - Part-of-Speech Tagging
    - Named Entities and Named Entity Tagging
    - HMM Part-of-Speech Tagging
    - Conditional Random Fields (CRFs)
    - Evaluation of Named Entity Recognition
    """
}
