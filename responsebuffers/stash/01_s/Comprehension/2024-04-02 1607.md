## Question
A data scientist is cleaning a dataset containing online product reviews to prepare it for sentiment analysis. The dataset contains several inconsistencies such as extra white spaces, HTML tags, and website URLs. Which of the following regular expressions should the data scientist use to sequentially remove extra white spaces, HTML tags, and website URLs, respectively, to ensure the most accurate text preprocessing for sentiment analysis?

1. Extra white spaces: `[\s]+`, HTML tags: `<.*?>`, URLs: `http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+`
2. Extra white spaces: `[\s]{2,}`, HTML tags: `<[^>]*>`, URLs: `http[s]?://[^\s]+`
3. Extra white spaces: `[\s]{2,}`, HTML tags: `</?[^>]+>`, URLs: `www\.[a-z\?\.]+`
4. Extra white spaces: `\s{2,}`, HTML tags: `[^<]*<[^>]*>`, URLs: `http[s]?://\S+`
5. Extra white spaces: `[\s]+`, HTML tags: `<.+?>`, URLs: `(http|https)://(\w+\.)(\w+)`

## Solution

To remove extra white spaces, HTML tags, and URLs from text data accurately, each pattern must be identified precisely:

**Extra white spaces:**
The goal is to find and replace sequences of white spaces with a single space. A regex pattern `[\s]+` or `\s{2,}` will match any sequence of white spaces (spaces, tabs, new lines) that occur more than once. The regex `[\s]{2,}` does the same but is less concise.

**HTML Tags:**
HTML tags can be removed using a regex pattern that captures sequences starting with `<`, followed by any characters except `>`, and ending with `>`. It's critical that the pattern is non-greedy (i.e., matches the shortest possible sequence) to avoid removing useful text between tags. Patterns like `<.*?>`, `<[^>]*>`, and `</?[^>]+>` achieve this. Patterns that match text outside of HTML tags or are not specific enough could lead to incorrect text removal.

**Website URLs:**
URLs can vary greatly, but they typically start with "