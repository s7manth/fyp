## Question

Suppose you are working with a large corpus of text, and you want to extract all the words that contain the letter sequence "th". However, the text contains many typos and misspellings, and some words may have been spelled differently in different parts of the corpus. Further, some words may be written as compound words or phrases, while others may be hyphenated or contain apostrophes.

Which of the following techniques would you use to extract all the words containing "th" from the corpus, taking into account these variations and complexities?

## Choices

1. Use regular expressions with capturing groups and backreferences.
2. Apply text normalization techniques, such as word decomposition and compound word splitting.
3. Calculate edit distances between words and threshold the differences based on a fixed parameter.
4. Utilize a combination of word tokenization, word normalization, and lemmatization.
5. Implement simple Unix tools for word tokenization and then apply sentence segmentation.

## Solution

To extract all the words containing "th" from the corpus while taking into account the variations and complexities mentioned, we would need to use a combination of techniques.

First, we can use text normalization techniques to standardize the spellings of words and handle compound words, hyphenated words, and words with apostrophes. Specifically, we can apply word decomposition to break down compound words into their individual components and word splitting to separate hyphenated words into their constituent parts.

Next, we can calculate edit distances between words and threshold the differences based on a fixed parameter to identify words containing "th". Edit distance is a measure of the similarity between two strings, with smaller distances indicating greater similarity. By setting a threshold based on the expected frequency of "th" in the language, we can filter out rare or misspelled words that do not contain "th".

Finally, to handle words that are spelled differently in different parts of the corpus, we can implement word tokenization and lemmatization. Word tokenization breaks down text into individual words, while lemmatization reduces words to their base or root form (e.g., "think" becomes "think"). By combining these techniques with regular expressions using capturing groups and backreferences, we can extract all the words containing "th" from the corpus.

## Correct Answer

The correct answer is: 4. Utilize a combination of word tokenization, word normalization, and lemmatization.

## Reasoning

This solution synthesizes multiple concepts and techniques covered in the course to address the given problem. By first applying text normalization techniques to standardize spellings and handle compound words, we can increase the accuracy of word extraction. Then, by calculating edit distances and thresholding based on language frequency, we can filter out rare or misspelled words that do not contain "th". Finally, using word tokenization and lemmatization, we can handle variations in word spellings and reduce the impact of typos and misspellings. This comprehensive approach ensures that all relevant words containing "th" are extracted from the corpus, even in the presence of complexities and variations.