The performance of Naive Bayes, Logistic Regression, and Multi-Layer Perceptron (MLP) in a text classification problem depends on various factors, including the nature of the dataset, the complexity of the decision boundaries, and the amount of training data available.

- **Naive Bayes** is known for its simplicity and speed. It works well with text data due to its assumption of feature independence, which is surprisingly effective for text despite the inherent dependencies between words. However, its simplicity can also be a limitation when dealing with complex decision boundaries.

- **Logistic Regression** can manage high-dimensional spaces efficiently, as text data often becomes after vectorization (e.g., via TF-IDF). It assumes linear relationships between the independent variables and the log odds of the outcomes, which can be powerful for linearly separable data but might struggle with more complex patterns.

- **Multi-Layer Perceptron (MLP)**, a type of neural network, can model non-linear relationships and complex decision boundaries through its hidden layers. While it has the potential to achieve higher accuracy, especially in complex scenarios where relationships between text features are non-linear, it requires more data and computational resources. Additionally, MLPs have a higher risk of overfitting compared to simpler models if not properly regularized and tuned.

Considering these aspects, the statement that **"MLP is expected to have the highest accuracy but may require significantly more computational resources and data than Naive Bayes or Logistic Regression."** seems the most accurate. Both Naive Bayes and Logistic Regression could perform well in text classification tasks, especially if the decision boundary is relatively simple or if there's limited data, but MLP has the potential to capture more complex patterns at the cost of computational efficiency and the need for more data to train effectively without overfitting.