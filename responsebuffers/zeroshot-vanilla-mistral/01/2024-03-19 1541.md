 ## Problem Statement

Given a corpus of customer reviews from an e-commerce website, you are tasked with performing text normalization and calculating the edit distance between words to identify similar product names mentioned in the reviews. Using simple Unix tools for word tokenization, apply techniques such as stemming, lemmatization, and remove stopwords to preprocess the data before comparison.

## Choices

1. To perform text normalization and calculate edit distances between words, first use lemmatization on the corpus. Then, apply stemming to the resulting lemmas for further reduction in word forms.
2. Use simple Unix tools like `awk` and `sed` for word tokenization, followed by applying regular expressions to perform text normalization. Finally, calculate edit distances between words using a dynamic programming algorithm.
3. Perform text normalization using lowercasing, remove stopwords with the help of NLTK library in Python, then apply stemming using the Snowball Stemmer for preprocessing data before calculating edit distances using a distance metric like Jaro-Winkler.
4. Apply simple Unix tools for word tokenization, and after text normalization by removing stopwords, use an off-the-shelf NLTK corpus for performing lemmatization on the resulting words. Calculate the edit distances between these lemmas for comparison.
5. Use a combination of techniques including stemming, lemmatization, and simple Unix tools for word tokenization to preprocess the data. Afterward, employ a machine learning algorithm like Longest Common Subsequence (LCS) to determine similar product names based on edit distances between words.

## Solution

The correct answer is 3.

## Reasoning

Firstly, text normalization involves converting all the words into their base forms, such as lowercasing or removing special characters. The preprocessing step of this task already includes this by using simple Unix tools for tokenization and applying lowercasing. Next, stopwords are removed to reduce noise in the data, which is done efficiently with the help of the NLTK library in Python.

The actual text normalization techniques like stemming or lemmatization need to be applied after this preprocessing step to reduce words to their root forms for further comparison. So, options 1 and 5 are incorrect since they suggest applying stemming and lemmatization before removing stopwords, which is not the correct sequence of steps in text normalization.

Option 2 is incorrect because it suggests using regular expressions for text normalization, while simple Unix tools like `awk` and `sed` can efficiently handle tokenization and lowercasing without requiring regular expressions. Furthermore, edit distances cannot be directly calculated with regular expressions; they require dynamic programming algorithms or other methods.

Option 4 is incorrect because it suggests applying an off-the-shelf NLTK corpus for lemmatization after removing stopwords, but the correct sequence of steps should be the reverse: remove stopwords first and then apply lemmatization. Additionally, this option proposes calculating edit distances between these lemmas which is a computationally expensive process compared to simple distance metrics like Jaro-Winkler.

Therefore, the correct answer is 3, where text normalization is performed using lowercasing and stopword removal followed by applying stemming using NLTK's Snowball Stemmer for preprocessing data before calculating edit distances between words with a metric like Jaro-Winkler.