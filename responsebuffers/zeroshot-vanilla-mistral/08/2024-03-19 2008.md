 ## Problem Statement

Consider a bidirectional transformer encoder fine-tuned on a new dataset that includes medical records. The goal is to extract information related to a specific disease from these records using masked language models and contextual embeddings. Choose the best option for each part of this task:

1. Preprocess the input text by applying:
   a) Forward-only encoding
   b) Bidirectional encoding with masked tokens
   c) Masked language model fine-tuning without bidirectionality
   d) Contextual embedding extraction without preprocessing

2. Fine-tune the transformer encoder using:
   a) A new dataset, ignoring previous knowledge
   b) Transfer learning with a smaller dataset and masked language models
   c) Span-based masking only for training data
   d) Fully connected layers for classification

3. Perform contextual embedding extraction by:
   a) Computing static word embeddings without considering context
   b) Using transformer encoder outputs as contextualized representations
   c) Applying pre-trained language models for text generation
   d) Training a separate RNN model for each position in the sequence

## Choices

1. Preprocess the input text by applying:
    a) Forward-only encoding
    b) Bidirectional encoding with masked tokens
    c) Masked language model fine-tuning without bidirectionality
    d) Contextual embedding extraction without preprocessing

2. Fine-tune the transformer encoder using:
    a) A new dataset, ignoring previous knowledge
    b) Transfer learning with a smaller dataset and masked language models
    c) Span-based masking only for training data
    d) Fully connected layers for classification

3. Perform contextual embedding extraction by:
    a) Computing static word embeddings without considering context
    b) Using transformer encoder outputs as contextualized representations
    c) Applying pre-trained language models for text generation
    d) Training a separate RNN model for each position in the sequence

## Solution

1. Preprocess the input text by applying: **b) Bidirectional encoding with masked tokens**

Solution reasoning: Bidirectional encoding is essential for handling medical records because it considers both past and future context to better understand complex information. Masked tokens allow fine-tuning on specific parts of the data, which is critical in this scenario.

2. Fine-tune the transformer encoder using: **b) Transfer learning with a smaller dataset and masked language models**

Solution reasoning: Transfer learning leverages previous knowledge to adapt to new tasks efficiently. In this case, fine-tuning on a smaller medical dataset with masked language models ensures better performance as it allows the model to focus on relevant features while still retaining general knowledge.

3. Perform contextual embedding extraction by: **b) Using transformer encoder outputs as contextualized representations**

Solution reasoning: Transformer encoder outputs provide more accurate and fine-grained contextualized embeddings than static word embeddings, making them the best choice for this task since they capture the nuances of medical records.

## Reasoning

The first question tests the understanding of preprocessing techniques, with a focus on bidirectional encoding and masked tokens, which are crucial in handling complex medical records. The second question covers fine-tuning strategies and how transfer learning can be applied to improve performance when dealing with smaller datasets. Lastly, question three evaluates students' knowledge of contextual embeddings, emphasizing the importance of transformer encoder outputs as more accurate representations compared to static word embeddings.