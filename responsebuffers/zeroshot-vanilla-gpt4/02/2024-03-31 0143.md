## Question
Given a sizable document corpus, you are to develop a trigram language model to generate text that mimics the style and context of the training data. To improve the model's handling of unseen trigrams and to lower the overall perplexity scores, you decide to apply a smoothing technique. Among the choices listed below, which smoothing technique does *not* directly leverage lower-order n-grams for estimating probabilities of unseen trigrams in the context of trigram language models?

1. Absolute discounting
2. Add-one (Laplace) smoothing
3. Kneser-Ney smoothing
4. Good-Turing discounting
5. Katz backoff

## Solution
To solve this question, one must understand the different smoothing techniques used in language models, particularly how each technique estimates the probabilities of unseen n-grams.

1. **Absolute discounting** reduces the count of all n-grams by a constant factor to allocate some probability mass to unseen n-grams. It leverages lower-order n-grams for estimating these probabilities by distributing the subtracted probability mass among them based on their occurrence in the training data.
   
2. **Add-one (Laplace) smoothing** simply adds one to the count of all n-grams (seen and unseen) across the vocabulary. Although it does not explicitly leverage lower-order n-grams for assigning unseen n-grams' probabilities, the effect of increasing counts uniformly across all n-grams indirectly impacts the probabilities calculated for lower-order n-grams.
   
3. **Kneser-Ney smoothing** specifically leverages lower-order n-grams in a sophisticated way by considering the diversity of contexts in which n-grams appear, thus providing a more nuanced approach to distributing probability mass to unseen n-grams.
   
4. **Good-Turing discounting** adjusts the counts of all seen n-grams according to the frequencies of their occurrences, particularly focusing on the counts of n-grams that appear only once ("singletons"), to account for unseen n-grams. While it does adjust probabilities based on observed frequencies, it doesn't directly use lower-order n-gram counts in the adjustment formula but rather applies a global recalculation based on frequency buckets.
   
5. **Katz backoff** is a method that explicitly uses lower-order n-grams when higher-order n-gram probabilities are zero (i.e., the n-gram has not been seen in the training set). It backs off to lower-order models (like bigrams or unigrams) with a penalty for each step back.

Given the question seeks the smoothing technique that does not **directly leverage lower-order n-grams** for estimating probabilities of unseen trigrams, the correct answer is **Add-one (Laplace) smoothing.** While it indirectly affects the probabilities of n-grams across the board due to uniform count adjustments, it does not directly compute unseen trigrams' probabilities based on lower-order n-gram probabilities.

## Correct Answer
2. Add-one (Laplace) smoothing

## Reasoning
Add-one (Laplace) smoothing is the only technique among the choices listed that does not explicitly or directly adjust the probability estimation process for unseen n-grams by using lower-order n-grams in its calculation. While it does affect the distribution of probabilities across all n-gram orders by uniformly increasing the counts, this adjustment does not involve a direct leveraging of lower-order n-gram counts or probabilities specifically for unseen n-grams, in contrast to the other smoothing methods mentioned, which more directly utilize lower-order n-grams in their respective formulas to address the zero-probability issue for unseen n-grams.