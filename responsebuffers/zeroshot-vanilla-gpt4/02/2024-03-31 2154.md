## Question
Given a dataset containing various text documents in the English language, you want to develop a trigram language model to generate text passages as part of an autonomous storytelling system. After implementing a basic trigram model, you observed that the generated text often contains repetitive phrases or gets stuck in a loop, repeating the same triad of words. Considering the need to improve the diversity of generated text without significantly increasing the model complexity, which of the following approaches should you adopt?

1. Increase the n-gram order from trigram to 5-gram to capture longer dependencies in the text.
2. Implement Kneser-Ney smoothing to better handle low-frequency trigrams and improve generalization capabilities.
3. Switch to a unigram model to reduce the dependencies between words and increase text diversity.
4. Introduce Stupid Backoff, but without smoothing, to handle unseen n-grams more effectively.
5. Employ perplexity-based selection for each next-word prediction to ensure higher randomness in the generated text.

## Solution

To determine the best approach for improving the diversity of generated text in a trigram model with observed repetitive phrases or loops, consider each option's impact on handling low-frequency events, generalization, and dependencies:

1. **Increase the n-gram order to 5-gram**: This would likely exacerbate the issue since higher-order n-grams would make the model more sensitive to the specific sequences seen in the training data, thus reducing diversity in generation due to even stronger dependencies.

2. **Implement Kneser-Ney smoothing**: Kneser-Ney smoothing is a powerful technique for dealing with the sparsity problem in n-gram models. It does so by effectively redistributing probability mass to unseen or low-frequency n-grams based on the context diversity of the following word. This could indeed help in avoiding repetitive loops by providing a smarter way to handle rare and unseen trigrams, thus likely improving the diversity of the generated text.

3. **Switch to a unigram model**: Reducing the dependency to unigrams would drastically increase the text diversity but at the cost of coherence and relevance of the generated text, making it unlikely to form meaningful stories in an autonomous storytelling system.

4. **Introduce Stupid Backoff, but without smoothing**: Stupid Backoff is a technique to handle unseen n-grams by backing off to lower-order n-grams without normalizing the probabilities. While it might help in handling unseen n-grams, the lack of smoothing means it doesn't directly address the root problem of distribution of unseen or low-frequency n-grams which leads to repetitive loops.

5. **Employ perplexity-based selection for each next-word prediction**: While this could theoretically increase randomness, perplexity itself is a measure of model's performance (specifically, it measures how well a probability distribution or probability model predicts a sample). Using it for next-word prediction decision-making does not directly address the issue of repetitive loops and might introduce a high level of randomness that could disrupt the coherence and relevance of the generated text.

Therefore, the best option among these for improving text diversity without significantly increasing complexity, while avoiding repetitive loops, is to implement Kneser-Ney smoothing.

## Correct Answer

2. Implement Kneser-Ney smoothing to better handle low-frequency trigrams and improve generalization capabilities.

## Reasoning

Kneser-Ney smoothing is one of the most effective techniques for dealing with the sparsity and zero-frequency problems inherent in n-gram language models. By adjusting the probability distribution to account more accurately for the probability of unseen n-grams based on the context diversity of the subsequent word, it helps in mitigating the issue of repetitive phrases or loops. This approach allows for a more natural handling of the unpredictability inherent in language, helping to boost the diversity and quality of the generated text. Unlike other methods which either increase complexity without addressing the core issue (Option 1), overly simplify the model at the expense of coherence (Option 3), do not address smoothing (Option 4), or could disrupt text coherence (Option 5), Kneser-Ney smoothing directly targets the model's ability to generalize from seen to unseen contexts in a balanced way, making it the most suitable choice for the specified problem.