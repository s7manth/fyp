## Question

Given a scenario where you're tasked with improving the performance of a machine translation system, you decide to employ an RNN-based Encoder-Decoder architecture. The system is translating sentences from English to French, and you're particularly focused on reducing the error in translating long sentences. You notice that the current architecture struggles with capturing long-range dependencies in the source sentences. To address this issue, you propose modifications to the architecture.

Which of the following modifications is MOST likely to improve the system's ability to capture long-range dependencies and hence improve the translation of long sentences?

1. Replacing the RNN units in both the encoder and decoder with simple feedforward neural networks.
2. Increasing the number of hidden layers in both the encoder and decoder RNNs.
3. Employing a Bidirectional RNN (BiRNN) in the encoder while keeping a unidirectional RNN in the decoder.
4. Decreasing the size of the hidden state vectors in the encoder and decoder to reduce the model's computational complexity.
5. Replacing the RNN units with Convolutional Neural Networks (CNNs) in both the encoder and decoder.

## Solution

The correct step for improving the system's ability to capture long-range dependencies in machine translation, particularly for long sentences, involves addressing the inherent limitations of traditional RNNs in processing such dependencies. Here's a breakdown of why each option stands or falls short:

1. **Replacing the RNN units with simple feedforward neural networks:** This approach would remove the sequential processing capability of the model, which is crucial for language tasks. Feedforward networks lack memory and would be even less capable of handling long-range dependencies than RNNs.

2. **Increasing the number of hidden layers in both the encoder and decoder RNNs:** While adding more layers can help the model learn more complex features and representations, it does not directly address the key issue of long-range dependency capture. Moreover, deeper RNNs are notoriously difficult to train due to problems like vanishing and exploding gradients.

3. **Employing a Bidirectional RNN (BiRNN) in the encoder while keeping a unidirectional RNN in the decoder:** This approach is promising. BiRNNs process data in both forward and backward directions. By doing so in the encoder, the model can have access to both past and future context within the source sentence, significantly improving its ability to capture long-range dependencies. The decoder, being unidirectional, maintains the necessary causal structure for generation tasks.

4. **Decreasing the size of the hidden state vectors in the encoder and decoder to reduce the model's computational complexity:** Reducing the hidden state size would likely worsen the model's performance, especially for complex tasks like machine translation. Smaller hidden states have limited capacity to store and process information, which is particularly detrimental for handling long-range dependencies.

5. **Replacing the RNN units with Convolutional Neural Networks (CNNs) in both the encoder and decoder:** CNNs are excellent for hierarchical feature extraction in spatial data (like images), but they are not inherently sequential like RNNs. While CNNs can be used in NLP tasks, and specific architectures might help with long-range dependencies, the option provided does not directly tackle the RNN-based Encoder-Decoder model's limitations in handling long sentences better than the others listed.

Based on these considerations, the most effective modification to improve the translation of long sentences by capturing long-range dependencies is using a Bidirectional RNN in the encoder while keeping a unidirectional RNN in the decoder.

## Correct Answer

3. Employing a Bidirectional RNN (BiRNN) in the encoder while keeping a unidirectional RNN in the decoder.

## Reasoning

The rationale behind selecting a Bidirectional RNN for the encoder stems from its ability to present the entire input sequence (both past and future tokens relative to the current token) to the model. This comprehensive context availability is crucial for understanding the full meaning of sentences, especially longer ones that traditional unidirectional RNNs might struggle with due to their sequential processing nature, which tends to fade information from early tokens in long sequences. The decoder remains unidirectional because, during translation, the generation of the target sentence needs to be causal, meaning the model can only consider previous and current tokens to predict the next one, maintaining the sequence's integrity and coherence.
