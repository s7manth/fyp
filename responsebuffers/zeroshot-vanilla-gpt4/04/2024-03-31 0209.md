## Question
Consider a sentiment analysis task where a machine learning model is developed to classify text reviews into positive, negative, or neutral sentiments. You are using multinomial logistic regression for this multiclass classification task. The model utilizes a cross-entropy loss function to learn from the training data, and you've decided to implement L2 regularization to combat overfitting. Given that the regularization strength is denoted by $\lambda$, which of the following options correctly describes how to update the weight vector $\mathbf{w}$ for a single class during the training using stochastic gradient descent (SGD)?

1. $\mathbf{w} := \mathbf{w} - \alpha\left(\frac{\partial}{\partial \mathbf{w}}(-\log\text{likelihood}) + \lambda \mathbf{w}\right)$
2. $\mathbf{w} := \mathbf{w} - \alpha\left(\frac{\partial}{\partial \mathbf{w}}(-\log\text{likelihood}) - \lambda \mathbf{w}\right)$
3. $\mathbf{w} := \mathbf{w} + \alpha\left(\frac{\partial}{\partial \mathbf{w}}(-\log\text{likelihood}) + \lambda\|\mathbf{w}\|\right)$
4. $\mathbf{w} := \mathbf{w} - \alpha\left(\frac{\partial}{\partial \mathbf{w}}(-\log\text{likelihood})\right)$, and separately, $\mathbf{w} := \mathbf{w} - \lambda \mathbf{w}$
5. None of the above correctly describes the weight update rule including L2 regularization for multinomial logistic regression using SGD.

## Solution

The correct procedure for updating the weights $\mathbf{w}$ in the context of SGD with L2 regularization for a multinomial logistic regression model can be expressed as follows:

First, it's imperative to recognize that the L2 regularization term is $\frac{\lambda}{2}\|\mathbf{w}\|^2$ (the squared norm of the weight vector multiplied by half of the regularization strength, $\lambda$). However, when calculating the gradient concerning $\mathbf{w}$, the differentiation of $\frac{\lambda}{2}\|\mathbf{w}\|^2$ with respect to $\mathbf{w}$ results in $\lambda \mathbf{w}$. This outcome stems from the property of differentiation where the exponent (2 in this case) is brought forward, multiplied by the variable ($\mathbf{w}$), and the exponent is reduced by 1, leading to $2 \cdot \frac{\lambda}{2} \mathbf{w}$, which simplifies to $\lambda \mathbf{w}$.

The weight update formula, incorporating the gradient of the loss with respect to the weights and the gradient of the regularization term, is then:

$$\mathbf{w} := \mathbf{w} - \alpha\left(\frac{\partial}{\partial \mathbf{w}}\text{(cross-entropy loss)} + \lambda \mathbf{w}\right)$$

This equation demonstrates that after computing the gradient of the loss function, you adjust the weights by moving in the opposite direction of this gradient (to minimize the loss) and also take into account the regularization term, which penalizes large weights by shrinking them according to the regularization strength $\lambda$.

Given this understanding, the correct option is:

1. $\mathbf{w} := \mathbf{w} - \alpha\left(\frac{\partial}{\partial \mathbf{w}}(-\log\text{likelihood}) + \lambda \mathbf{w}\right)$

This option correctly captures the influence of both the gradient of the loss function and the L2 regularization term on the weight vector $\mathbf{w}$ during an iteration of SGD.

## Correct Answer

1. $\mathbf{w} := \mathbf{w} - \alpha\left(\frac{\partial}{\partial \mathbf{w}}(-\log\text{likelihood}) + \lambda \mathbf{w}\right)$

## Reasoning

The key aspects to consider for solving this problem are the understanding of how the cross-entropy loss and L2 regularization affect weight updates in multinomial logistic regression, especially when using SGD:

- **Cross-entropy loss**: It quantifies the difference between the predicted probabilities and the actual class distributions. Its gradient with respect to the weights indicates the direction in which the weights should be adjusted to reduce the loss.
- **L2 regularization**: A technique used to prevent overfitting by discouraging overly complex models. It does so by adding a penalty on the size of the coefficients to the loss function.
- **SGD weight update rule**: Weights are updated by moving in the opposite direction of the gradient of the loss function, scaled by the learning rate $\alpha$. When L2 regularization is applied, the regularization term $\lambda \mathbf{w}$ is added to the gradient of the loss function.

Option 1 correctly combines these elements into the SGD update rule that considers both the loss gradient and the L2 regularization effect, aligning with the theoretical and practical aspects of training multinomial logistic regression models.