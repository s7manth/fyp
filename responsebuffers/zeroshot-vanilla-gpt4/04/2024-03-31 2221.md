## Question
Consider a multinomial logistic regression problem where a model is being trained to classify text documents into one of three categories: Technology (T), Entertainment (E), and Health (H). The model uses a feature vector extracted from the text of the document, $\mathbf{x}$, and applies the softmax function to predict the probability distribution over the three categories. To regularize the model, an L2 regularization term is added to the cross-entropy loss function.

Given the regularization parameter $\lambda$, and the weight matrices for the three categories $\mathbf{W}_{T}$, $\mathbf{W}_{E}$, and $\mathbf{W}_{H}$, which of the following correctly represents the gradient of the loss function with respect to the weight matrix of the Technology category ($\mathbf{W}_{T}$), used for updating $\mathbf{W}_{T}$ during gradient descent?

1. $\nabla_{\mathbf{W}_{T}} L = (\hat{y}_{T} - y_{T}) \mathbf{x} + 2\lambda\mathbf{W}_{T}$
2. $\nabla_{\mathbf{W}_{T}} L = \sum_{i=1}^{N}(\hat{y}_{Ti} - y_{Ti}) \mathbf{x}_i + \lambda\mathbf{W}_{T}$
3. $\nabla_{\mathbf{W}_{T}} L = \sum_{i=1}^{N}(\hat{y}_{Ti} - y_{Ti}) \mathbf{x}_i + 2\lambda\mathbf{W}_{T}$
4. $\nabla_{\mathbf{W}_{T}} L = (\hat{y}_{T} - y_{T}) \mathbf{x} + \lambda\mathbf{W}_{T}$
5. $\nabla_{\mathbf{W}_{T}} L = \sum_{i=1}^{N}(\hat{y}_{Ti} - y_{i}) \mathbf{x}_i + 2\lambda\mathbf{W}_{T}$

## Solution
To find the correct answer, we need to understand the components of the loss function in the context of multinomial logistic regression and gradient descent, including how regularization is applied.

1. **Multinomial Logistic Regression**: The prediction $\hat{y}$ is made using the softmax function, applied to the linear combinations of the input features $\mathbf{x}$ and the weight matrices $\mathbf{W}$ for each category. For a given document $i$ and category $T$, the predicted probability that document $i$ belongs to category $T$ can be expressed as $\hat{y}_{Ti}$.

2. **Cross-Entropy Loss Function**: The cross-entropy loss for multinomial (or softmax) logistic regression can be expressed as a sum over all $N$ documents, where $y_{Ti}$ is the true label (1 if the document belongs to category $T$, else 0), and $\hat{y}_{Ti}$ is the predicted probability for category $T$.

3. **L2 Regularization**: To control for overfitting, an L2 regularization term is added to the loss function, which is proportional to the square of the norm of the weight matrix. For the weight matrix of the Technology category $\mathbf{W}_{T}$, this term is $\lambda\|\mathbf{W}_{T}\|^2$, where $\lambda$ is the regularization parameter. The gradient of this term with respect to $\mathbf{W}_{T}$ is $2\lambda\mathbf{W}_{T}$ because the derivative of $\|\mathbf{W}_{T}\|^2$ with respect to $\mathbf{W}_{T}$ is $2\mathbf{W}_{T}$.

4. **Combining Components**: The total gradient of the loss ($L$) with respect to $\mathbf{W}_{T}$ incorporates both the gradient of the cross-entropy loss and the gradient of the regularization term. The cross-entropy component is a sum over all $N$ documents of the product of the difference between the predicted probability $\hat{y}_{Ti}$ and the actual label $y_{Ti}$, and the feature vector $\mathbf{x}_i$. The L2 regularization component is $2\lambda\mathbf{W}_{T}$.

Therefore, the correct expression for the gradient of the loss function with respect to $\mathbf{W}_{T}$, incorporating both the error from the cross-entropy loss and the impact of L2 regularization, is option 3:

$$\nabla_{\mathbf{W}_{T}} L = \sum_{i=1}^{N}(\hat{y}_{Ti} - y_{Ti}) \mathbf{x}_i + 2\lambda\mathbf{W}_{T}$$

## Correct Answer
3. $\nabla_{\mathbf{W}_{T}} L = \sum_{i=1}^{N}(\hat{y}_{Ti} - y_{Ti}) \mathbf{x}_i + 2\lambda\mathbf{W}_{T}$

## Reasoning
This solution integrates understanding of the softmax function, cross-entropy loss, and L2 regularization within the specific context of multinomial logistic regression to derive the correct gradient expression. It also correctly identifies that the regularization term contributes to the gradient according to the derivative of the squared norm of the weights, leading to the inclusion of the $2\lambda\mathbf{W}_{T}$ term alongside the sum over all documents of the product of the prediction error and input features for the Technology category.