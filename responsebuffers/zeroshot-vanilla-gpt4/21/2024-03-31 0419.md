## Question

In a recent project for Natural Language Processing, you are analyzing the coherence relations and discourse structure in text documents using Representation Learning. Your approach involves applying a neural network model to encode sentences into vector spaces where coherence relations can be learned and predicted. Considering the project's goal to improve document summarization by understanding deeper text coherence, which of the following methodologies would be least effective in capturing both local and global coherence in the text?

1. Training a Siamese Network to distinguish coherent from incoherent pairs of sentences based on their vector representation.
2. Using a Transformer-based model with attention mechanisms to capture dependencies and relations across sentences in a paragraph.
3. Implementing a Unigram Model to predict the likelihood of a sentence given its preceding sentences in the document.
4. Employing a Graph Neural Network to model the document as a graph, where sentences are nodes, and edges represent coherence relations between them.
5. Designing a Long Short-Term Memory (LSTM) network to sequentially process sentences, exploiting both past and future context for coherence understanding.

## Solution

The key to answering this question lies in understanding the difference between local and global coherence and how various models can capture these aspects of text coherence. 

- **Local Coherence** refers to the coherence relations between nearby elements in the text, such as adjacent sentences.
- **Global Coherence** refers to the overarching structure and thematic consistency throughout the entire text or document.

**Siamese Network (Option 1):** Siamese networks can effectively measure similarity or distance between pairs of inputs, and in this context, they could differentiate between coherent and incoherent pairs of sentences. This method strongly caters to local coherence by analyzing sentence pairs, but it does not inherently analyze or capture global coherence throughout an entire document.

**Transformer-based Model (Option 2):** With its attention mechanisms, a Transformer-based model is adept at considering both local and global dependencies across sentences in a document. This model captures long-distance relationships and can effectively contribute to understanding both local and global coherence.

**Unigram Model (Option 3):** A Unigram Model, given its simplistic approach which involves predicting the likelihood of individual tokens without considering their context or relations, is not well-suited for understanding coherence. Coherence involves relational understanding between different parts of the text, which a unigram model fundamentally lacks both at local and especially at a global level.

**Graph Neural Network (Option 4):** By modeling the document as a graph where sentences are nodes connected based on coherence relations, a Graph Neural Network can capture complex inter-sentence relationships. This approach can be effective for both local and global coherence, as it allows for the modeling of non-sequential relations throughout a document.

**LSTM Network (Option 5):** LSTMs are capable of capturing relationships in sequential data by considering context from both past and future instances in the sequence. This makes them suitable for understanding local coherence by analyzing the flow of information in text. However, the effectiveness of LSTM in capturing global coherence depends on the implementation and the length of text it can process effectively due to potential issues with long-range dependencies.

### Correct Answer

3. Implementing a Unigram Model to predict the likelihood of a sentence given its preceding sentences in the document.

### Reasoning

A Unigram Model operates at the level of individual token probabilities independent of their context. This severely limits its ability to understand or capture any form of coherence, whether local or global. Coherence relies on understanding the relationships and dependencies between different parts of the text, which requires considering the context in which tokens appear. The other options, despite varying strengths and weaknesses, offer mechanisms to relate different parts of the text in ways that a Unigram Model cannot, making it the least effective choice for the stated goal of improving document summarization through understanding text coherence.