## Question

In an effort to enhance a text summarization application, an NLP researcher is exploring the incorporation of discourse structure for better capturing the global coherence of source texts. The application’s current model struggles with generating summaries that accurately reflect the hierarchical relationships and logical flow of ideas in complex academic texts. Considering the application's needs and the principles of discourse structure parsing, which of the following approaches would most effectively improve the summarization quality by enhancing the understanding of global coherence in texts?

1. Training a Transformer-based model exclusively on sentence-level embeddings to predict the next sentence in a sequence, aiming to capture the local coherence between adjacent sentences.
2. Implementing an RST (Rhetorical Structure Theory)-based discourse parser that can identify and hierarchically organize rhetorical relations within the text, before passing this structured representation to the summarization model.
3. Enhancing the model with a BERT-based entity recognition system to keep track of entities and their occurrences, focusing on improving entity-based coherence without altering the model's approach to understanding hierarchical text structures.
4. Applying a sequence-to-sequence model that employs attention mechanisms at word-level granularity alone, assuming this will implicitly capture the discourse-level relationships necessary for understanding global coherence.
5. Integrating a graph neural network (GNN) that models the text as a graph of sentence-level embeddings without explicit recognition or tagging of rhetorical or discourse relations, hypothesizing that the model will learn these implicitly through graph topology.

## Solution

To select the most effective approach for enhancing the summarization quality by improving the understanding of global coherence, we need to consider the capabilities of each proposed method in capturing hierarchical relationships and logical flow in texts. 

1. **Transformer-based model for sentence prediction**: This approach focuses mainly on local coherence by predicting the next sentence in a sequence. While beneficial for understanding immediate context, it lacks explicit mechanisms for identifying higher-level discourse structures that contribute to global coherence.

2. **RST-based discourse parser**: RST (Rhetorical Structure Theory) provides a framework for analyzing the structural and functional attributes of texts. An RST-based discourse parser can identify rhetorical relations (e.g., contrast, cause-effect), organizing sentences and paragraphs into a hierarchical tree structure. This method directly tackles the application’s need to understand the hierarchical relationships and logical flow, making it suitable for improving global coherence in summaries.

3. **BERT-based entity recognition system**: Focusing on entity-based coherence might help maintain consistency of entities throughout the summary. However, it does not address the hierarchical organization of ideas or the logical connections within the text, which are critical for capturing global coherence.

4. **Sequence-to-sequence model with word-level attention**: While attention mechanisms can provide insights into the importance of specific words, relying solely on word-level granularity might not be sufficient to capture the discourse-level relationships and structures necessary for understanding global coherence.

5. **Graph neural network (GNN) with sentence-level embeddings**: Modeling the text as a graph might help in capturing some relational aspects between sentences. However, without explicit recognition of rhetorical or discourse relations, this approach may not fully address the need for understanding hierarchical and logical structures inherent in complex academic texts.

Given these considerations, the most effective approach for enhancing the summarization quality by improving the understanding of global coherence is:

**2. Implementing an RST (Rhetorical Structure Theory)-based discourse parser that can identify and hierarchically organize rhetorical relations within the text, before passing this structured representation to the summarization model.**

## Correct Answer

2. Implementing an RST (Rhetorical Structure Theory)-based discourse parser that can identify and hierarchically organize rhetorical relations within the text, before passing this structured representation to the summarization model.

## Reasoning

The reasoning behind this choice is grounded in the requirements of capturing hierarchical relationships and the logical flow of ideas, which are essential components of global coherence. An RST-based discourse parser directly addresses these aspects by identifying rhetorical relations and organizing text into a hierarchical structure. This structured representation enables the summarization model to better understand and reflect the global coherence of the source texts, translating into more coherent and logically structured summaries. This approach aligns with the application's goal of improving summarization quality for complex academic texts.where logical flow and hierarchical organization of ideas are crucial.