The reasoning behind choosing option 2 lies in the fundamental goal of pre-training language models: to develop an understanding of language context and structure that can be generalized across various tasks. Span-based masking, by requiring the model to predict longer sequences of text, inherently challenges the model to understand the text in a more comprehensive manner. This involves recognizing patterns, understanding semantics, and synthesizing information from various parts of the input data. Consequently, span-based masking fosters a deeper understanding of contextual relationships within the text, a crucial aspect of natural language understanding and generation.