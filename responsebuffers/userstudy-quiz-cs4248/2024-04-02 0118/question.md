In the context of fine-tuning bidirectional transformer encoders for natural language processing tasks, which of the following statements accurately reflects an advantage of using span-based masking over traditional token-based masking strategies during the pre-training phase?

1. Span-based masking reduces the computational complexity of the model, leading to faster training times.
2. By masking contiguous sequences of tokens, span-based masking encourages the model to learn higher-order dependencies between non-adjacent tokens, enhancing its ability to understand context.
3. Span-based masking significantly reduces the size of the pre-training dataset needed, as it provides more information per masked prediction.
4. Span-based masking enables the model to operate with lower memory requirements, as it processes fewer, larger tokens rather than many smaller ones.
5. Span-based masking improves the alignment of the pre-trained model with recurrent neural network architectures, facilitating easier integration.