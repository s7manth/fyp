A data scientist is building a next-word prediction model for a mobile keyboard app that requires the model to be both efficient and reasonably accurate. The dataset comprises diverse sources, including social media posts, news articles, and technical manuals. Given the diversity of the dataset and the need for real-time prediction with limited hardware resources, the data scientist considers various N-gram language models and smoothing techniques. Which of the following options best balances model size, complexity, and prediction quality for this application?

1. A unigram model with Laplace smoothing
2. A bigram model with Good-Turing smoothing
3. A trigram model with Kneser-Ney smoothing
4. A quadrigram model without any smoothing
5. A 5-gram model with Stupid Backoff