Span-based masking during the pre-training phase enriches the model's ability to understand and infer from larger contexts by encouraging it to predict missing spans of text. This understanding closely aligns with the requirements of text summarization, which involves grasping the main ideas and details across a text to generate coherent, concise summaries. This process naturally benefits from the model's enhanced capability to deal with longer spans of text, allowing it to better capture and reproduce the semantic richness necessary for high-quality summarization. Thus, employing span-based masking feasibly leads to improvements in the model's summarization performance by bolstering its comprehension of contextual dependencies.