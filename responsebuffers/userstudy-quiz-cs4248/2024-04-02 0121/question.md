In natural language processing, the Encoder-Decoder architecture is widely used for tasks such as machine translation. An advanced NLP system employs a stacked LSTM-based Encoder-Decoder model to translate sentences from English to French. The system's performance is critically dependent on the ability of the encoder to capture the semantic meanings of the source sentences, and the decoder's capacity to generate accurate translations. Taking into consideration the strengths of LSTMs and the operational dynamics of Encoder-Decoder models, which of the following adjustments is MOST likely to improve the system's translation accuracy?

1. Increasing the depth of the encoder LSTM layers, while keeping the decoder LSTM layers shallow.
2. Replacing the LSTM layers in the encoder with simple RNN layers to expedite the training process.
3. Incorporating attention mechanisms in both the encoder and decoder to enhance the model's focus on relevant parts of the input sentence throughout the translation.
4. Doubling the size of the LSTM units in the decoder only, assuming that the bottleneck is in generating the target language.
5. Implementing dropout in the encoder but not the decoder to prevent overfitting to the source language.